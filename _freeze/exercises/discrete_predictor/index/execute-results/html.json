{
  "hash": "95edc6eda58f1c05e5ffa9850db0e55e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle:  Palmer penguins and discrete predictors\ndescription: |\n  fitting a model with discrete predictors.\nexecute:\n  freeze: true\nformat:\n  html:\n    code-tools: true\neditor_options: \n  chunk_output_type: console\n---\n\nIn this section we're going to look at a simple model with a single predictor variable which divides the dataset into categories. In this example, categories are treated as \"fixed\" effects.\n\n## Load packages and data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.6\n✔ forcats   1.0.1     ✔ stringr   1.6.0\n✔ ggplot2   4.0.1     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.2.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(rstanarm)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: Rcpp\nThis is rstanarm version 2.32.2\n- See https://mc-stan.org/rstanarm/articles/priors for changes to default priors!\n- Default priors may change, so it's safest to specify priors, even if equivalent to the defaults.\n- For execution on a local, multicore CPU with excess RAM we recommend calling\n  options(mc.cores = parallel::detectCores())\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(lme4)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: Matrix\n\nAttaching package: 'Matrix'\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(tidybayes)\n```\n:::\n\n\n## Data exploration\n\nLet's start by taking a look at the Palmer Penguin dataset, specifically the distribution of observations of bill size.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npenguins |> \n  ggplot(aes(x=bill_dep)) + \n  geom_histogram(binwidth = .5)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![Histogram of bill depth for all the penguins in the Palmer Penguin dataset.](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nThere's quite a lot of variation in these measurements, with a suggestion of perhaps more than one peak in this distribution.\n\nThere's also some NA values -- we'll drop them before we move on:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npenguins_noNAbill <- penguins |> \n  drop_na(bill_dep)\n```\n:::\n\n\n## A simple model\n\n$$\n\\begin{align}\n\\text{Bill depth} &\\sim \\text{Normal}(\\mu, \\sigma)\\\\\n\\mu &\\sim \\text{Normal}(17, 2) \\\\\n\\sigma &\\sim \\text{Exponential}(1) \\\\\n\\end{align}\n$$\n\n### Prior predictive simulation (using rstanarm)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbill_stan_PD <- stan_glm(\n  bill_dep ~ 1,\n  data = penguins_noNAbill,\n  family = gaussian(),\n  prior_intercept = normal(17, 2),\n  prior_aux = exponential(1),\n  prior_PD = TRUE,\n  chains = 4,\n  iter = 2000,\n  refresh = 0,\n  seed = 525600\n)\n\nbill_stan_PD\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nstan_glm\n family:       gaussian [identity]\n formula:      bill_dep ~ 1\n observations: 342\n predictors:   1\n------\n            Median MAD_SD\n(Intercept) 17.1    1.9  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 0.7    0.7   \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n```\n\n\n:::\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npp_check(bill_stan_PD, nreps = 20)\n```\n\n::: {.cell-output-display}\n![prior predictive simulations of penguin bills](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\nSome things to notice about the process and results above:\n\n-   similar to yesterday's example, we've fit a univariate model to these data. However, because it is a *gaussian* model, it requires two parameters: a mean and a standard deviation.\n-   although the priors are close to a visual inspection of the histogram, the assumption here is that the prior parameters come from expertise or prior simulations, NOT from looking at the data before modelling!\n\nWe can use the [`posterior` package](https://mc-stan.org/posterior/) to extract and conveniently summarize the draws from the distribution.\n\n## Fit the model\n\nNow we can fit the model directly in rstanarm and look at the results:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbill_stan <- stan_glm(\n  bill_dep ~ 1,\n  data = penguins_noNAbill,\n  family = gaussian(),\n  prior_intercept = normal(17, 2),\n  prior_aux = exponential(0.5),\n  chains = 4,\n  iter = 2000,\n  refresh = 0,\n  seed = 525600\n)\n\nbill_stan\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nstan_glm\n family:       gaussian [identity]\n formula:      bill_dep ~ 1\n observations: 342\n predictors:   1\n------\n            Median MAD_SD\n(Intercept) 17.2    0.1  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 2.0    0.1   \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbill_stan |> \n  posterior::summarise_draws() |> \n  knitr::kable()\n```\n\n::: {.cell-output-display}\n\n\n|variable    |     mean|   median|        sd|       mad|        q5|       q95|     rhat| ess_bulk| ess_tail|\n|:-----------|--------:|--------:|---------:|---------:|---------:|---------:|--------:|--------:|--------:|\n|(Intercept) | 17.14934| 17.15088| 0.1075699| 0.1094916| 16.973739| 17.326793| 1.000930| 2529.865| 2333.974|\n|sigma       |  1.98253|  1.97824| 0.0776162| 0.0766075|  1.860122|  2.116191| 1.000684| 2527.450| 1936.182|\n\n\n:::\n:::\n\n\n## Plotting parameters.\n\nWe don't have one value for each of our unknown numbers: we have thousands. We need to get a sense of what these possible values mean scientifically. An excellent way to do this is by making as many pictures as possible. We will start with making plots of specific parameters.\n\nWe can look at the distributions easily using the `bayesplot` package.\n\n\n::: {.cell layout-ncol=\"2\"}\n\n```{.r .cell-code}\nbayesplot::mcmc_hist(bill_stan, pars = \"(Intercept)\")  + \n  coord_cartesian(xlim = c(15, 20))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n\n```{.r .cell-code}\nbayesplot::mcmc_hist(bill_stan, pars = \"sigma\") + \n  coord_cartesian(xlim = c(0, 4))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-2.png){width=672}\n:::\n:::\n\n\nNotice that the distributions do not have the same shape as the prior-- this is particularly true for $\\sigma$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npost_draws <- posterior::as_draws_df(bill_stan)\nset.seed(525600)\nndraws <- nrow(post_draws)\n\nprior_draws <- tibble(\n  Intercept = rnorm(ndraws, mean = 17, sd = 2),\n  sigma     = rexp(ndraws, rate = 0.5)\n)\n\ncombined_draws <- post_draws |>\n  transmute(\n    Intercept = `(Intercept)`,\n    sigma = sigma,\n    prior_Intercept = prior_draws$Intercept,\n    prior_sigma     = prior_draws$sigma\n  )\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Dropping 'draws_df' class as required metadata was removed.\n```\n\n\n:::\n:::\n\n\n\n\n::: {.cell layout-ncol=\"2\"}\n\n```{.r .cell-code}\nbayesplot::mcmc_hist(\n  combined_draws,\n  pars = c(\"Intercept\", \"prior_Intercept\")\n) +\n  coord_cartesian(xlim = c(10, 25))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![comparisons of priors and posteriors for two parameters, showing how much we've learned from the data.](index_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n\n```{.r .cell-code}\nbayesplot::mcmc_hist(\n  combined_draws,\n  pars = c(\"sigma\", \"prior_sigma\")\n) +\n  coord_cartesian(xlim = c(0, 6))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![comparisons of priors and posteriors for two parameters, showing how much we've learned from the data.](index_files/figure-html/unnamed-chunk-10-2.png){width=672}\n:::\n:::\n\n\n## Posterior predictions: the easy way to check your model\n\nIn my experience, ecologists (rightly!) care a great deal about model diagnostics. And with good reason: you need to know how much to trust a model before using it to make a scientific claim. Bayes offers a straightforward way to show how well a model is doing: plot model predictions, and compare them to the observed data. This involves using the model as a data generating machine, which we'll look at next.\n\n### Pseudocode\n\nHere is the procedure for generating posterior predictions:\n\n-   Select some posterior posterior draws.\n-   For each draw, extract all the model parameters\n-   For each draw, plug the sampled parameters in to the model. Use all the same predictors, factors, etc as the original model.\n-   For each draw, draw a random dataset that is the *same size and shape* as your original data.\n-   Overlay the simulated datasets on the observed data.\n\n### Posterior prediction in R\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# just get some draws\ndraws_matrix <- posterior::as_draws_matrix(bill_stan)\n\n## set up a matrix. for every posterior sample, \n## (that is, for a value of mu and a value of sigma) \n## draw a whole fake dataset from a normal distribution with that mean and sd. \nnsamples <- 50\nyrep <- matrix(0, \n               ncol = length(penguins_noNAbill$bill_dep), \n               nrow = nsamples)\n\n# pick some random rows\nset.seed(1234)\nchosen_samples <- sample(1:nrow(draws_matrix), \n                         replace = FALSE,\n                         size = nsamples)\n\nsubset_draws <- draws_matrix[chosen_samples,]\n\nfor (r in 1:nsamples){\n yrep[r,] <- rnorm(n = length(penguins_noNAbill$bill_dep), \n                   mean = subset_draws[r, \"(Intercept)\"], \n                   sd = subset_draws[r, \"sigma\"])\n}\n\nbayesplot::ppc_dens_overlay(y = penguins_noNAbill$bill_dep,\n                            yrep = yrep)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\nThis is the manual approach, which demonstrates the entire process explicitly. However, thanks to the power of rstanarm, this can also be accomplished in a single line:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npp_check(bill_stan, ndraws = 50)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: The following arguments were unrecognized and ignored: ndraws\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n::: callout-tip\n### EXERCISE\n\nLook at the other graphical posterior predictive checks available in the `bayesplot` package by examining [the vignette](https://mc-stan.org/bayesplot/articles/graphical-ppcs.html). Experiment with some different possibilities for these data.\n:::\n\nThe posterior predictive distribution gives us a straightforward way to test our model's performance:\n\n1.  we use the model to generate fake observations.\n2.  plot these on top of the real data\n3.  if the data is a really poor match, we know our model has a distorted view of the world.\n\n## Different groups are different\n\nlet's add in differences among species\n\n\n::: {.cell}\n\n```{.r .cell-code}\npenguins |> \n  ggplot(aes(x = bill_dep, fill = species))+ \n  geom_histogram(binwidth = .5) + \n  scale_fill_brewer(palette = \"Dark2\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\nNow we can see that the distribution is probably three different distributions, all placed together.\n\n::: callout-warning\nSometimes scientists will plot histograms of data at the beginning of a research project, and use the histogram to decide if their data are \"normally distributed\" or not. This is not helpful! Instead, decide on a model first, and ask yourself what kind of data you expect.\n:::\n\n## Adding a discrete predictor variable.\n\nHere we extend the model to give each species a different average bill depth. How many parameters are in this model?\n\n$$\n\\begin{align}\n\\text{Bill depth}_{i} &\\sim \\text{Normal}(\\mu_{\\text{species}[i]}, \\sigma) \\\\\n\\mu_{\\text{species}} &\\sim \\text{Normal}(17, 2) \\\\\n\\sigma &\\sim \\text{Exponential}(2) \\\\\n\\end{align}\n$$\n\n::: {.callout-tip collapse=\"true\"}\n## Quick detour : vector indexing\n\nA **very** useful technique, in both R and Stan, is transforming a vector with *indexing*. Vector indexing requires two vectors: the first contains values we want to select or replicate, the second contains integers giving the positions of the elements we want. For example:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsome_values <- c(\"taco\", \"cat\", \"goat\", \"cheeze\", \"pizza\")\npositions <- c(1,1,2,2,3,1,1,5)\n\nsome_values[positions]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"taco\"  \"taco\"  \"cat\"   \"cat\"   \"goat\"  \"taco\"  \"taco\"  \"pizza\"\n```\n\n\n:::\n:::\n\n\nThis works for number values as well, and is very useful when you want to do simulations! let's simulate three groups with different averages.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(525600)\nsome_means <- c(12, 17, 19)\nsome_labels <- c(\"taco\", \"cat\", \"goat\")\n\ndf_of_means <- data.frame(index = rep(1:3, each = 42)) |> \n  mutate(the_mean = some_means[index],\n         labels = some_labels[index],\n         obs = rnorm(n = length(the_mean),\n                     mean = the_mean,\n                     sd = 1))\n\ndf_of_means |> \n  ggplot(aes(x = obs, fill = labels)) + \n  geom_histogram()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n:::\n\n### Sampling the species model\n\n::: callout-tip\n### EXERCISE\n\nFit one the species-specific model above using rstanarm. TIP: set the formula to be `bill_dep ~ 0 + species`. \n1. What changes do you need to make to the prior? \n2. Visualize the posterior with `bayesplot`. Does it look better than the model without species? How can you tell?\n:::\n\n::: {.callout-note collapse=\"true\"}\n### SOLUTION\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbill_spp_stan <- stan_glm(\n  bill_dep ~ 0 + species,\n  data = penguins_noNAbill,\n  family = gaussian(),\n  prior = normal(17, 2),\n  prior_aux = exponential(0.5),\n  chains = 4,\n  iter = 2000,\n  refresh = 0,\n  seed = 525600\n)\n\nbill_spp_stan\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nstan_glm\n family:       gaussian [identity]\n formula:      bill_dep ~ 0 + species\n observations: 342\n predictors:   3\n------\n                 Median MAD_SD\nspeciesAdelie    18.3    0.1  \nspeciesChinstrap 18.4    0.1  \nspeciesGentoo    15.0    0.1  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 1.1    0.0   \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n```\n\n\n:::\n:::\n\n\nWe can repeat the posterior checking from before:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Posterior predictive check\nbrms::pp_check(bill_spp_stan, type = \"dens_overlay\", ndraws = 50)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: The following arguments were unrecognized and ignored: type, ndraws\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\nThe predicted distribution is now much more like the real data!\n:::\n\n#### Further questions & challenges\n\n-   I recommended that you remove the intercept from the model using `bill_dep ~ 0 + species`. What changes if you put it back in? why?\n\n### Visualizing species -- using `tidybayes`\n\nWe can also make figures for each individual species. Here we will move away from using `bayesplot` and try to visualize our posterior using the handy functions in the [`tidybayes` package](https://mjskay.github.io/tidybayes/).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidybayes)\n\npenguins_noNAbill |> \n  select(species) |> \n  distinct() |> \n  add_predicted_draws(bill_spp_stan, ndraws = 100) |> \n  ggplot(aes(x = .prediction, y = species, fill = species)) + \n  stat_halfeye()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\nWe can visualize the uncertainty in predicted values AND in group means :\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngrid <- penguins_noNAbill %>%\n  modelr::data_grid(species)\n\nmeans <- grid %>%\n  add_epred_draws(bill_spp_stan)\n\npreds <-  grid %>%\n  add_predicted_draws(bill_spp_stan)\n\npenguins_noNAbill %>%\n  ggplot(aes(y = species, x = bill_dep)) +\n  stat_interval(aes(x = .prediction), data = preds) +\n  stat_pointinterval(aes(x = .epred), data = means, .width = c(.66, .95), position = position_nudge(y = -0.3)) +\n  geom_jitter(height = .05, pch = 21, fill = \"orange\") +\n  scale_color_brewer() + \n  theme_dark()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\n### Exercises\n\n#### Level 1\n\n-   repeat this activity for another variable in the dataset. Does the same code work on bill length? What about body size? What would you change about the model (if anything)\n-   use bayesplot to examine the fit of body size to these data.\n\n#### Level 2\n\n-   generate some random groups of your own, with known means. How well does the model fit these data?\n\n#### Level 3\n\n-   As you can see, the model assumes the same sigma for all species. what if you relax this?\n\n### Optional!\n\nTry this on your own data!\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}