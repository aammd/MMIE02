[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MMIE02: Mixed Methods in Ecology",
    "section": "",
    "text": "What is a Linear Model? \nIntro to some useful datasets\nLinear models in more detail\nProbability distributions\nsimulation exercise"
  },
  {
    "objectID": "index.html#monday-02-feb",
    "href": "index.html#monday-02-feb",
    "title": "MMIE02: Mixed Methods in Ecology",
    "section": "",
    "text": "What is a Linear Model? \nIntro to some useful datasets\nLinear models in more detail\nProbability distributions\nsimulation exercise"
  },
  {
    "objectID": "index.html#tuesday-03-feb",
    "href": "index.html#tuesday-03-feb",
    "title": "MMIE02: Mixed Methods in Ecology",
    "section": "Tuesday 03 Feb",
    "text": "Tuesday 03 Feb\n\nDiscrete predictors\nLinear regression"
  },
  {
    "objectID": "index.html#wednesday-04-feb",
    "href": "index.html#wednesday-04-feb",
    "title": "MMIE02: Mixed Methods in Ecology",
    "section": "Wednesday 04 Feb",
    "text": "Wednesday 04 Feb\n\nHierarchical model slides\nOne random effect: species as a random effect\nLogistic regression"
  },
  {
    "objectID": "index.html#thursday-05-feb",
    "href": "index.html#thursday-05-feb",
    "title": "MMIE02: Mixed Methods in Ecology",
    "section": "Thursday 05 Feb",
    "text": "Thursday 05 Feb"
  },
  {
    "objectID": "slides/01-02-simulation/index.html#simulation-can-change-your-science",
    "href": "slides/01-02-simulation/index.html#simulation-can-change-your-science",
    "title": "Model building with Simulation",
    "section": "Simulation can change your science",
    "text": "Simulation can change your science\nIf you can’t simulate from a model, you do not understand it"
  },
  {
    "objectID": "slides/01-02-simulation/index.html#simulation-recipie",
    "href": "slides/01-02-simulation/index.html#simulation-recipie",
    "title": "Model building with Simulation",
    "section": "Simulation recipie",
    "text": "Simulation recipie\n\nwrite down your statistical model\nfor each unknown variable: Assign priors (if you’re Bayesian) or pick a plausible value (if you’re frequentist)\ntreat those values like the truth and make a fake dataset with them\ntake this fake dataset and fit the very same model back to these values\nsee if the models estimates are close to the truth"
  },
  {
    "objectID": "slides/01-02-simulation/index.html#why-do-this",
    "href": "slides/01-02-simulation/index.html#why-do-this",
    "title": "Model building with Simulation",
    "section": "Why do this",
    "text": "Why do this\n\nthe MINIMUM test of a model’s ability is its ability to recover these known parameters\nin reality we DON’T know the true model, but we can hope!"
  },
  {
    "objectID": "slides/01-02-simulation/index.html#exercise-together-simulate-for-a-regression",
    "href": "slides/01-02-simulation/index.html#exercise-together-simulate-for-a-regression",
    "title": "Model building with Simulation",
    "section": "Exercise together – simulate for a regression",
    "text": "Exercise together – simulate for a regression"
  },
  {
    "objectID": "slides/01-02-simulation/index.html#solo-exercise-simulate-for-a-regression-with-two-predictors",
    "href": "slides/01-02-simulation/index.html#solo-exercise-simulate-for-a-regression-with-two-predictors",
    "title": "Model building with Simulation",
    "section": "Solo exercise – simulate for a regression with two predictors",
    "text": "Solo exercise – simulate for a regression with two predictors\n\nshould we use the Penguin data actually? have something simple."
  },
  {
    "objectID": "slides/01-02-simulation/index.html#simulating-for-factors",
    "href": "slides/01-02-simulation/index.html#simulating-for-factors",
    "title": "Model building with Simulation",
    "section": "Simulating for factors",
    "text": "Simulating for factors\n\nneed to consider the contrast matrix that we use for the model\nthere are several forms to consider!"
  },
  {
    "objectID": "slides/01-02-simulation/index.html#discussion-think-about-your-design",
    "href": "slides/01-02-simulation/index.html#discussion-think-about-your-design",
    "title": "Model building with Simulation",
    "section": "Discussion – think about YOUR design",
    "text": "Discussion – think about YOUR design"
  },
  {
    "objectID": "slides/01-02-simulation/index.html#plotting-to-understand-models",
    "href": "slides/01-02-simulation/index.html#plotting-to-understand-models",
    "title": "Model building with Simulation",
    "section": "Plotting to understand models",
    "text": "Plotting to understand models\nWe’ve already seen the power of a simulation and plots to understand the model BEFORE we run it.\nLet’s examine plots to understand models AFTER we have run a model."
  },
  {
    "objectID": "slides/01-02-simulation/index.html#prediction-vs-expectation",
    "href": "slides/01-02-simulation/index.html#prediction-vs-expectation",
    "title": "Model building with Simulation",
    "section": "Prediction vs expectation",
    "text": "Prediction vs expectation\nThere are two kinds of predictions we can make for a simple model:\n\nWhat does the model think is the average response: this is called the “fitted” or “expected” value.\nWhat kind of observations might we make in the future? : these are called predicted observations."
  },
  {
    "objectID": "slides/01-01-intro/index.html#what-we-want-to-do-during-this-course",
    "href": "slides/01-01-intro/index.html#what-we-want-to-do-during-this-course",
    "title": "Introduction",
    "section": "What we want to do during this course",
    "text": "What we want to do during this course\n\nGeneral overview of linear models\nProbability distributions\nFrequentist and Bayesian approaches\nGLMMs for different kinds of data"
  },
  {
    "objectID": "slides/01-01-intro/index.html#quick-personal-introduction",
    "href": "slides/01-01-intro/index.html#quick-personal-introduction",
    "title": "Introduction",
    "section": "Quick personal introduction",
    "text": "Quick personal introduction\n\n\n\nField ecologist turned statistican\nstarted off in insect-plant interactions, has now done:\n\nfish populations\nkangaroo life history\nfly mating behaviour\nanimal personality\n\nMostly a Bayesian\nPlease introduce yourselves and your projects!"
  },
  {
    "objectID": "slides/01-01-intro/index.html#statistical-rethinking",
    "href": "slides/01-01-intro/index.html#statistical-rethinking",
    "title": "Introduction",
    "section": "Statistical Rethinking",
    "text": "Statistical Rethinking\n\nA good portion of this course material is based on this book! it is a fabulous guide to self-teaching!"
  },
  {
    "objectID": "slides/01-01-intro/index.html#bayesian-data-analysis",
    "href": "slides/01-01-intro/index.html#bayesian-data-analysis",
    "title": "Introduction",
    "section": "Bayesian Data Analysis",
    "text": "Bayesian Data Analysis\n\nEverything is there – rather technical !"
  },
  {
    "objectID": "slides/01-01-intro/index.html#bayesian-data-analysis-1",
    "href": "slides/01-01-intro/index.html#bayesian-data-analysis-1",
    "title": "Introduction",
    "section": "Bayesian Data Analysis",
    "text": "Bayesian Data Analysis\n\nExcellent pencil-and-paper intro to Bayesian thinking"
  },
  {
    "objectID": "slides/01-01-intro/index.html#course-datasets",
    "href": "slides/01-01-intro/index.html#course-datasets",
    "title": "Introduction",
    "section": "Course datasets",
    "text": "Course datasets\n\nPalmer Penguins\nmite data from the vegan package\nQuinn and Keough datasets"
  },
  {
    "objectID": "slides/01-01-intro/index.html#some-examples-of-linear-models-in-ecology",
    "href": "slides/01-01-intro/index.html#some-examples-of-linear-models-in-ecology",
    "title": "Introduction",
    "section": "Some examples of linear models in Ecology:",
    "text": "Some examples of linear models in Ecology:\n\na randomized block design\nBACI - pre-treatment variable\ngrowth over time\nfactorial designs"
  },
  {
    "objectID": "slides/01-01-intro/index.html#extensions-to-other-kinds-of-data",
    "href": "slides/01-01-intro/index.html#extensions-to-other-kinds-of-data",
    "title": "Introduction",
    "section": "Extensions to other kinds of data",
    "text": "Extensions to other kinds of data\nLinear models can be “generalized” to deal with many kinds of data types:\n\ncounts\nproportions\nsurvival times\nand more"
  },
  {
    "objectID": "slides/01-01-intro/index.html#putting-models-inside-models-many-names-for-the-same-idea",
    "href": "slides/01-01-intro/index.html#putting-models-inside-models-many-names-for-the-same-idea",
    "title": "Introduction",
    "section": "Putting models inside models: Many names for the same idea",
    "text": "Putting models inside models: Many names for the same idea\nHierarchical models, AKA:\n\nRandom effect models\nMixed models\nMultilevel models\nVariance component models\nError component models"
  },
  {
    "objectID": "slides/01-01-intro/index.html#structure-of-linear-models",
    "href": "slides/01-01-intro/index.html#structure-of-linear-models",
    "title": "Introduction",
    "section": "Structure of linear models",
    "text": "Structure of linear models\n\\[\n\\begin{align}\ny &\\sim \\text{Normal}(\\bar{y}, \\sigma) \\\\\n\\bar{y} &= \\textbf{X} \\beta_1 + b_0 \\\\\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/01-01-intro/index.html#frequentist-vs-bayes-rival-stastical-paradigms",
    "href": "slides/01-01-intro/index.html#frequentist-vs-bayes-rival-stastical-paradigms",
    "title": "Introduction",
    "section": "Frequentist vs Bayes: “Rival” stastical paradigms?",
    "text": "Frequentist vs Bayes: “Rival” stastical paradigms?"
  },
  {
    "objectID": "slides/01-01-intro/index.html#frequentist-approach",
    "href": "slides/01-01-intro/index.html#frequentist-approach",
    "title": "Introduction",
    "section": "Frequentist approach",
    "text": "Frequentist approach\n\nMaximum likelihood – finding parameter values that make the data most likely to have happened.\nHypothesis testing: if the data are more extreme than we would observe 1/20 times if the null were true, then we assume that the null is NOT true."
  },
  {
    "objectID": "slides/01-01-intro/index.html#bayesian-approach",
    "href": "slides/01-01-intro/index.html#bayesian-approach",
    "title": "Introduction",
    "section": "Bayesian approach",
    "text": "Bayesian approach\nLooking for the joint probability of the parameters given the data.\n\\[\n[\\theta|\\text{data}] = \\frac{[\\text{data}|\\theta] \\times [\\theta]}{[\\text{data}]}\n\\]\n\nusing Bayes Rule does not make you a Bayesian. Using probability as a measure of uncertainty makes you a Bayesian"
  },
  {
    "objectID": "slides/01-01-intro/index.html#math-explanation",
    "href": "slides/01-01-intro/index.html#math-explanation",
    "title": "Introduction",
    "section": "Math explanation",
    "text": "Math explanation\n$$ [a,b,| X] = \n$$\n\nHypothesis testing: asking if a parameter is well-separated from 0."
  },
  {
    "objectID": "slides/01-01-intro/index.html#discussion",
    "href": "slides/01-01-intro/index.html#discussion",
    "title": "Introduction",
    "section": "Discussion",
    "text": "Discussion\n\nbefore telling you my opinion of linear models, I’d like to have a group discussion about what limitations each of US has experienced with them."
  },
  {
    "objectID": "slides/01-01-intro/index.html#linear-models-strengths",
    "href": "slides/01-01-intro/index.html#linear-models-strengths",
    "title": "Introduction",
    "section": "Linear models strengths",
    "text": "Linear models strengths\n\nEasy to interpret (?)\nEasy to fit .. most of the time\nDesigned to match experiments and test specific hypotheses\nDecent approximation to natural phenomenon (c.f. Taylor expansions)"
  },
  {
    "objectID": "slides/01-01-intro/index.html#linear-model-weaknesses",
    "href": "slides/01-01-intro/index.html#linear-model-weaknesses",
    "title": "Introduction",
    "section": "Linear model weaknesses",
    "text": "Linear model weaknesses\n\nEasy to OVER interpret\nDoesn’t always fit if your data aren’t suitable\noverused and uncritically used\nLinear models don’t establish causes on their own\nNature has no straight lines"
  },
  {
    "objectID": "slides/offsets-simulation/index.html#link-functions-the-secret-advantage",
    "href": "slides/offsets-simulation/index.html#link-functions-the-secret-advantage",
    "title": "One weird trick: offsets in link functions",
    "section": "Link functions: the secret advantage",
    "text": "Link functions: the secret advantage\na GLM looks in a general way like this:\n\\[\n\\begin{align}\ny &\\sim \\text{Distribution}(\\mu, \\theta)\\\\\ng^{-1}(\\mu) &= \\beta_0 + \\beta_1x ...\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/offsets-simulation/index.html#link-functions-the-secret-advantage-1",
    "href": "slides/offsets-simulation/index.html#link-functions-the-secret-advantage-1",
    "title": "One weird trick: offsets in link functions",
    "section": "Link functions: the secret advantage",
    "text": "Link functions: the secret advantage\nFor example, a Poisson distribution looks like this:\n\\[\n\\begin{align}\ny &\\sim \\text{Poisson}(\\lambda\n)\\\\\n\\ln(\\lambda) &= \\beta_0 + \\beta_1x ...\n\\end{align}\n\\] but let’s just \\(\\alpha\\) as the log of the average (\\(\\lambda\\)) of the distribution.\n\\[\n\\begin{align}\ny &\\sim \\text{Poisson}(\\lambda\n)\\\\\n\\ln(\\lambda) &= \\alpha.\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/offsets-simulation/index.html#simulations",
    "href": "slides/offsets-simulation/index.html#simulations",
    "title": "One weird trick: offsets in link functions",
    "section": "Simulations",
    "text": "Simulations"
  },
  {
    "objectID": "slides/offsets-simulation/index.html#simulation-validation-poisson",
    "href": "slides/offsets-simulation/index.html#simulation-validation-poisson",
    "title": "One weird trick: offsets in link functions",
    "section": "Simulation validation – Poisson",
    "text": "Simulation validation – Poisson\nImaginary study of a little plant, with 4 plants / m2 and 30 samples\n\n\navg_plant_m2 &lt;- 4\nn_plots &lt;- 30\n# simulate\nset.seed(1234)\nplant_counts &lt;- rpois(\n  n_plots,\n  avg_plant_m2)\nhist(plant_counts)\n\n\n\n\n\n\n\n\n\n\nconfint(glm(plant_counts ~ 1, \n    family = poisson(link = \"identity\")))\n\n   2.5 %   97.5 % \n2.993295 4.358789"
  },
  {
    "objectID": "slides/offsets-simulation/index.html#simulation-poisson-on-the-log-scale",
    "href": "slides/offsets-simulation/index.html#simulation-poisson-on-the-log-scale",
    "title": "One weird trick: offsets in link functions",
    "section": "Simulation – Poisson on the log scale",
    "text": "Simulation – Poisson on the log scale\nWe’re more used to doing a Poisson model on the log scale, like this:\n\n\nlog_avg_plant_m2 &lt;- log(4)\nn_plots &lt;- 30\n# simulate\nset.seed(1234)\nplant_counts &lt;- rpois(\n  n_plots,\n  exp(log_avg_plant_m2))\nhist(plant_counts)\n\n\n\n\n\n\n\n\n\n\nconfint(glm(plant_counts ~ 1, \n    family = poisson(link = \"log\")))\n\n   2.5 %   97.5 % \n1.096352 1.472180 \n\nlog_avg_plant_m2\n\n[1] 1.386294"
  },
  {
    "objectID": "slides/offsets-simulation/index.html#simulation-validation-poisson-1",
    "href": "slides/offsets-simulation/index.html#simulation-validation-poisson-1",
    "title": "One weird trick: offsets in link functions",
    "section": "Simulation validation – Poisson",
    "text": "Simulation validation – Poisson\nNow imagine our plots are either 1, 2 or 5 m2\n\nsuppressPackageStartupMessages(library(tidyverse))\nset.seed(1618)\nfake_plant_data &lt;- tibble(\n  plot_size = rep(c(1, 2, 5), each = 10),\n  avg_plants = plot_size * exp(log_avg_plant_m2),\n  obs_plants = rpois(n = 30, lambda = avg_plants))\n\n# plot the histogram\nfake_plant_data |&gt; \n  ggplot(aes(x = obs_plants))+ \n  geom_histogram()"
  },
  {
    "objectID": "slides/offsets-simulation/index.html#simulation-validation-poisson-1-output",
    "href": "slides/offsets-simulation/index.html#simulation-validation-poisson-1-output",
    "title": "One weird trick: offsets in link functions",
    "section": "Simulation validation – Poisson",
    "text": "Simulation validation – Poisson"
  },
  {
    "objectID": "slides/offsets-simulation/index.html#wrong-model-wrong-answer",
    "href": "slides/offsets-simulation/index.html#wrong-model-wrong-answer",
    "title": "One weird trick: offsets in link functions",
    "section": "Wrong model, wrong answer",
    "text": "Wrong model, wrong answer\nWhat happens if we fit the wrong model to these data, ignoring sample size?\n\n# fit the wrong(!) model\nconfint(glm(obs_plants ~ 1, \n            data = fake_plant_data,\n    family = poisson(link = \"log\")))\n\n   2.5 %   97.5 % \n2.154985 2.384859 \n\n\nbut the true value of log_avg_plant_m2 is 1.3862944 !!\nput another way, the true density is 4 plants/m2 but the model thinks it is 9.7"
  },
  {
    "objectID": "slides/offsets-simulation/index.html#solution-use-an-offset-term",
    "href": "slides/offsets-simulation/index.html#solution-use-an-offset-term",
    "title": "One weird trick: offsets in link functions",
    "section": "Solution: use an offset term",
    "text": "Solution: use an offset term\n\n\nLet’s say that:\n\n\\(A\\) is the area of a plot in m2\n\\(\\lambda\\) is the average plants/m2\nthe log of average plants/m2 is \\(\\alpha\\), so \\(\\alpha = \\ln(\\lambda)\\)\n\n\n\\[\n\\begin{align}\ny &\\sim \\text{Poisson}(A\\lambda) \\\\\n&\\sim \\text{Poisson}(Ae^\\alpha) \\\\\n&\\sim \\text{Poisson}(e^{\\ln{A}}e^\\alpha) \\\\\n&\\sim \\text{Poisson}(e^{\\alpha + \\ln{A}}) \\\\\n\\end{align}\n\\] So with the log link, the linear predictor becomes\n\\[\n\\alpha + \\ln{A}\n\\]"
  },
  {
    "objectID": "slides/offsets-simulation/index.html#using-offset-in-a-linear-model",
    "href": "slides/offsets-simulation/index.html#using-offset-in-a-linear-model",
    "title": "One weird trick: offsets in link functions",
    "section": "using offset in a linear model",
    "text": "using offset in a linear model\nWe need to add \\(\\ln{A}\\) to our model but we want a coefficient of exactly 1 – no need for a seperate slope term!\nNOTE you need to perform the log operation yourself!\n\nconfint(glm(obs_plants ~ 1 + offset(log(plot_size)), \n            data = fake_plant_data,\n    family = poisson(link = \"log\")))\n\n   2.5 %   97.5 % \n1.174155 1.404030 \n\n\nOnce again the true value of log_avg_plant_m2 is 1.3862944"
  },
  {
    "objectID": "todo.html",
    "href": "todo.html",
    "title": "MMIE02",
    "section": "",
    "text": "ordinal simulations\npredicting a new group – or skip it? how to summarize it quickly\ninteractions – should have an example of how to look at that.\nMite data count glm?\ncourse bibliography\nhow to use dHarma"
  },
  {
    "objectID": "exercises/overdispersion/index.html",
    "href": "exercises/overdispersion/index.html",
    "title": "Overdispersion: or, how to use predictions to find problems",
    "section": "",
    "text": "library(rstanarm)\n\nLoading required package: Rcpp\n\n\nThis is rstanarm version 2.32.2\n\n\n- See https://mc-stan.org/rstanarm/articles/priors for changes to default priors!\n\n\n- Default priors may change, so it's safest to specify priors, even if equivalent to the defaults.\n\n\n- For execution on a local, multicore CPU with excess RAM we recommend calling\n\n\n  options(mc.cores = parallel::detectCores())\n\nsuppressPackageStartupMessages(library(tidyverse))\nlibrary(tidybayes)\nIn this exercise we’ll look at two ways to think about fixing overdispersion in a count model, and also practice making prediction figures to diagnose a model."
  },
  {
    "objectID": "exercises/overdispersion/index.html#observation-level-random-effects-mite-abundance",
    "href": "exercises/overdispersion/index.html#observation-level-random-effects-mite-abundance",
    "title": "Overdispersion: or, how to use predictions to find problems",
    "section": "Observation-level random effects: Mite abundance",
    "text": "Observation-level random effects: Mite abundance\nHere’s a partially complete model for abundance over time\n\\[\n\\begin{align}\n\\text{N}_i &\\sim \\text{Poisson}(e^a) \\\\\na &= \\bar\\beta + \\beta_{\\text{water}} \\cdot \\text{water}_i \\\\\n\\bar\\beta &\\sim \\text{Normal}(?, ?) \\\\\n\\beta_{\\text{water}} &\\sim \\text{Normal}(?, ?) \\\\\n\\end{align}\n\\]\n\n\n\n\n\n\nEXERCISE\n\n\n\nSimulate from this model, and look at your simulations to decide on a reasonable prior for the data.\n\n\n\n\n\n\n\n\nSOLUTION\n\n\n\n\n\n\nn &lt;- 30\nwater &lt;- seq(from = -5, to = 5, length.out = n)\n\nb0 &lt;- rnorm(1, mean = log(17), sd = .3)\nb1 &lt;- rnorm(1, mean = 0, sd = .2)\n\nS &lt;- rpois(n, lambda = exp(b0 + b1*water))\nplot(water, S)\n\n\n\n\n\n\n\n\n\n\n\n\nData preparation & visualization\nFirst we need to load and prepare the data:\n\ndata(mite, package = \"vegan\")\ndata(\"mite.env\", package = \"vegan\")\n\n# combine data and environment\n\nmite_data_long &lt;- mite |&gt; \n  tibble::rownames_to_column(var = \"site_id\") |&gt; \n  bind_cols(mite.env) |&gt; \n  pivot_longer(Brachy:Trimalc2,\n               names_to = \"spp\", values_to = \"abd\")\n\nFirst let’s transform the mite dataset into a dataframe of total community abundance (N) per site. We’ll also standardize the water content while we’re at it:\n\nmite_community_abd &lt;- mite_data_long |&gt; \n  group_by(site_id, WatrCont) |&gt; \n  summarize(N = sum(abd)) |&gt;\n  ungroup() |&gt; \n  mutate(water_c = (WatrCont - mean(WatrCont))/100)\n\n`summarise()` has grouped output by 'site_id'. You can override using the\n`.groups` argument.\n\nknitr::kable(head(mite_community_abd))\n\n\n\n\nsite_id\nWatrCont\nN\nwater_c\n\n\n\n\n1\n350.15\n140\n-0.6048571\n\n\n10\n220.73\n166\n-1.8990571\n\n\n11\n134.13\n216\n-2.7650571\n\n\n12\n405.91\n213\n-0.0472571\n\n\n13\n243.70\n177\n-1.6693571\n\n\n14\n239.51\n269\n-1.7112571\n\n\n\n\n\nWe get a nice histogram of community abundance, and a clear negative relationship with water volume:\nmite_community_abd |&gt; \n  ggplot(aes(x = N)) + \n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\nmite_community_abd |&gt; \n  ggplot(aes(x = water_c, y = N)) + \n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\n## data! \nglimpse(mite_community_abd)\n\nRows: 70\nColumns: 4\n$ site_id  &lt;chr&gt; \"1\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"1…\n$ WatrCont &lt;dbl&gt; 350.15, 220.73, 134.13, 405.91, 243.70, 239.51, 350.64, 321.8…\n$ N        &lt;int&gt; 140, 166, 216, 213, 177, 269, 100, 97, 90, 118, 118, 268, 184…\n$ water_c  &lt;dbl&gt; -0.60485714, -1.89905714, -2.76505714, -0.04725714, -1.669357…\n\nmite_stan &lt;- stan_glm(\n  N ~ 1 + water_c,\n  data = mite_community_abd,\n  family = poisson(link = \"log\"),\n  prior_intercept = normal(2.8, 0.3, autoscale = FALSE),\n  prior = normal(0, 0.2, autoscale = FALSE),\n  refresh = 0\n)\n\n\nsummary(mite_stan)\n\n\nModel Info:\n function:     stan_glm\n family:       poisson [log]\n formula:      N ~ 1 + water_c\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 70\n predictors:   2\n\nEstimates:\n              mean   sd   10%   50%   90%\n(Intercept) 4.9    0.0  4.9   4.9   5.0  \nwater_c     0.0    0.0  0.0   0.0   0.0  \n\nFit Diagnostics:\n           mean   sd    10%   50%   90%\nmean_PPD 139.7    2.0 137.1 139.7 142.3\n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.0  1.0  2660 \nwater_c       0.0  1.0  2369 \nmean_PPD      0.0  1.0  3487 \nlog-posterior 0.0  1.0  1491 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n\n\nNote that the water coefficient is quite positive – not very consistent with our figure!\n\nbayesplot::mcmc_areas(mite_stan, \"water_c\")\n\n\n\n\n\n\n\n\nPlot the predicted line:\n\nlibrary(modelr)\nmite_community_abd |&gt; \n  modelr::data_grid(water_c = seq_range(water_c, n = 7)) |&gt; \n  tidybayes::add_predicted_rvars(mite_stan) |&gt; \n  ggplot(aes(x = water_c, ydist = .prediction)) + \n  stat_dist_lineribbon() + \n  scale_fill_brewer(palette = \"Greens\", direction=-1) + \n  geom_point(aes(x = water_c, y = N),\n             data = mite_community_abd, inherit.aes = FALSE)\n\n\n\n\n\n\n\n\n\npp_check(mite_stan, type = \"dens_overlay\")\n\nWarning: The following arguments were unrecognized and ignored: type"
  },
  {
    "objectID": "exercises/overdispersion/index.html#improving-the-poisson-model-with-an-observation-level-random-effect",
    "href": "exercises/overdispersion/index.html#improving-the-poisson-model-with-an-observation-level-random-effect",
    "title": "Overdispersion: or, how to use predictions to find problems",
    "section": "Improving the Poisson model with an observation-level random effect",
    "text": "Improving the Poisson model with an observation-level random effect\nObservation-level random effects add flexibility to a Poisson model and allow us to model the additional variation.\n\n## data! \nglimpse(mite_community_abd)\n\nRows: 70\nColumns: 4\n$ site_id  &lt;chr&gt; \"1\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"1…\n$ WatrCont &lt;dbl&gt; 350.15, 220.73, 134.13, 405.91, 243.70, 239.51, 350.64, 321.8…\n$ N        &lt;int&gt; 140, 166, 216, 213, 177, 269, 100, 97, 90, 118, 118, 268, 184…\n$ water_c  &lt;dbl&gt; -0.60485714, -1.89905714, -2.76505714, -0.04725714, -1.669357…\n\nmite_stan_olre &lt;- stan_glmer(\n  N ~ 1 + water_c + (1|site_id),\n  data = mite_community_abd,\n  family = poisson(link = \"log\"),\n  prior_intercept = normal(2.8, 0.3, autoscale = FALSE),\n  prior = normal(0, 0.2, autoscale = FALSE),\n  prior_covariance = decov(shape = 1, scale = .5),\n  refresh = 0\n)\n\nWarning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#bulk-ess\n\n\nWarning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#tail-ess\n\nsummary(mite_stan_olre)\n\n\nModel Info:\n function:     stan_glmer\n family:       poisson [log]\n formula:      N ~ 1 + water_c + (1 | site_id)\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 70\n groups:       site_id (70)\n\nEstimates:\n                                         mean   sd   10%   50%   90%\n(Intercept)                             4.7    0.1  4.6   4.7   4.8 \nwater_c                                -0.1    0.0 -0.2  -0.1   0.0 \nb[(Intercept) site_id:1]                0.2    0.1  0.0   0.2   0.3 \nb[(Intercept) site_id:10]               0.2    0.1  0.0   0.2   0.4 \nb[(Intercept) site_id:11]               0.4    0.2  0.2   0.4   0.6 \nb[(Intercept) site_id:12]               0.6    0.1  0.5   0.6   0.8 \nb[(Intercept) site_id:13]               0.3    0.1  0.1   0.3   0.5 \nb[(Intercept) site_id:14]               0.7    0.1  0.6   0.7   0.9 \nb[(Intercept) site_id:15]              -0.2    0.1 -0.3  -0.2   0.0 \nb[(Intercept) site_id:16]              -0.2    0.1 -0.4  -0.2  -0.1 \nb[(Intercept) site_id:17]              -0.3    0.1 -0.5  -0.3  -0.1 \nb[(Intercept) site_id:18]              -0.1    0.1 -0.2  -0.1   0.1 \nb[(Intercept) site_id:19]               0.0    0.1 -0.1   0.0   0.2 \nb[(Intercept) site_id:2]                0.9    0.1  0.8   0.9   1.0 \nb[(Intercept) site_id:20]               0.2    0.2  0.0   0.2   0.4 \nb[(Intercept) site_id:21]              -0.2    0.2 -0.4  -0.2   0.0 \nb[(Intercept) site_id:22]               0.2    0.1  0.0   0.2   0.4 \nb[(Intercept) site_id:23]              -0.3    0.1 -0.5  -0.3  -0.2 \nb[(Intercept) site_id:24]              -0.4    0.1 -0.6  -0.4  -0.3 \nb[(Intercept) site_id:25]               0.0    0.1 -0.2   0.0   0.1 \nb[(Intercept) site_id:26]               0.2    0.1  0.0   0.2   0.3 \nb[(Intercept) site_id:27]               0.2    0.2  0.0   0.2   0.4 \nb[(Intercept) site_id:28]              -0.1    0.1 -0.2   0.0   0.1 \nb[(Intercept) site_id:29]              -0.1    0.1 -0.2  -0.1   0.1 \nb[(Intercept) site_id:3]                0.5    0.1  0.3   0.5   0.6 \nb[(Intercept) site_id:30]              -0.1    0.1 -0.3  -0.1   0.0 \nb[(Intercept) site_id:31]               0.2    0.1  0.1   0.2   0.4 \nb[(Intercept) site_id:32]              -0.3    0.1 -0.5  -0.3  -0.1 \nb[(Intercept) site_id:33]               0.2    0.1  0.0   0.2   0.3 \nb[(Intercept) site_id:34]               0.5    0.1  0.4   0.5   0.6 \nb[(Intercept) site_id:35]              -0.1    0.1 -0.3  -0.1   0.1 \nb[(Intercept) site_id:36]               0.2    0.1  0.0   0.2   0.3 \nb[(Intercept) site_id:37]               0.2    0.1  0.1   0.2   0.3 \nb[(Intercept) site_id:38]               0.6    0.1  0.5   0.6   0.8 \nb[(Intercept) site_id:39]               0.1    0.2 -0.1   0.1   0.4 \nb[(Intercept) site_id:4]                0.9    0.1  0.8   0.9   1.0 \nb[(Intercept) site_id:40]               0.3    0.1  0.2   0.3   0.5 \nb[(Intercept) site_id:41]              -0.2    0.1 -0.4  -0.2  -0.1 \nb[(Intercept) site_id:42]               0.2    0.1  0.1   0.2   0.3 \nb[(Intercept) site_id:43]               0.5    0.1  0.3   0.5   0.6 \nb[(Intercept) site_id:44]              -0.3    0.2 -0.5  -0.3   0.0 \nb[(Intercept) site_id:45]               0.5    0.1  0.3   0.5   0.6 \nb[(Intercept) site_id:46]               0.4    0.1  0.2   0.4   0.5 \nb[(Intercept) site_id:47]               0.0    0.1 -0.2   0.0   0.2 \nb[(Intercept) site_id:48]               0.1    0.1  0.0   0.1   0.3 \nb[(Intercept) site_id:49]              -0.1    0.1 -0.2  -0.1   0.1 \nb[(Intercept) site_id:5]                0.4    0.1  0.2   0.4   0.5 \nb[(Intercept) site_id:50]               0.3    0.1  0.2   0.3   0.4 \nb[(Intercept) site_id:51]              -0.1    0.1 -0.3  -0.1   0.0 \nb[(Intercept) site_id:52]               0.2    0.1  0.0   0.2   0.4 \nb[(Intercept) site_id:53]               0.3    0.1  0.1   0.3   0.4 \nb[(Intercept) site_id:54]              -0.6    0.2 -0.9  -0.6  -0.4 \nb[(Intercept) site_id:55]              -0.6    0.1 -0.8  -0.6  -0.4 \nb[(Intercept) site_id:56]               0.1    0.1 -0.1   0.1   0.3 \nb[(Intercept) site_id:57]              -1.9    0.3 -2.2  -1.9  -1.5 \nb[(Intercept) site_id:58]               0.0    0.1 -0.1   0.0   0.2 \nb[(Intercept) site_id:59]               0.0    0.2 -0.2   0.0   0.3 \nb[(Intercept) site_id:6]                0.5    0.1  0.4   0.5   0.7 \nb[(Intercept) site_id:60]               0.2    0.1  0.1   0.2   0.3 \nb[(Intercept) site_id:61]              -0.7    0.2 -0.9  -0.7  -0.4 \nb[(Intercept) site_id:62]              -1.6    0.2 -1.9  -1.6  -1.3 \nb[(Intercept) site_id:63]              -0.2    0.1 -0.3  -0.2   0.0 \nb[(Intercept) site_id:64]              -0.2    0.1 -0.4  -0.2   0.0 \nb[(Intercept) site_id:65]               0.1    0.1 -0.1   0.1   0.2 \nb[(Intercept) site_id:66]               0.2    0.1  0.0   0.2   0.3 \nb[(Intercept) site_id:67]               2.4    0.2  2.1   2.4   2.7 \nb[(Intercept) site_id:68]               0.2    0.1  0.0   0.2   0.4 \nb[(Intercept) site_id:69]               0.6    0.1  0.4   0.6   0.7 \nb[(Intercept) site_id:7]                0.3    0.1  0.2   0.3   0.5 \nb[(Intercept) site_id:70]               0.2    0.1  0.0   0.2   0.4 \nb[(Intercept) site_id:8]                0.0    0.1 -0.2   0.0   0.1 \nb[(Intercept) site_id:9]                0.0    0.1 -0.1   0.0   0.2 \nSigma[site_id:(Intercept),(Intercept)]  0.3    0.1  0.2   0.3   0.4 \n\nFit Diagnostics:\n           mean   sd    10%   50%   90%\nmean_PPD 139.7    2.0 137.1 139.7 142.3\n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n                                       mcse Rhat n_eff\n(Intercept)                            0.0  1.0   222 \nwater_c                                0.0  1.0   328 \nb[(Intercept) site_id:1]               0.0  1.0   433 \nb[(Intercept) site_id:10]              0.0  1.0   343 \nb[(Intercept) site_id:11]              0.0  1.0   327 \nb[(Intercept) site_id:12]              0.0  1.0   435 \nb[(Intercept) site_id:13]              0.0  1.0   335 \nb[(Intercept) site_id:14]              0.0  1.0   299 \nb[(Intercept) site_id:15]              0.0  1.0   497 \nb[(Intercept) site_id:16]              0.0  1.0   569 \nb[(Intercept) site_id:17]              0.0  1.0   517 \nb[(Intercept) site_id:18]              0.0  1.0   420 \nb[(Intercept) site_id:19]              0.0  1.0   531 \nb[(Intercept) site_id:2]               0.0  1.0   409 \nb[(Intercept) site_id:20]              0.0  1.0   314 \nb[(Intercept) site_id:21]              0.0  1.0   385 \nb[(Intercept) site_id:22]              0.0  1.0   313 \nb[(Intercept) site_id:23]              0.0  1.0   790 \nb[(Intercept) site_id:24]              0.0  1.0   619 \nb[(Intercept) site_id:25]              0.0  1.0   401 \nb[(Intercept) site_id:26]              0.0  1.0   546 \nb[(Intercept) site_id:27]              0.0  1.0   327 \nb[(Intercept) site_id:28]              0.0  1.0   519 \nb[(Intercept) site_id:29]              0.0  1.0   495 \nb[(Intercept) site_id:3]               0.0  1.0   436 \nb[(Intercept) site_id:30]              0.0  1.0   610 \nb[(Intercept) site_id:31]              0.0  1.0   544 \nb[(Intercept) site_id:32]              0.0  1.0   502 \nb[(Intercept) site_id:33]              0.0  1.0   539 \nb[(Intercept) site_id:34]              0.0  1.0   391 \nb[(Intercept) site_id:35]              0.0  1.0   455 \nb[(Intercept) site_id:36]              0.0  1.0   483 \nb[(Intercept) site_id:37]              0.0  1.0   508 \nb[(Intercept) site_id:38]              0.0  1.0   465 \nb[(Intercept) site_id:39]              0.0  1.0   430 \nb[(Intercept) site_id:4]               0.0  1.0   358 \nb[(Intercept) site_id:40]              0.0  1.0   494 \nb[(Intercept) site_id:41]              0.0  1.0   712 \nb[(Intercept) site_id:42]              0.0  1.0   441 \nb[(Intercept) site_id:43]              0.0  1.0   467 \nb[(Intercept) site_id:44]              0.0  1.0   478 \nb[(Intercept) site_id:45]              0.0  1.0   445 \nb[(Intercept) site_id:46]              0.0  1.0   447 \nb[(Intercept) site_id:47]              0.0  1.0   454 \nb[(Intercept) site_id:48]              0.0  1.0   550 \nb[(Intercept) site_id:49]              0.0  1.0   638 \nb[(Intercept) site_id:5]               0.0  1.0   338 \nb[(Intercept) site_id:50]              0.0  1.0   508 \nb[(Intercept) site_id:51]              0.0  1.0   635 \nb[(Intercept) site_id:52]              0.0  1.0   466 \nb[(Intercept) site_id:53]              0.0  1.0   542 \nb[(Intercept) site_id:54]              0.0  1.0   896 \nb[(Intercept) site_id:55]              0.0  1.0   806 \nb[(Intercept) site_id:56]              0.0  1.0   556 \nb[(Intercept) site_id:57]              0.0  1.0  1365 \nb[(Intercept) site_id:58]              0.0  1.0   521 \nb[(Intercept) site_id:59]              0.0  1.0   439 \nb[(Intercept) site_id:6]               0.0  1.0   347 \nb[(Intercept) site_id:60]              0.0  1.0   542 \nb[(Intercept) site_id:61]              0.0  1.0   724 \nb[(Intercept) site_id:62]              0.0  1.0  1190 \nb[(Intercept) site_id:63]              0.0  1.0   606 \nb[(Intercept) site_id:64]              0.0  1.0   751 \nb[(Intercept) site_id:65]              0.0  1.0   618 \nb[(Intercept) site_id:66]              0.0  1.0   514 \nb[(Intercept) site_id:67]              0.0  1.0   334 \nb[(Intercept) site_id:68]              0.0  1.0   476 \nb[(Intercept) site_id:69]              0.0  1.0   440 \nb[(Intercept) site_id:7]               0.0  1.0   435 \nb[(Intercept) site_id:70]              0.0  1.0   552 \nb[(Intercept) site_id:8]               0.0  1.0   375 \nb[(Intercept) site_id:9]               0.0  1.0   421 \nSigma[site_id:(Intercept),(Intercept)] 0.0  1.0   398 \nmean_PPD                               0.0  1.0  4089 \nlog-posterior                          0.4  1.0   516 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n\n\n\nlibrary(modelr)\nmite_community_abd |&gt; \n  modelr::data_grid(water_c = seq_range(water_c, n = 7),\n                    site_id = \"Ireland\") |&gt; \n  tidybayes::add_predicted_rvars(mite_stan_olre,\n                                 allow_new_levels = TRUE,\n                                 sample_new_levels = \"gaussian\") |&gt; \n  ggplot(aes(x = water_c, ydist = .prediction)) + \n  stat_dist_lineribbon() + \n  scale_fill_brewer(palette = \"Greens\", direction=-1) + \n  geom_point(aes(x = water_c, y = N),\n             data = mite_community_abd, inherit.aes = FALSE)\n\n\n\n\n\n\n\n\n\npp_check(mite_stan_olre)"
  },
  {
    "objectID": "exercises/overdispersion/index.html#negative-binomial-regression",
    "href": "exercises/overdispersion/index.html#negative-binomial-regression",
    "title": "Overdispersion: or, how to use predictions to find problems",
    "section": "Negative binomial regression",
    "text": "Negative binomial regression\nAnother way to model variable count data is via the Negative Binomial distribution. Read more about the distribution on Wikipedia, check the way the distribution is parameterized in brms , and look at what link family are used via ?brms::brmsfamily which is also online here\n\nmite_negbin_stan_prior &lt;- stan_glm(\n  N ~ 1 + water_c,\n  data = mite_community_abd,\n  family = neg_binomial_2(link = \"log\"),\n  prior_intercept = normal(2.8, 0.3, autoscale = FALSE),\n  prior = normal(0, 0.2, autoscale = FALSE),\n  prior_aux = normal(3, 1, autoscale = FALSE),  # shape (log scale in brms ≈ aux in rstanarm)\n  prior_PD = FALSE,\n  refresh = 0\n)\n\nmite_community_abd |&gt;\n  modelr::data_grid(water_c = seq_range(water_c, n = 7)) |&gt;\n  tidybayes::add_predicted_draws(mite_negbin_stan_prior, ndraws = 12) |&gt;\n  ggplot(aes(x = water_c, y = .prediction)) +\n  geom_point() +\n  facet_wrap(~.draw)\n\n\n\n\n\n\n\n\nNow run the model for real\n\nmite_negbin_stan &lt;- stan_glm(\n  N ~ 1 + water_c,\n  data = mite_community_abd,\n  family = neg_binomial_2(link = \"log\"),\n  prior_intercept = normal(2.8, 0.3, autoscale = FALSE),\n  prior = normal(0, 0.2, autoscale = FALSE),\n  prior_aux = normal(3, 1, autoscale = FALSE),  # shape (log scale in brms ≈ aux in rstanarm)\n  prior_PD = FALSE,\n  refresh = 0\n)\n\nAnd plot the predictions\n\nmite_community_abd |&gt; \n  modelr::data_grid(water_c = seq_range(water_c, n = 7),\n                    site_id = \"Ireland\") |&gt; \n  tidybayes::add_predicted_rvars(mite_negbin_stan,\n                                 allow_new_levels = TRUE,\n                                 sample_new_levels = \"gaussian\") |&gt; \n  ggplot(aes(x = water_c, ydist = .prediction)) + \n  stat_dist_lineribbon() + \n  scale_fill_brewer(palette = \"Greens\", direction=-1) + \n  geom_point(aes(x = water_c, y = N),\n             data = mite_community_abd, inherit.aes = FALSE)\n\n\n\n\n\n\n\n\n\npp_check(mite_negbin_stan)\n\n\n\n\n\n\n\n\nAnd we can also compare these three models, using\n\nrstanarm::loo_compare(\n  loo(mite_stan), \n  loo(mite_stan_olre), \n  loo(mite_negbin_stan)\n)\n\nWarning: Found 3 observation(s) with a pareto_k &gt; 0.7. We recommend calling 'loo' again with argument 'k_threshold = 0.7' in order to calculate the ELPD without the assumption that these observations are negligible. This will refit the model 3 times to compute the ELPDs for the problematic observations directly.\n\n\nWarning: Found 64 observations with a pareto_k &gt; 0.7. With this many problematic observations we recommend calling 'kfold' with argument 'K=10' to perform 10-fold cross-validation rather than LOO.\n\n\nWarning: Found 1 observation(s) with a pareto_k &gt; 0.7. We recommend calling 'loo' again with argument 'k_threshold = 0.7' in order to calculate the ELPD without the assumption that these observations are negligible. This will refit the model 1 times to compute the ELPDs for the problematic observations directly.\n\n\n                 elpd_diff se_diff\nmite_stan_olre       0.0       0.0\nmite_negbin_stan   -85.8      15.9\nmite_stan        -1514.0     723.3\n\n\n\nExercise\nRun a negative binomial regression, and compare to the example above."
  },
  {
    "objectID": "exercises/species_random/index.html",
    "href": "exercises/species_random/index.html",
    "title": "Random groups with not many levels: species as random effects",
    "section": "",
    "text": "We’ll use the penguin data to show how random effects can be used:\nlibrary(ggplot2)\nsuppressPackageStartupMessages(library(tidyverse))\nlibrary(tidybayes)\nsuppressPackageStartupMessages(library(rstanarm))"
  },
  {
    "objectID": "exercises/species_random/index.html#statistical-models-of-penguin-bill-morphology.",
    "href": "exercises/species_random/index.html#statistical-models-of-penguin-bill-morphology.",
    "title": "Random groups with not many levels: species as random effects",
    "section": "Statistical models of Penguin bill morphology.",
    "text": "Statistical models of Penguin bill morphology.\nWe’ll start with a review of the Simpson’s Paradox demonstration.\nLet’s begin with plotting the data:\n\npenguins |&gt; \n  ggplot(aes(x = bill_len, y = bill_dep)) + \n  geom_point() + \n  stat_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nBill depth (mm) as predicted by bill length (mm) across the entire palmerpenguins dataset."
  },
  {
    "objectID": "exercises/species_random/index.html#data-preparation",
    "href": "exercises/species_random/index.html#data-preparation",
    "title": "Random groups with not many levels: species as random effects",
    "section": "Data preparation",
    "text": "Data preparation\n\n## get data ready\nbill_len_centered &lt;- with(penguins,\n                          bill_len - mean(bill_len,\n                                                na.rm = TRUE))\npeng_dep_len_df &lt;- penguins |&gt; \n  tidyr::drop_na(bill_dep, bill_len) |&gt; \n  mutate(bill_len_cen = bill_len - mean(bill_len))"
  },
  {
    "objectID": "exercises/species_random/index.html#model-fitting",
    "href": "exercises/species_random/index.html#model-fitting",
    "title": "Random groups with not many levels: species as random effects",
    "section": "Model fitting",
    "text": "Model fitting\nWe’ll fit three different models here and then compare them.\nFirst, a model with no species in it:\n\n## fit model with rstanarm\nnormal_reg_stan &lt;- stan_glm(\n  bill_dep ~ 1 + bill_len_cen,\n  data = peng_dep_len_df,\n  family = gaussian(),\n\n  # priors\n  prior = normal(0, 0.5),          # slope (bill_len_cen)\n  prior_intercept = normal(17, 2), # intercept\n  prior_aux = exponential(0.5),    # sigma\n\n  chains = 4,\n  cores = 4,\n  iter = 2000,\n  refresh = 0,\n  seed = 525600\n)\n\nnormal_reg_stan\n\nstan_glm\n family:       gaussian [identity]\n formula:      bill_dep ~ 1 + bill_len_cen\n observations: 342\n predictors:   2\n------\n             Median MAD_SD\n(Intercept)  17.1    0.1  \nbill_len_cen -0.1    0.0  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 1.9    0.1   \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nThen, one that adds species as a fixed effect:\n\n## fit model with rstanarm\nnormal_reg_spp_stan &lt;- stan_glm(\n  bill_dep ~ 1 + bill_len_cen + species,\n  data = peng_dep_len_df,\n  family = gaussian(),\n\n  # priors\n  prior = normal(0, 0.5),          # slope (bill_len_cen)\n  prior_intercept = normal(17, 2), # intercept\n  prior_aux = exponential(0.5),    # sigma\n\n  chains = 4,\n  cores = 4,\n  iter = 2000,\n  refresh = 0,\n  seed = 525600\n)\n\nnormal_reg_spp_stan\n\nstan_glm\n family:       gaussian [identity]\n formula:      bill_dep ~ 1 + bill_len_cen + species\n observations: 342\n predictors:   4\n------\n                 Median MAD_SD\n(Intercept)      18.9    0.1  \nbill_len_cen      0.1    0.0  \nspeciesChinstrap -1.1    0.2  \nspeciesGentoo    -4.3    0.2  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 1.0    0.0   \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nFinally, a random effect for species. With only three levels, this random effect will probably be exactly like the fixed effect above:\n\n## fit model with rstanarm\nnormal_reg_hier_spp_stan &lt;- stan_glmer(\n  bill_dep ~ 1 + bill_len_cen + (1|species),\n  data = peng_dep_len_df,\n  family = gaussian(),\n\n  # priors\n  prior = normal(0, 0.5),          # slope (bill_len_cen)\n  prior_intercept = normal(17, 2), # intercept\n  prior_aux = exponential(2),    # sigma\n\n  chains = 4,\n  cores = 4,\n  iter = 2000,\n  refresh = 1000,\n  seed = 525600\n)\n\nnormal_reg_hier_spp_stan"
  },
  {
    "objectID": "exercises/species_random/index.html#pairwise-differences",
    "href": "exercises/species_random/index.html#pairwise-differences",
    "title": "Random groups with not many levels: species as random effects",
    "section": "Pairwise differences:",
    "text": "Pairwise differences:\n\nget_variables(normal_reg_hier_spp_stan)\n\n [1] \"(Intercept)\"                           \n [2] \"bill_len_cen\"                          \n [3] \"b[(Intercept) species:Adelie]\"         \n [4] \"b[(Intercept) species:Chinstrap]\"      \n [5] \"b[(Intercept) species:Gentoo]\"         \n [6] \"sigma\"                                 \n [7] \"Sigma[species:(Intercept),(Intercept)]\"\n [8] \"accept_stat__\"                         \n [9] \"stepsize__\"                            \n[10] \"treedepth__\"                           \n[11] \"n_leapfrog__\"                          \n[12] \"divergent__\"                           \n[13] \"energy__\"                              \n\nnormal_reg_hier_spp_stan %&gt;%\n  spread_draws(b[term, spp]) |&gt; \n  compare_levels(b, by = spp) |&gt; \n  ggplot(aes(x = b, y = spp)) + \n  stat_halfeye()"
  },
  {
    "objectID": "exercises/species_random/index.html#model-comparison",
    "href": "exercises/species_random/index.html#model-comparison",
    "title": "Random groups with not many levels: species as random effects",
    "section": "Model comparison",
    "text": "Model comparison\nTry comparing all three models:\n\nrstanarm::loo_compare(\n  loo(normal_reg_stan), \n  loo(normal_reg_spp_stan), \n  loo(normal_reg_hier_spp_stan)\n)\n\n                         elpd_diff se_diff\nnormal_reg_hier_spp_stan    0.0       0.0 \nnormal_reg_spp_stan        -9.1       4.1 \nnormal_reg_stan          -238.6      16.1 \n\n\nLet’s look at the actual coefficients involved\n\nget_variables(normal_reg_hier_spp_stan)\n\n [1] \"(Intercept)\"                           \n [2] \"bill_len_cen\"                          \n [3] \"b[(Intercept) species:Adelie]\"         \n [4] \"b[(Intercept) species:Chinstrap]\"      \n [5] \"b[(Intercept) species:Gentoo]\"         \n [6] \"sigma\"                                 \n [7] \"Sigma[species:(Intercept),(Intercept)]\"\n [8] \"accept_stat__\"                         \n [9] \"stepsize__\"                            \n[10] \"treedepth__\"                           \n[11] \"n_leapfrog__\"                          \n[12] \"divergent__\"                           \n[13] \"energy__\"                              \n\nhier_avg &lt;- normal_reg_hier_spp_stan %&gt;%\n  spread_rvars(`(Intercept)`, b[spp]) |&gt; \n  mutate(spp_avg = `(Intercept)` + b,\n         spp = str_replace(spp, \"\\\\(Intercept\\\\) species:\", \"\"),\n         model = \"hierarchical\") |&gt; \n  select(model, spp, spp_avg)\n\nget_variables(normal_reg_spp_stan) \n\n [1] \"(Intercept)\"      \"bill_len_cen\"     \"speciesChinstrap\" \"speciesGentoo\"   \n [5] \"sigma\"            \"accept_stat__\"    \"stepsize__\"       \"treedepth__\"     \n [9] \"n_leapfrog__\"     \"divergent__\"      \"energy__\"        \n\nsimple_avg &lt;- normal_reg_spp_stan |&gt; \n  spread_rvars(`(Intercept)`, speciesChinstrap, speciesGentoo) |&gt; \n  mutate(Adelie = `(Intercept)`,\n         Chinstrap = Adelie + speciesChinstrap,\n         Gentoo = Adelie + speciesGentoo) |&gt; \n  select(Adelie, Chinstrap, Gentoo) |&gt; \n  tidyr::pivot_longer(everything(), names_to = \"spp\", values_to = \"spp_avg\") |&gt; \n  mutate(model = \"simple\", .before = \"spp\")\n\nbind_rows(simple_avg, hier_avg) |&gt; \n  ggplot(aes(x= spp, ydist = spp_avg, col = model)) + \n  stat_halfeye(position = position_dodge(width = .5))"
  },
  {
    "objectID": "exercises/species_random/index.html#making-novel-predictions",
    "href": "exercises/species_random/index.html#making-novel-predictions",
    "title": "Random groups with not many levels: species as random effects",
    "section": "Making novel predictions",
    "text": "Making novel predictions\n\nlibrary(modelr)\n\nfake_penguins &lt;- expand.grid(\n  bill_len_cen = modelr::seq_range(peng_dep_len_df$bill_len_cen, n = 4),\n  species = c(\"Hollander\",levels(peng_dep_len_df$species))\n)\n\nadd_predicted_rvars(fake_penguins, normal_reg_hier_spp_stan) |&gt; \n  ggplot(aes(x = bill_len_cen, ydist = .prediction)) + \n  tidybayes::stat_lineribbon() + \n  facet_wrap(~species)"
  },
  {
    "objectID": "exercises/discrete_predictor/index.html",
    "href": "exercises/discrete_predictor/index.html",
    "title": "Palmer penguins and discrete predictors",
    "section": "",
    "text": "In this section we’re going to look at a simple model with a single predictor variable which divides the dataset into categories. In this example, categories are treated as “fixed” effects."
  },
  {
    "objectID": "exercises/discrete_predictor/index.html#load-packages-and-data",
    "href": "exercises/discrete_predictor/index.html#load-packages-and-data",
    "title": "Palmer penguins and discrete predictors",
    "section": "Load packages and data",
    "text": "Load packages and data\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.6\n✔ forcats   1.0.1     ✔ stringr   1.6.0\n✔ ggplot2   4.0.1     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.2.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(rstanarm)\n\nLoading required package: Rcpp\nThis is rstanarm version 2.32.2\n- See https://mc-stan.org/rstanarm/articles/priors for changes to default priors!\n- Default priors may change, so it's safest to specify priors, even if equivalent to the defaults.\n- For execution on a local, multicore CPU with excess RAM we recommend calling\n  options(mc.cores = parallel::detectCores())\n\nlibrary(lme4)\n\nLoading required package: Matrix\n\nAttaching package: 'Matrix'\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\nlibrary(tidybayes)"
  },
  {
    "objectID": "exercises/discrete_predictor/index.html#data-exploration",
    "href": "exercises/discrete_predictor/index.html#data-exploration",
    "title": "Palmer penguins and discrete predictors",
    "section": "Data exploration",
    "text": "Data exploration\nLet’s start by taking a look at the Palmer Penguin dataset, specifically the distribution of observations of bill size.\n\npenguins |&gt; \n  ggplot(aes(x=bill_dep)) + \n  geom_histogram(binwidth = .5)\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\nHistogram of bill depth for all the penguins in the Palmer Penguin dataset.\n\n\n\n\nThere’s quite a lot of variation in these measurements, with a suggestion of perhaps more than one peak in this distribution.\nThere’s also some NA values – we’ll drop them before we move on:\n\npenguins_noNAbill &lt;- penguins |&gt; \n  drop_na(bill_dep)"
  },
  {
    "objectID": "exercises/discrete_predictor/index.html#a-simple-model",
    "href": "exercises/discrete_predictor/index.html#a-simple-model",
    "title": "Palmer penguins and discrete predictors",
    "section": "A simple model",
    "text": "A simple model\n\\[\n\\begin{align}\n\\text{Bill depth} &\\sim \\text{Normal}(\\mu, \\sigma)\\\\\n\\mu &\\sim \\text{Normal}(17, 2) \\\\\n\\sigma &\\sim \\text{Exponential}(1) \\\\\n\\end{align}\n\\]\n\nPrior predictive simulation (using rstanarm)\n\nbill_stan_PD &lt;- stan_glm(\n  bill_dep ~ 1,\n  data = penguins_noNAbill,\n  family = gaussian(),\n  prior_intercept = normal(17, 2),\n  prior_aux = exponential(1),\n  prior_PD = TRUE,\n  chains = 4,\n  iter = 2000,\n  refresh = 0,\n  seed = 525600\n)\n\nbill_stan_PD\n\nstan_glm\n family:       gaussian [identity]\n formula:      bill_dep ~ 1\n observations: 342\n predictors:   1\n------\n            Median MAD_SD\n(Intercept) 17.1    1.9  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 0.7    0.7   \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\n\npp_check(bill_stan_PD, nreps = 20)\n\n\n\n\nprior predictive simulations of penguin bills\n\n\n\n\nSome things to notice about the process and results above:\n\nsimilar to yesterday’s example, we’ve fit a univariate model to these data. However, because it is a gaussian model, it requires two parameters: a mean and a standard deviation.\nalthough the priors are close to a visual inspection of the histogram, the assumption here is that the prior parameters come from expertise or prior simulations, NOT from looking at the data before modelling!\n\nWe can use the posterior package to extract and conveniently summarize the draws from the distribution."
  },
  {
    "objectID": "exercises/discrete_predictor/index.html#fit-the-model",
    "href": "exercises/discrete_predictor/index.html#fit-the-model",
    "title": "Palmer penguins and discrete predictors",
    "section": "Fit the model",
    "text": "Fit the model\nNow we can fit the model directly in rstanarm and look at the results:\n\nbill_stan &lt;- stan_glm(\n  bill_dep ~ 1,\n  data = penguins_noNAbill,\n  family = gaussian(),\n  prior_intercept = normal(17, 2),\n  prior_aux = exponential(0.5),\n  chains = 4,\n  iter = 2000,\n  refresh = 0,\n  seed = 525600\n)\n\nbill_stan\n\nstan_glm\n family:       gaussian [identity]\n formula:      bill_dep ~ 1\n observations: 342\n predictors:   1\n------\n            Median MAD_SD\n(Intercept) 17.2    0.1  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 2.0    0.1   \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\n\nbill_stan |&gt; \n  posterior::summarise_draws() |&gt; \n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\n(Intercept)\n17.14934\n17.15088\n0.1075699\n0.1094916\n16.973739\n17.326793\n1.000930\n2529.865\n2333.974\n\n\nsigma\n1.98253\n1.97824\n0.0776162\n0.0766075\n1.860122\n2.116191\n1.000684\n2527.450\n1936.182"
  },
  {
    "objectID": "exercises/discrete_predictor/index.html#plotting-parameters.",
    "href": "exercises/discrete_predictor/index.html#plotting-parameters.",
    "title": "Palmer penguins and discrete predictors",
    "section": "Plotting parameters.",
    "text": "Plotting parameters.\nWe don’t have one value for each of our unknown numbers: we have thousands. We need to get a sense of what these possible values mean scientifically. An excellent way to do this is by making as many pictures as possible. We will start with making plots of specific parameters.\nWe can look at the distributions easily using the bayesplot package.\nbayesplot::mcmc_hist(bill_stan, pars = \"(Intercept)\")  + \n  coord_cartesian(xlim = c(15, 20))\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\nbayesplot::mcmc_hist(bill_stan, pars = \"sigma\") + \n  coord_cartesian(xlim = c(0, 4))\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\nNotice that the distributions do not have the same shape as the prior– this is particularly true for \\(\\sigma\\):\n\npost_draws &lt;- posterior::as_draws_df(bill_stan)\nset.seed(525600)\nndraws &lt;- nrow(post_draws)\n\nprior_draws &lt;- tibble(\n  Intercept = rnorm(ndraws, mean = 17, sd = 2),\n  sigma     = rexp(ndraws, rate = 0.5)\n)\n\ncombined_draws &lt;- post_draws |&gt;\n  transmute(\n    Intercept = `(Intercept)`,\n    sigma = sigma,\n    prior_Intercept = prior_draws$Intercept,\n    prior_sigma     = prior_draws$sigma\n  )\n\nWarning: Dropping 'draws_df' class as required metadata was removed.\n\n\nbayesplot::mcmc_hist(\n  combined_draws,\n  pars = c(\"Intercept\", \"prior_Intercept\")\n) +\n  coord_cartesian(xlim = c(10, 25))\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\nbayesplot::mcmc_hist(\n  combined_draws,\n  pars = c(\"sigma\", \"prior_sigma\")\n) +\n  coord_cartesian(xlim = c(0, 6))\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\n\n\ncomparisons of priors and posteriors for two parameters, showing how much we’ve learned from the data.\n\n\n\n\n\n\n\ncomparisons of priors and posteriors for two parameters, showing how much we’ve learned from the data."
  },
  {
    "objectID": "exercises/discrete_predictor/index.html#posterior-predictions-the-easy-way-to-check-your-model",
    "href": "exercises/discrete_predictor/index.html#posterior-predictions-the-easy-way-to-check-your-model",
    "title": "Palmer penguins and discrete predictors",
    "section": "Posterior predictions: the easy way to check your model",
    "text": "Posterior predictions: the easy way to check your model\nIn my experience, ecologists (rightly!) care a great deal about model diagnostics. And with good reason: you need to know how much to trust a model before using it to make a scientific claim. Bayes offers a straightforward way to show how well a model is doing: plot model predictions, and compare them to the observed data. This involves using the model as a data generating machine, which we’ll look at next.\n\nPseudocode\nHere is the procedure for generating posterior predictions:\n\nSelect some posterior posterior draws.\nFor each draw, extract all the model parameters\nFor each draw, plug the sampled parameters in to the model. Use all the same predictors, factors, etc as the original model.\nFor each draw, draw a random dataset that is the same size and shape as your original data.\nOverlay the simulated datasets on the observed data.\n\n\n\nPosterior prediction in R\n\n# just get some draws\ndraws_matrix &lt;- posterior::as_draws_matrix(bill_stan)\n\n## set up a matrix. for every posterior sample, \n## (that is, for a value of mu and a value of sigma) \n## draw a whole fake dataset from a normal distribution with that mean and sd. \nnsamples &lt;- 50\nyrep &lt;- matrix(0, \n               ncol = length(penguins_noNAbill$bill_dep), \n               nrow = nsamples)\n\n# pick some random rows\nset.seed(1234)\nchosen_samples &lt;- sample(1:nrow(draws_matrix), \n                         replace = FALSE,\n                         size = nsamples)\n\nsubset_draws &lt;- draws_matrix[chosen_samples,]\n\nfor (r in 1:nsamples){\n yrep[r,] &lt;- rnorm(n = length(penguins_noNAbill$bill_dep), \n                   mean = subset_draws[r, \"(Intercept)\"], \n                   sd = subset_draws[r, \"sigma\"])\n}\n\nbayesplot::ppc_dens_overlay(y = penguins_noNAbill$bill_dep,\n                            yrep = yrep)\n\n\n\n\n\n\n\n\nThis is the manual approach, which demonstrates the entire process explicitly. However, thanks to the power of rstanarm, this can also be accomplished in a single line:\n\npp_check(bill_stan, ndraws = 50)\n\nWarning: The following arguments were unrecognized and ignored: ndraws\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEXERCISE\n\n\n\nLook at the other graphical posterior predictive checks available in the bayesplot package by examining the vignette. Experiment with some different possibilities for these data.\n\n\nThe posterior predictive distribution gives us a straightforward way to test our model’s performance:\n\nwe use the model to generate fake observations.\nplot these on top of the real data\nif the data is a really poor match, we know our model has a distorted view of the world."
  },
  {
    "objectID": "exercises/discrete_predictor/index.html#different-groups-are-different",
    "href": "exercises/discrete_predictor/index.html#different-groups-are-different",
    "title": "Palmer penguins and discrete predictors",
    "section": "Different groups are different",
    "text": "Different groups are different\nlet’s add in differences among species\n\npenguins |&gt; \n  ggplot(aes(x = bill_dep, fill = species))+ \n  geom_histogram(binwidth = .5) + \n  scale_fill_brewer(palette = \"Dark2\")\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nNow we can see that the distribution is probably three different distributions, all placed together.\n\n\n\n\n\n\nWarning\n\n\n\nSometimes scientists will plot histograms of data at the beginning of a research project, and use the histogram to decide if their data are “normally distributed” or not. This is not helpful! Instead, decide on a model first, and ask yourself what kind of data you expect."
  },
  {
    "objectID": "exercises/discrete_predictor/index.html#adding-a-discrete-predictor-variable.",
    "href": "exercises/discrete_predictor/index.html#adding-a-discrete-predictor-variable.",
    "title": "Palmer penguins and discrete predictors",
    "section": "Adding a discrete predictor variable.",
    "text": "Adding a discrete predictor variable.\nHere we extend the model to give each species a different average bill depth. How many parameters are in this model?\n\\[\n\\begin{align}\n\\text{Bill depth}_{i} &\\sim \\text{Normal}(\\mu_{\\text{species}[i]}, \\sigma) \\\\\n\\mu_{\\text{species}} &\\sim \\text{Normal}(17, 2) \\\\\n\\sigma &\\sim \\text{Exponential}(2) \\\\\n\\end{align}\n\\]\n\n\n\n\n\n\nQuick detour : vector indexing\n\n\n\n\n\nA very useful technique, in both R and Stan, is transforming a vector with indexing. Vector indexing requires two vectors: the first contains values we want to select or replicate, the second contains integers giving the positions of the elements we want. For example:\n\nsome_values &lt;- c(\"taco\", \"cat\", \"goat\", \"cheeze\", \"pizza\")\npositions &lt;- c(1,1,2,2,3,1,1,5)\n\nsome_values[positions]\n\n[1] \"taco\"  \"taco\"  \"cat\"   \"cat\"   \"goat\"  \"taco\"  \"taco\"  \"pizza\"\n\n\nThis works for number values as well, and is very useful when you want to do simulations! let’s simulate three groups with different averages.\n\nset.seed(525600)\nsome_means &lt;- c(12, 17, 19)\nsome_labels &lt;- c(\"taco\", \"cat\", \"goat\")\n\ndf_of_means &lt;- data.frame(index = rep(1:3, each = 42)) |&gt; \n  mutate(the_mean = some_means[index],\n         labels = some_labels[index],\n         obs = rnorm(n = length(the_mean),\n                     mean = the_mean,\n                     sd = 1))\n\ndf_of_means |&gt; \n  ggplot(aes(x = obs, fill = labels)) + \n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\n\n\nSampling the species model\n\n\n\n\n\n\nEXERCISE\n\n\n\nFit one the species-specific model above using rstanarm. TIP: set the formula to be bill_dep ~ 0 + species. 1. What changes do you need to make to the prior? 2. Visualize the posterior with bayesplot. Does it look better than the model without species? How can you tell?\n\n\n\n\n\n\n\n\nSOLUTION\n\n\n\n\n\n\nbill_spp_stan &lt;- stan_glm(\n  bill_dep ~ 0 + species,\n  data = penguins_noNAbill,\n  family = gaussian(),\n  prior = normal(17, 2),\n  prior_aux = exponential(0.5),\n  chains = 4,\n  iter = 2000,\n  refresh = 0,\n  seed = 525600\n)\n\nbill_spp_stan\n\nstan_glm\n family:       gaussian [identity]\n formula:      bill_dep ~ 0 + species\n observations: 342\n predictors:   3\n------\n                 Median MAD_SD\nspeciesAdelie    18.3    0.1  \nspeciesChinstrap 18.4    0.1  \nspeciesGentoo    15.0    0.1  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 1.1    0.0   \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nWe can repeat the posterior checking from before:\n\n# Posterior predictive check\nbrms::pp_check(bill_spp_stan, type = \"dens_overlay\", ndraws = 50)\n\nWarning: The following arguments were unrecognized and ignored: type, ndraws\n\n\n\n\n\n\n\n\n\nThe predicted distribution is now much more like the real data!\n\n\n\n\nFurther questions & challenges\n\nI recommended that you remove the intercept from the model using bill_dep ~ 0 + species. What changes if you put it back in? why?\n\n\n\n\nVisualizing species – using tidybayes\nWe can also make figures for each individual species. Here we will move away from using bayesplot and try to visualize our posterior using the handy functions in the tidybayes package.\n\nlibrary(tidybayes)\n\npenguins_noNAbill |&gt; \n  select(species) |&gt; \n  distinct() |&gt; \n  add_predicted_draws(bill_spp_stan, ndraws = 100) |&gt; \n  ggplot(aes(x = .prediction, y = species, fill = species)) + \n  stat_halfeye()\n\n\n\n\n\n\n\n\nWe can visualize the uncertainty in predicted values AND in group means :\n\ngrid &lt;- penguins_noNAbill %&gt;%\n  modelr::data_grid(species)\n\nmeans &lt;- grid %&gt;%\n  add_epred_draws(bill_spp_stan)\n\npreds &lt;-  grid %&gt;%\n  add_predicted_draws(bill_spp_stan)\n\npenguins_noNAbill %&gt;%\n  ggplot(aes(y = species, x = bill_dep)) +\n  stat_interval(aes(x = .prediction), data = preds) +\n  stat_pointinterval(aes(x = .epred), data = means, .width = c(.66, .95), position = position_nudge(y = -0.3)) +\n  geom_jitter(height = .05, pch = 21, fill = \"orange\") +\n  scale_color_brewer() + \n  theme_dark()\n\n\n\n\n\n\n\n\n\n\nExercises\n\nLevel 1\n\nrepeat this activity for another variable in the dataset. Does the same code work on bill length? What about body size? What would you change about the model (if anything)\nuse bayesplot to examine the fit of body size to these data.\n\n\n\nLevel 2\n\ngenerate some random groups of your own, with known means. How well does the model fit these data?\n\n\n\nLevel 3\n\nAs you can see, the model assumes the same sigma for all species. what if you relax this?\n\n\n\n\nOptional!\nTry this on your own data!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "exercises/logistic_regression/index.html",
    "href": "exercises/logistic_regression/index.html",
    "title": "Logistic GLMs: mite responses to water",
    "section": "",
    "text": "We’ve already looked at univariate models. When we fit the same model to multiple different groups, we don’t expect the same values for all the coefficients. Each unit in our experiment we are studying will respond to the same variable in different ways.\nHierarchical models represent a way to model this variation, in ways that range from simple to complex.\nBefore we dive in with hierarchical structure, let’s build a bridge between these two approaches.\nThis is useful to help us understand what a hierarchical model does.\nHowever it is also useful from a strict model-building perspective – so useful that Andrew Gelman calls it a “Secret Weapon”"
  },
  {
    "objectID": "exercises/logistic_regression/index.html#loading-models-and-data",
    "href": "exercises/logistic_regression/index.html#loading-models-and-data",
    "title": "Logistic GLMs: mite responses to water",
    "section": "Loading models and data",
    "text": "Loading models and data\n\nsuppressPackageStartupMessages(library(tidyverse))\nsuppressPackageStartupMessages(library(rstanarm))\nlibrary(tidybayes)\n\ndata(mite, package = \"vegan\")\ndata(\"mite.env\", package = \"vegan\")\ndata(\"mite.xy\", package = \"vegan\")\n\nAnd some quick data restructuring to combine both.\n\n# combine data and environment\nmite_data_long &lt;- bind_cols(mite.env, mite) |&gt; \n  pivot_longer(Brachy:Trimalc2, names_to = \"spp\", values_to = \"abd\")\n\nTo keep things simple and univariate, let’s consider only water concentration as an independent variable.\nFirst, a quick word about centering and scaling a predictor variable:\n\nI center the predictor by subtracting the mean. This changes the intercept of my linear predictor. it becomes the mean log-odds of occurrance when the water content is average\nI divide water content by 100. The dataset has units of grams per Litre of water (see ?vegan::mite.env for more details). This is fine, but I don’t think mites are able to sense differences as precise as a millimeter of water either way. by dividing by 10 I transform this into centilitres, which is more informative.\n\n\nmite_data_long_transformed &lt;- mite_data_long |&gt; \n  mutate(presabs = as.numeric(abd&gt;0),\n         # center predictors\n         water = (WatrCont - mean(WatrCont)) / 100\n         )\n\nmite_data_long_transformed |&gt; \n  ggplot(aes(x = water, y = presabs)) + \n  geom_point() + \n  stat_smooth(method = \"glm\", method.args = list(family = \"binomial\")) + \n  facet_wrap(~spp)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nsome things to notice about this figure:\n\nthe x-axis scale has been transformed from “grams per litre” to “centilitres away from average\nthere is a ton of variation in how different species respond to water!\n\n\nmite_many_glms &lt;- mite_data_long_transformed |&gt; \n  nest_by(spp) |&gt; \n  mutate(logistic_regressions = list(\n    glm(presabs ~ water,\n        family = \"binomial\",\n        data = data))) |&gt; \n  mutate(coefs = list(broom::tidy(logistic_regressions)))\n\n\n\n\nmite_many_glms &lt;- mite_data_long_transformed |&gt; \n  nest_by(spp) |&gt; \n  mutate(logistic_regressions = list(\n    glm(presabs ~ 1 + water, \n        family = binomial(link = \"logit\"),\n        data = data)\n  )) |&gt; \n  mutate(coefs = list(broom::tidy(logistic_regressions)))\n\nbroom::tidy(mite_many_glms$logistic_regressions[[5]])\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   -0.644     0.353     -1.82 0.0684   \n2 water         -1.82      0.446     -4.08 0.0000450\n\nbroom::glance(mite_many_glms$logistic_regressions[[5]])\n\n# A tibble: 1 × 8\n  null.deviance df.null logLik   AIC   BIC deviance df.residual  nobs\n          &lt;dbl&gt;   &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;       &lt;int&gt; &lt;int&gt;\n1          95.6      69  -28.3  60.6  65.1     56.6          68    70\n\nmite_many_glms$coefs[[5]]\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   -0.644     0.353     -1.82 0.0684   \n2 water         -1.82      0.446     -4.08 0.0000450\n\n\n\n\n\n\n\n\nSplit-Apply-Combine\n\n\n\nTo explore this kind of thinking, we are going to use an approach sometimes called “split-apply-combine”\nThere are many possible ways to do this in practice. We are using a technique here from the tidyverse, which you can read more about.\n\n\n\nmite_many_glm_coefs &lt;- mite_many_glms |&gt; \n  select(-data, -logistic_regressions) |&gt; \n  unnest(coefs)\n\nmite_many_glm_coefs |&gt; \n  ggplot(aes(x = estimate, y = spp,\n             xmin = estimate - std.error,\n             xmax = estimate + std.error)) + \n  geom_pointrange() + \n  facet_wrap(~term, scales = \"free\")\n\n\n\n\n\n\n\n\nAs you can see, some of these estimates are high, others low. We could also plot these as histograms to see this distribution.\n\nmite_many_glm_coefs |&gt; \n  ggplot(aes(x = estimate)) + \n  geom_histogram(binwidth = .5) + \n  facet_wrap(~term, scales = \"free\")\n\n\n\n\n\n\n\n\nOnce again, the two parameters of this model represent:\n\nIntercept The probability (in log-odds) of a species being present at the average water concentration. some species are common, others are rare.\nwater this is the change in probability (in log-odds) as water increases by one centilitre per litre of substrate."
  },
  {
    "objectID": "exercises/logistic_regression/index.html#modelling-multiple-slopes-in-rstanarm",
    "href": "exercises/logistic_regression/index.html#modelling-multiple-slopes-in-rstanarm",
    "title": "Logistic GLMs: mite responses to water",
    "section": "Modelling multiple slopes in rstanarm",
    "text": "Modelling multiple slopes in rstanarm\n\nmite_data_long_transformed |&gt; glimpse()\n\nRows: 2,450\nColumns: 9\n$ SubsDens  &lt;dbl&gt; 39.18, 39.18, 39.18, 39.18, 39.18, 39.18, 39.18, 39.18, 39.1…\n$ WatrCont  &lt;dbl&gt; 350.15, 350.15, 350.15, 350.15, 350.15, 350.15, 350.15, 350.…\n$ Substrate &lt;fct&gt; Sphagn1, Sphagn1, Sphagn1, Sphagn1, Sphagn1, Sphagn1, Sphagn…\n$ Shrub     &lt;ord&gt; Few, Few, Few, Few, Few, Few, Few, Few, Few, Few, Few, Few, …\n$ Topo      &lt;fct&gt; Hummock, Hummock, Hummock, Hummock, Hummock, Hummock, Hummoc…\n$ spp       &lt;chr&gt; \"Brachy\", \"PHTH\", \"HPAV\", \"RARD\", \"SSTR\", \"Protopl\", \"MEGR\",…\n$ abd       &lt;int&gt; 17, 5, 5, 3, 2, 1, 4, 2, 2, 1, 4, 1, 17, 4, 9, 50, 3, 1, 1, …\n$ presabs   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ water     &lt;dbl&gt; -0.6048571, -0.6048571, -0.6048571, -0.6048571, -0.6048571, …\n\nmite_slopes_corr_stan &lt;- stan_glmer(\n  presabs ~ 1 + water + (1 + water | spp), \n  family = binomial(link = \"logit\"),\n  data = mite_data_long_transformed,\n  \n  # priors \n  prior = normal(0, .2),\n  prior_intercept = normal(-.2, .5),\n  prior_covariance = decov(\n    regularization = 2,\n    concentration = 1, \n    shape = .4, \n    scale = 1),\n    \n  chains = 4,\n  cores = 4, \n  iter = 2000,\n  refresh = 0\n)\n\nsummary(mite_slopes_corr_stan)\n\n\nModel Info:\n function:     stan_glmer\n family:       binomial [logit]\n formula:      presabs ~ 1 + water + (1 + water | spp)\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 2450\n groups:       spp (35)\n\nEstimates:\n                                     mean   sd   10%   50%   90%\n(Intercept)                        -0.3    0.2 -0.6  -0.3   0.0 \nwater                              -0.4    0.1 -0.5  -0.4  -0.2 \nb[(Intercept) spp:Brachy]           2.6    0.5  2.0   2.6   3.2 \nb[water spp:Brachy]                -0.1    0.3 -0.5  -0.1   0.2 \nb[(Intercept) spp:Ceratoz1]         0.8    0.3  0.3   0.8   1.2 \nb[water spp:Ceratoz1]               0.4    0.2  0.1   0.4   0.6 \nb[(Intercept) spp:Ceratoz3]         0.1    0.3 -0.4   0.1   0.5 \nb[water spp:Ceratoz3]               0.6    0.2  0.3   0.6   0.9 \nb[(Intercept) spp:Eupelops]        -0.2    0.3 -0.6  -0.2   0.3 \nb[water spp:Eupelops]              -0.2    0.2 -0.5  -0.2   0.1 \nb[(Intercept) spp:FSET]            -0.3    0.4 -0.8  -0.3   0.2 \nb[water spp:FSET]                  -1.2    0.4 -1.7  -1.2  -0.8 \nb[(Intercept) spp:Galumna1]        -0.7    0.4 -1.3  -0.7  -0.2 \nb[water spp:Galumna1]              -1.1    0.4 -1.6  -1.1  -0.7 \nb[(Intercept) spp:HMIN]             0.1    0.4 -0.4   0.1   0.6 \nb[water spp:HMIN]                  -0.9    0.3 -1.3  -0.9  -0.5 \nb[(Intercept) spp:HMIN2]           -0.6    0.4 -1.1  -0.6  -0.1 \nb[water spp:HMIN2]                 -0.8    0.3 -1.3  -0.8  -0.4 \nb[(Intercept) spp:HPAV]             3.2    0.6  2.5   3.2   4.0 \nb[water spp:HPAV]                  -0.2    0.3 -0.6  -0.2   0.2 \nb[(Intercept) spp:HRUF]            -1.5    0.4 -2.1  -1.5  -1.0 \nb[water spp:HRUF]                   0.1    0.3 -0.2   0.1   0.4 \nb[(Intercept) spp:LCIL]             2.1    0.5  1.5   2.1   2.7 \nb[water spp:LCIL]                   1.5    0.3  1.1   1.5   1.9 \nb[(Intercept) spp:Lepidzts]        -2.1    0.5 -2.8  -2.1  -1.5 \nb[water spp:Lepidzts]              -0.4    0.3 -0.9  -0.4   0.0 \nb[(Intercept) spp:LRUG]             1.3    0.4  0.9   1.3   1.8 \nb[water spp:LRUG]                   1.1    0.3  0.8   1.1   1.5 \nb[(Intercept) spp:MEGR]             0.5    0.4  0.0   0.5   1.0 \nb[water spp:MEGR]                  -0.6    0.3 -0.9  -0.6  -0.3 \nb[(Intercept) spp:Miniglmn]        -2.1    0.5 -2.8  -2.1  -1.5 \nb[water spp:Miniglmn]              -0.6    0.3 -1.1  -0.6  -0.2 \nb[(Intercept) spp:MPRO]            -1.8    0.5 -2.4  -1.8  -1.3 \nb[water spp:MPRO]                   0.0    0.3 -0.4   0.0   0.3 \nb[(Intercept) spp:NCOR]             0.3    0.3 -0.1   0.3   0.7 \nb[water spp:NCOR]                   0.5    0.2  0.3   0.5   0.8 \nb[(Intercept) spp:NPRA]             1.0    0.4  0.6   1.0   1.5 \nb[water spp:NPRA]                  -0.3    0.2 -0.6  -0.3   0.0 \nb[(Intercept) spp:ONOV]             3.3    0.7  2.5   3.3   4.2 \nb[water spp:ONOV]                  -0.9    0.4 -1.4  -0.9  -0.4 \nb[(Intercept) spp:Oppiminu]         0.0    0.3 -0.4   0.0   0.4 \nb[water spp:Oppiminu]               0.5    0.2  0.2   0.5   0.7 \nb[(Intercept) spp:Oribatl1]        -0.1    0.4 -0.6  -0.1   0.3 \nb[water spp:Oribatl1]              -0.6    0.3 -1.0  -0.6  -0.3 \nb[(Intercept) spp:PHTH]            -0.7    0.4 -1.2  -0.7  -0.2 \nb[water spp:PHTH]                  -0.9    0.3 -1.3  -0.9  -0.5 \nb[(Intercept) spp:PLAG2]           -0.6    0.4 -1.1  -0.6  -0.2 \nb[water spp:PLAG2]                  0.3    0.2  0.0   0.3   0.6 \nb[(Intercept) spp:PPEL]            -2.0    0.5 -2.6  -1.9  -1.3 \nb[water spp:PPEL]                  -0.2    0.3 -0.6  -0.2   0.2 \nb[(Intercept) spp:Protopl]         -1.7    0.5 -2.3  -1.7  -1.2 \nb[water spp:Protopl]               -0.4    0.3 -0.8  -0.4  -0.1 \nb[(Intercept) spp:PWIL]             0.2    0.3 -0.2   0.2   0.6 \nb[water spp:PWIL]                  -0.2    0.2 -0.5  -0.2   0.0 \nb[(Intercept) spp:RARD]            -1.0    0.4 -1.6  -1.0  -0.5 \nb[water spp:RARD]                  -1.0    0.3 -1.4  -1.0  -0.5 \nb[(Intercept) spp:SLAT]            -1.9    0.5 -2.5  -1.8  -1.3 \nb[water spp:SLAT]                  -0.5    0.3 -0.9  -0.5  -0.1 \nb[(Intercept) spp:SSTR]            -1.8    0.4 -2.3  -1.7  -1.2 \nb[water spp:SSTR]                  -0.1    0.3 -0.5  -0.1   0.2 \nb[(Intercept) spp:Stgncrs2]        -1.3    0.4 -1.9  -1.3  -0.7 \nb[water spp:Stgncrs2]              -0.7    0.3 -1.1  -0.7  -0.3 \nb[(Intercept) spp:SUCT]             3.9    0.8  3.0   3.9   5.0 \nb[water spp:SUCT]                  -0.7    0.4 -1.1  -0.6  -0.2 \nb[(Intercept) spp:Trhypch1]         0.1    0.3 -0.3   0.1   0.5 \nb[water spp:Trhypch1]               0.8    0.2  0.5   0.8   1.1 \nb[(Intercept) spp:Trimalc2]        -1.1    0.4 -1.7  -1.1  -0.6 \nb[water spp:Trimalc2]               1.3    0.3  0.9   1.3   1.6 \nb[(Intercept) spp:TVEL]             0.6    0.4  0.1   0.6   1.1 \nb[water spp:TVEL]                  -1.0    0.3 -1.4  -1.0  -0.6 \nb[(Intercept) spp:TVIE]            -0.5    0.4 -0.9  -0.5   0.0 \nb[water spp:TVIE]                   0.8    0.2  0.5   0.8   1.1 \nSigma[spp:(Intercept),(Intercept)]  2.7    0.7  1.9   2.6   3.7 \nSigma[spp:water,(Intercept)]        0.1    0.3 -0.2   0.1   0.4 \nSigma[spp:water,water]              0.7    0.2  0.5   0.7   1.0 \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 0.4    0.0  0.4   0.4   0.4  \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n                                   mcse Rhat n_eff\n(Intercept)                        0.0  1.0   393 \nwater                              0.0  1.0   708 \nb[(Intercept) spp:Brachy]          0.0  1.0  1270 \nb[water spp:Brachy]                0.0  1.0  2630 \nb[(Intercept) spp:Ceratoz1]        0.0  1.0   745 \nb[water spp:Ceratoz1]              0.0  1.0  1414 \nb[(Intercept) spp:Ceratoz3]        0.0  1.0   718 \nb[water spp:Ceratoz3]              0.0  1.0  1803 \nb[(Intercept) spp:Eupelops]        0.0  1.0   746 \nb[water spp:Eupelops]              0.0  1.0  2047 \nb[(Intercept) spp:FSET]            0.0  1.0   786 \nb[water spp:FSET]                  0.0  1.0  2598 \nb[(Intercept) spp:Galumna1]        0.0  1.0   996 \nb[water spp:Galumna1]              0.0  1.0  2196 \nb[(Intercept) spp:HMIN]            0.0  1.0   847 \nb[water spp:HMIN]                  0.0  1.0  2537 \nb[(Intercept) spp:HMIN2]           0.0  1.0   976 \nb[water spp:HMIN2]                 0.0  1.0  2482 \nb[(Intercept) spp:HPAV]            0.0  1.0  1758 \nb[water spp:HPAV]                  0.0  1.0  3164 \nb[(Intercept) spp:HRUF]            0.0  1.0  1011 \nb[water spp:HRUF]                  0.0  1.0  2215 \nb[(Intercept) spp:LCIL]            0.0  1.0  1268 \nb[water spp:LCIL]                  0.0  1.0  3165 \nb[(Intercept) spp:Lepidzts]        0.0  1.0  1541 \nb[water spp:Lepidzts]              0.0  1.0  2581 \nb[(Intercept) spp:LRUG]            0.0  1.0   871 \nb[water spp:LRUG]                  0.0  1.0  2614 \nb[(Intercept) spp:MEGR]            0.0  1.0   756 \nb[water spp:MEGR]                  0.0  1.0  1963 \nb[(Intercept) spp:Miniglmn]        0.0  1.0  1592 \nb[water spp:Miniglmn]              0.0  1.0  2503 \nb[(Intercept) spp:MPRO]            0.0  1.0  1206 \nb[water spp:MPRO]                  0.0  1.0  2510 \nb[(Intercept) spp:NCOR]            0.0  1.0   673 \nb[water spp:NCOR]                  0.0  1.0  1542 \nb[(Intercept) spp:NPRA]            0.0  1.0   787 \nb[water spp:NPRA]                  0.0  1.0  1761 \nb[(Intercept) spp:ONOV]            0.0  1.0  1397 \nb[water spp:ONOV]                  0.0  1.0  2391 \nb[(Intercept) spp:Oppiminu]        0.0  1.0   738 \nb[water spp:Oppiminu]              0.0  1.0  1586 \nb[(Intercept) spp:Oribatl1]        0.0  1.0   851 \nb[water spp:Oribatl1]              0.0  1.0  2224 \nb[(Intercept) spp:PHTH]            0.0  1.0   860 \nb[water spp:PHTH]                  0.0  1.0  2352 \nb[(Intercept) spp:PLAG2]           0.0  1.0   746 \nb[water spp:PLAG2]                 0.0  1.0  1793 \nb[(Intercept) spp:PPEL]            0.0  1.0  1355 \nb[water spp:PPEL]                  0.0  1.0  2551 \nb[(Intercept) spp:Protopl]         0.0  1.0  1164 \nb[water spp:Protopl]               0.0  1.0  2178 \nb[(Intercept) spp:PWIL]            0.0  1.0   779 \nb[water spp:PWIL]                  0.0  1.0  1577 \nb[(Intercept) spp:RARD]            0.0  1.0   923 \nb[water spp:RARD]                  0.0  1.0  2073 \nb[(Intercept) spp:SLAT]            0.0  1.0  1086 \nb[water spp:SLAT]                  0.0  1.0  2308 \nb[(Intercept) spp:SSTR]            0.0  1.0  1254 \nb[water spp:SSTR]                  0.0  1.0  2416 \nb[(Intercept) spp:Stgncrs2]        0.0  1.0  1175 \nb[water spp:Stgncrs2]              0.0  1.0  2000 \nb[(Intercept) spp:SUCT]            0.0  1.0  1610 \nb[water spp:SUCT]                  0.0  1.0  2151 \nb[(Intercept) spp:Trhypch1]        0.0  1.0   575 \nb[water spp:Trhypch1]              0.0  1.0  2037 \nb[(Intercept) spp:Trimalc2]        0.0  1.0  1048 \nb[water spp:Trimalc2]              0.0  1.0  2784 \nb[(Intercept) spp:TVEL]            0.0  1.0   722 \nb[water spp:TVEL]                  0.0  1.0  2693 \nb[(Intercept) spp:TVIE]            0.0  1.0   759 \nb[water spp:TVIE]                  0.0  1.0  1741 \nSigma[spp:(Intercept),(Intercept)] 0.0  1.0   804 \nSigma[spp:water,(Intercept)]       0.0  1.0   640 \nSigma[spp:water,water]             0.0  1.0   958 \nmean_PPD                           0.0  1.0  3913 \nlog-posterior                      0.3  1.0   812 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n\n\nThe code in rstanarm calls for a particular way to define the prior for the slopes and intercepts\n\ncurve(dgamma(x, shape = .4, scale = 7), xlim = c(0, 5))\n\n\n\n\n\n\n\nmean(rgamma(5000, shape = .4, scale = 1))\n\n[1] 0.3927667\n\n\nplot it on the data:\n\npredicted_mite_curves &lt;- mite_data_long_transformed |&gt; \n  modelr::data_grid(\n    water = modelr::seq_range(water, n = 20) ,\n    spp = spp #head(spp)\n  ) |&gt; \n  add_epred_rvars(mite_slopes_corr_stan)\n\npredicted_mite_curves |&gt; \n  ggplot(aes(x = water, ydist = .epred)) + \n  tidybayes::stat_lineribbon() + \n  facet_wrap(~spp)"
  },
  {
    "objectID": "exercises/logistic_regression/index.html#comparing-slopes-from-fixed-and-hierarchical-models",
    "href": "exercises/logistic_regression/index.html#comparing-slopes-from-fixed-and-hierarchical-models",
    "title": "Logistic GLMs: mite responses to water",
    "section": "Comparing slopes from fixed and hierarchical models",
    "text": "Comparing slopes from fixed and hierarchical models\nLet’s extract the coefficients from both fixed and hierarchical models and see what they tell us about regularization.\nFirst, fix a simple model with no hierarchy on the slopes or intercepts\n\nmite_slopes_simple_stan &lt;- stan_glm(\n  presabs ~ 1 + water + spp + water:spp,\n  family = binomial(link = \"logit\"),\n  data = mite_data_long_transformed,\n  \n  # priors \n  prior = normal(0, 5),\n  prior_intercept = normal(-.2, .5),\n    \n  chains = 4,\n  cores = 4,\n  iter = 2000,\n  refresh = 0\n)\n\nWarning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#bulk-ess\n\nsummary(mite_slopes_simple_stan)\n\n\nModel Info:\n function:     stan_glm\n family:       binomial [logit]\n formula:      presabs ~ 1 + water + spp + water:spp\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 2450\n predictors:   70\n\nEstimates:\n                    mean   sd   10%   50%   90%\n(Intercept)        2.0    0.4  1.6   2.0   2.5 \nwater             -0.4    0.2 -0.8  -0.4  -0.1 \nsppCeratoz1       -1.5    0.4 -2.1  -1.5  -1.0 \nsppCeratoz3       -2.2    0.4 -2.8  -2.2  -1.7 \nsppEupelops       -2.5    0.4 -3.0  -2.5  -1.9 \nsppFSET           -2.7    0.5 -3.3  -2.6  -2.0 \nsppGalumna1       -3.2    0.5 -3.8  -3.1  -2.5 \nsppHMIN           -2.2    0.5 -2.8  -2.1  -1.5 \nsppHMIN2          -3.0    0.5 -3.6  -2.9  -2.4 \nsppHPAV            1.6    0.9  0.5   1.5   2.7 \nsppHRUF           -3.9    0.5 -4.6  -3.9  -3.2 \nsppLCIL            0.3    0.6 -0.5   0.3   1.1 \nsppLepidzts       -4.7    0.7 -5.5  -4.6  -3.9 \nsppLRUG           -0.8    0.5 -1.5  -0.8  -0.2 \nsppMEGR           -1.8    0.5 -2.4  -1.8  -1.2 \nsppMiniglmn       -4.8    0.7 -5.7  -4.7  -3.9 \nsppMPRO           -4.2    0.6 -5.0  -4.2  -3.5 \nsppNCOR           -2.0    0.4 -2.6  -2.0  -1.4 \nsppNPRA           -1.2    0.5 -1.8  -1.2  -0.6 \nsppONOV            2.9    1.3  1.3   2.7   4.6 \nsppOppiminu       -2.3    0.4 -2.8  -2.3  -1.7 \nsppOribatl1       -2.4    0.5 -3.1  -2.4  -1.9 \nsppPHTH           -3.1    0.5 -3.8  -3.1  -2.5 \nsppPLAG2          -2.9    0.4 -3.5  -2.9  -2.4 \nsppPPEL           -4.4    0.6 -5.2  -4.4  -3.7 \nsppProtopl        -4.2    0.6 -4.9  -4.1  -3.5 \nsppPWIL           -2.1    0.4 -2.6  -2.1  -1.5 \nsppRARD           -3.5    0.5 -4.2  -3.5  -2.8 \nsppSLAT           -4.3    0.6 -5.2  -4.3  -3.6 \nsppSSTR           -4.2    0.6 -4.9  -4.1  -3.5 \nsppStgncrs2       -3.7    0.6 -4.4  -3.7  -3.0 \nsppSUCT            5.4    2.2  2.8   5.2   8.4 \nsppTrhypch1       -2.2    0.4 -2.7  -2.2  -1.6 \nsppTrimalc2       -3.6    0.5 -4.3  -3.6  -2.9 \nsppTVEL           -1.7    0.5 -2.3  -1.7  -1.1 \nsppTVIE           -2.8    0.5 -3.4  -2.8  -2.2 \nwater:sppCeratoz1  0.5    0.3  0.1   0.5   0.9 \nwater:sppCeratoz3  0.7    0.3  0.3   0.7   1.1 \nwater:sppEupelops -0.1    0.3 -0.5  -0.1   0.3 \nwater:sppFSET     -1.5    0.5 -2.2  -1.5  -0.8 \nwater:sppGalumna1 -1.3    0.5 -1.9  -1.3  -0.6 \nwater:sppHMIN     -1.0    0.4 -1.5  -1.0  -0.4 \nwater:sppHMIN2    -0.9    0.4 -1.4  -0.9  -0.4 \nwater:sppHPAV     -0.4    0.5 -1.0  -0.4   0.2 \nwater:sppHRUF      0.2    0.4 -0.2   0.2   0.7 \nwater:sppLCIL      2.0    0.5  1.4   2.0   2.6 \nwater:sppLepidzts -0.5    0.4 -1.1  -0.5   0.1 \nwater:sppLRUG      1.4    0.4  0.9   1.4   1.9 \nwater:sppMEGR     -0.6    0.4 -1.0  -0.6  -0.1 \nwater:sppMiniglmn -0.7    0.5 -1.3  -0.7  -0.2 \nwater:sppMPRO      0.0    0.4 -0.5   0.0   0.5 \nwater:sppNCOR      0.6    0.3  0.3   0.6   1.0 \nwater:sppNPRA     -0.3    0.3 -0.7  -0.3   0.2 \nwater:sppONOV     -1.8    0.7 -2.8  -1.8  -0.9 \nwater:sppOppiminu  0.6    0.3  0.2   0.6   1.0 \nwater:sppOribatl1 -0.6    0.4 -1.1  -0.6  -0.2 \nwater:sppPHTH     -1.0    0.4 -1.5  -0.9  -0.4 \nwater:sppPLAG2     0.4    0.3  0.0   0.4   0.8 \nwater:sppPPEL     -0.2    0.4 -0.8  -0.2   0.3 \nwater:sppProtopl  -0.4    0.4 -1.0  -0.4   0.1 \nwater:sppPWIL     -0.2    0.3 -0.6  -0.2   0.2 \nwater:sppRARD     -1.1    0.5 -1.7  -1.1  -0.5 \nwater:sppSLAT     -0.5    0.4 -1.0  -0.5   0.1 \nwater:sppSSTR     -0.1    0.4 -0.6  -0.1   0.4 \nwater:sppStgncrs2 -0.8    0.4 -1.3  -0.8  -0.2 \nwater:sppSUCT     -2.1    1.0 -3.4  -2.0  -1.0 \nwater:sppTrhypch1  1.0    0.3  0.6   1.0   1.4 \nwater:sppTrimalc2  1.6    0.4  1.1   1.6   2.1 \nwater:sppTVEL     -1.1    0.4 -1.7  -1.1  -0.5 \nwater:sppTVIE      0.9    0.3  0.5   0.9   1.3 \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 0.4    0.0  0.4   0.4   0.4  \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n                  mcse Rhat n_eff\n(Intercept)       0.0  1.0   305 \nwater             0.0  1.0   405 \nsppCeratoz1       0.0  1.0   418 \nsppCeratoz3       0.0  1.0   430 \nsppEupelops       0.0  1.0   445 \nsppFSET           0.0  1.0   573 \nsppGalumna1       0.0  1.0   615 \nsppHMIN           0.0  1.0   501 \nsppHMIN2          0.0  1.0   490 \nsppHPAV           0.0  1.0  1255 \nsppHRUF           0.0  1.0   560 \nsppLCIL           0.0  1.0   719 \nsppLepidzts       0.0  1.0   948 \nsppLRUG           0.0  1.0   525 \nsppMEGR           0.0  1.0   490 \nsppMiniglmn       0.0  1.0   938 \nsppMPRO           0.0  1.0   700 \nsppNCOR           0.0  1.0   423 \nsppNPRA           0.0  1.0   495 \nsppONOV           0.0  1.0  1977 \nsppOppiminu       0.0  1.0   429 \nsppOribatl1       0.0  1.0   479 \nsppPHTH           0.0  1.0   574 \nsppPLAG2          0.0  1.0   448 \nsppPPEL           0.0  1.0   772 \nsppProtopl        0.0  1.0   716 \nsppPWIL           0.0  1.0   449 \nsppRARD           0.0  1.0   636 \nsppSLAT           0.0  1.0   753 \nsppSSTR           0.0  1.0   613 \nsppStgncrs2       0.0  1.0   655 \nsppSUCT           0.1  1.0  1741 \nsppTrhypch1       0.0  1.0   449 \nsppTrimalc2       0.0  1.0   624 \nsppTVEL           0.0  1.0   515 \nsppTVIE           0.0  1.0   452 \nwater:sppCeratoz1 0.0  1.0   583 \nwater:sppCeratoz3 0.0  1.0   621 \nwater:sppEupelops 0.0  1.0   648 \nwater:sppFSET     0.0  1.0  1339 \nwater:sppGalumna1 0.0  1.0  1227 \nwater:sppHMIN     0.0  1.0  1108 \nwater:sppHMIN2    0.0  1.0   963 \nwater:sppHPAV     0.0  1.0  1150 \nwater:sppHRUF     0.0  1.0   734 \nwater:sppLCIL     0.0  1.0  1133 \nwater:sppLepidzts 0.0  1.0  1055 \nwater:sppLRUG     0.0  1.0   821 \nwater:sppMEGR     0.0  1.0   774 \nwater:sppMiniglmn 0.0  1.0  1138 \nwater:sppMPRO     0.0  1.0   928 \nwater:sppNCOR     0.0  1.0   578 \nwater:sppNPRA     0.0  1.0   667 \nwater:sppONOV     0.0  1.0  1596 \nwater:sppOppiminu 0.0  1.0   574 \nwater:sppOribatl1 0.0  1.0   907 \nwater:sppPHTH     0.0  1.0  1079 \nwater:sppPLAG2    0.0  1.0   583 \nwater:sppPPEL     0.0  1.0   898 \nwater:sppProtopl  0.0  1.0   942 \nwater:sppPWIL     0.0  1.0   693 \nwater:sppRARD     0.0  1.0  1132 \nwater:sppSLAT     0.0  1.0   975 \nwater:sppSSTR     0.0  1.0   930 \nwater:sppStgncrs2 0.0  1.0   927 \nwater:sppSUCT     0.0  1.0  1440 \nwater:sppTrhypch1 0.0  1.0   610 \nwater:sppTrimalc2 0.0  1.0   871 \nwater:sppTVEL     0.0  1.0  1136 \nwater:sppTVIE     0.0  1.0   642 \nmean_PPD          0.0  1.0  3959 \nlog-posterior     0.2  1.0  1244 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1)."
  },
  {
    "objectID": "exercises/logistic_regression/index.html#looking-at-regularization",
    "href": "exercises/logistic_regression/index.html#looking-at-regularization",
    "title": "Logistic GLMs: mite responses to water",
    "section": "Looking at regularization",
    "text": "Looking at regularization\nOne of the main reasons to use hierarchical models is regularization. This brings coefficients closer to the overall average of the model. This effect is greater for more extreme parameter estimates, and for those estimated from smaller sample sizes.\nWhy is this good? Because we want to make inferences about the regular features of a sample, ie those that are likely to be repeated in the future. Extreme and uncertain events are unlikely to be seen again, by definition. This kind of regularization always leads to better predictions for new data outside our sample.\nThis code is long, and not necessarily meant to be reused. However, it shows a process that might be useful in your own work: calculating species-specific slopes.\n\n# get_variables(mite_slopes_simple_stan)\n\n## Simple model\nmodel_posterior &lt;- mite_slopes_simple_stan |&gt; \n  posterior::as_draws_rvars() |&gt; \n  enframe(name = \"term\", value = \"draws\") |&gt; \n  unnest(draws)\n\nwater_firstspp &lt;- model_posterior |&gt; \n  filter(term == \"water\") |&gt; \n  rename(main_effect = term, main_post = draws)\n\nwater_firstspp\n\n# A tibble: 1 × 2\n  main_effect     main_post\n  &lt;chr&gt;          &lt;rvar[1d]&gt;\n1 water        -0.45 ± 0.25\n\nwater_interactions &lt;- model_posterior |&gt; \n  filter(str_detect(term, \"water:\"))\n\nsimple_slopes &lt;- bind_cols(water_firstspp, water_interactions) |&gt; \n  mutate(slope = draws + main_post,\n         term = str_replace(term, \"water:spp\", \"\")) |&gt; \n  select(species = term, slope) |&gt; \n  mutate(model = \"simple\", .before = \"species\")\n\n## add back in the very first species, Brachy, which was defined as the intercept\nsimple_slopes &lt;- simple_slopes |&gt; \n  bind_rows(\n    tibble(model = \"simple\",\n           species = \"Brachy\",\n           slope = water_firstspp$main_post)\n  )\n\n\n## Hierarchical model\n# get_variables(mite_slopes_corr_stan)\nhier_slopes &lt;- mite_slopes_corr_stan |&gt; \n  spread_rvars(water, b[spp]) |&gt; \n  filter(str_detect(spp,\"water\")) |&gt; \n  mutate(species=str_replace(spp, \"water spp:\", \"\"),\n         slope = water + b) |&gt; \n  select(species, slope) |&gt; \n  mutate(model = \"hierarchical\", .before = \"species\")\n\nbind_rows(simple_slopes, hier_slopes) |&gt; \n  ggplot(aes(x = species, ydist = slope, col = model)) + \n  stat_pointinterval(position = position_dodge(width = .4)) + \n  coord_flip()"
  },
  {
    "objectID": "exercises/logistic_regression/index.html#posterior-calculations-species-richness",
    "href": "exercises/logistic_regression/index.html#posterior-calculations-species-richness",
    "title": "Logistic GLMs: mite responses to water",
    "section": "Posterior calculations: species richness",
    "text": "Posterior calculations: species richness\nEverything calculated with posterior samples is also part of the posterior distribution. Its’ possible to calculate multiple things out of this model. For example, if we add together the predicted species richness at each site, we can draw the curve for how species richness might be expected to vary across this water gradient.\n\npredicted_mite_curves |&gt; \n  pivot_wider(names_from = spp, values_from = .epred) |&gt; \n  nest_by(water) |&gt; \n  mutate(S = Reduce(`+`, data)) |&gt; \n  ggplot(aes(x = water, ydist = S)) + \n  stat_lineribbon()"
  },
  {
    "objectID": "exercises/logistic_regression/index.html#comparison-to-lme4",
    "href": "exercises/logistic_regression/index.html#comparison-to-lme4",
    "title": "Logistic GLMs: mite responses to water",
    "section": "Comparison to lme4",
    "text": "Comparison to lme4\n\nlibrary(lme4)\n\nLoading required package: Matrix\n\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\nmite_water_lmer &lt;- glmer(presabs ~ 1 + water + (1 + water | spp), \n  family = binomial(link = \"logit\"),\n  data = mite_data_long_transformed,)\n\nsummary(mite_water_lmer)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: presabs ~ 1 + water + (1 + water | spp)\n   Data: mite_data_long_transformed\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n   2462.5    2491.6   -1226.3    2452.5      2445 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-7.6137 -0.5919 -0.2328  0.5754  5.3452 \n\nRandom effects:\n Groups Name        Variance Std.Dev. Corr\n spp    (Intercept) 2.6202   1.6187       \n        water       0.5549   0.7449   0.10\nNumber of obs: 2450, groups:  spp, 35\n\nFixed effects:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -0.3595     0.2810  -1.280    0.201    \nwater        -0.5437     0.1349  -4.032 5.54e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n      (Intr)\nwater 0.099"
  },
  {
    "objectID": "exercises/01-simulation/index.html",
    "href": "exercises/01-simulation/index.html",
    "title": "Introduction to Simulation for validating a model",
    "section": "",
    "text": "library(tidyverse)\nlibrary(lme4)\nlibrary(rstanarm)"
  },
  {
    "objectID": "exercises/01-simulation/index.html#simple-exercise-in-simulation",
    "href": "exercises/01-simulation/index.html#simple-exercise-in-simulation",
    "title": "Introduction to Simulation for validating a model",
    "section": "Simple exercise in simulation",
    "text": "Simple exercise in simulation\nLet’s imagine we are taking a walk as a group today at this beautiful field site. What is the number of birds (total abundance of ALL species) each of us is going to see on our hike?\n\nSome questions to ask about simulated data\n\nWhat kind of observations are you going to make? Do they have a minimum or maximum value? Are they integers, or are they decimal numbers, or something else?\nWhere do the numbers come from? This could be anything, from simple linear approximations (i.e. the models we’re looking at in this course) to ODEs, mathematical models, GAMs, etc.\nHow many observations will we be making?\n\nOne of the most useful traits of Bayesian models is that they are generative: they can be used to make a simulated dataset. We’ll do that now for our bird example.\nlet’s simulate from a Poisson distribution:\n\nset.seed(525600)\nn_people &lt;- 21\navg_birds_per_person &lt;- runif(1, min = 0, max = 30)\nbird_count &lt;- rpois(n_people, lambda = avg_birds_per_person)\n\nSome things to note in the code above:\nEvery statistical distribution that is in R (which is a lot! almost all! ) has four different functions. If the distribution is called dist, then they are:\n\nrdist = draw random numbers from dist\nqdist = the quantile function – what value gives a certain proportion of the distribution?\npdist = the probability density function – what proportion of the distribution is below a certain value?\nddist the density function = draws the “shape” of a distribution. How probable are specific values?\n\nThe other thing to note is that there are TWO simulation steps here: first, simulating a value of the average (\\(\\lambda\\)) and second, simulating observations. In our model, the Uniform distribution was referred to as the prior, and the Poisson distribution was referred to as a likelihood, but here you can see that they are very nearly the same thing: just statements about what distribution of values might be most consistent with the data.\n\nPlotting the result\nLet’s take a look at our simulated values:\n\nhist(bird_count, col = \"lightblue\", xlim = c(0, 50))\n\n\n\n\nHistogram of simulated counts of birds\n\n\n\n\nThis is pretty great, and represents one possible realization of sampling. However, one sample isn’t enough to tell us about what our \\(\\text{Uniform}(0, 60)\\) prior really means.\n\n\n\n\n\n\nEXERCISE\n\n\n\nTry to make many different simulations (say, 12 simulations). This represents 12 different repeats of the whole process: draw a value from the uniform prior, THEN draw a value from the poisson. Visualize them any way you want! (the worked example below uses ggplot2)\n\n\n\n\n\n\n\n\nSOLUTION\n\n\n\n\n\n\nset.seed(525600)\n\nsimulate_some_birds &lt;- function() {\n  lambda &lt;- runif(1, min = 0, max = 60)\n  tibble(birds_seen = rpois(23, lambda = lambda),\n         lambda = lambda)\n}\n\nbird_simulations &lt;- purrr::map(1:12, function(x) simulate_some_birds()) |&gt; \n  list_rbind(names_to = \"simulation_id\")\n  \n\nbird_simulations |&gt; \n  ggplot(aes(x = birds_seen)) + \n  geom_histogram(bins = 28) + \n  facet_wrap(~simulation_id) + \n  theme_bw() + \n  labs(x = \"Number of birds observed per person\")\n\n\n\n\nTwelve different simulations of a possible bird dataset. Do all of these seem plausible?\n\n\n\n\n\n\n\nThis figure shows different simulations of what, according to our prior, might be reasonable datasets for us to study. Do any of them seem implausible to you? If so, try changing the prior. The goal is to make fake datasets that seem plausible, but which still include the possibility of some surprising observations.\nWhen you have a prior that generates observations that cover a range of scientifically reasonable values, then you are ready to move on to fitting real data.\nAnd then translate it into rstanarm.\n\n\n\nThe model formula\nBoth lme4 and rstanarm use the same classic R formula syntax:\nbirds_seen ~ 1\nWe’re using ~ 1 because our model is very simple, requiring only an intercept.\nBecause it is a Poisson distribution, we also specify the response distribution here, via the family argument.\n\n\nrstanarm code for prior simulation\n\n## make the dataset\nbird_simulation &lt;- data.frame(bird_count = bird_count)\n\nbird_model &lt;- stan_glm(bird_count ~ 1, \n                       family = poisson(link = \"identity\"),\n                       data = bird_simulation,\n                       refresh = 0L,\n                       ## PRIOR ONLY\n                       prior_PD = TRUE,\n                       prior_intercept = normal(location = 30, scale = 10)\n)\n\nbird_model\n\nstan_glm\n family:       poisson [identity]\n formula:      bird_count ~ 1\n observations: 21\n predictors:   1\n------\n            Median MAD_SD\n(Intercept) 30.5   10.0  \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\nprior_summary(bird_model)\n\nPriors for model 'bird_model' \n------\nIntercept (after predictors centered)\n ~ normal(location = 30, scale = 10)\n------\nSee help('prior_summary.stanreg') for more details\n\n\n\n\nSampling a prior in rstanarm\nSimulating from a prior is so essential that many Bayesian tools allow you to do this directly.\nWhen we run stan_glm, with prior_PD = TRUE, we sample only from the prior. This generates a large number of simulated datasets – the default is 4000! Each time the model samples, it draws a new value for the unobserved average (avg_birds_per_person) and then 22 values for the number of birds seen by each person.\nLet’s pull out just a few of these datasets and visualize them.\nWe’ll use a wonderful package called tidybayes to easily extract posterior draws from the result of rstanarm.\n\nbird_counts_simulated &lt;- tidybayes::add_predicted_draws(\n  bird_simulation, \n  bird_model)\n\nlibrary(tidybayes)\n\nbird_counts_simulated |&gt; \n  ungroup() |&gt; \n  filter(.draw %in% sample(1:4000, replace = FALSE, size = 16)) |&gt;   \n  ggplot(aes(x = .prediction)) + \n  geom_dotplot() + \n  facet_wrap(~.draw)\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n\nPrior simulations of bird observations"
  },
  {
    "objectID": "exercises/01-simulation/index.html#parameter-recovery",
    "href": "exercises/01-simulation/index.html#parameter-recovery",
    "title": "Introduction to Simulation for validating a model",
    "section": "Parameter recovery",
    "text": "Parameter recovery\nLet’s go back and look at the fake datasets we created in R\n\navg_birds_per_person\n\n[1] 17.12789\n\nbird_count\n\n [1] 23 10 19 27 20 15 16 18 18 22 14 14 14 18 17 13 26 19 16 13 10\n\n\nand let’s see if we can recapture the only known parameter, avg_birds_per_person, which is equal to 17.127887.\nWe’ll do it first in R, using the function fitdistr from the MASS package:\n\nMASS::fitdistr(bird_count, dpois, start = list(lambda=10))\n\nWarning in stats::optim(x = c(23L, 10L, 19L, 27L, 20L, 15L, 16L, 18L, 18L, : one-dimensional optimization by Nelder-Mead is unreliable:\nuse \"Brent\" or optimize() directly\n\n\n     lambda  \n  17.2382812 \n ( 0.9060239)\n\n\nThis could also be done with glm\n\nbird_glm &lt;- glm(bird_count ~ 1, family = \"poisson\")\nexp(coef(bird_glm))\n\n(Intercept) \n    17.2381 \n\n\nYou can see that in all cases we are getting close to the value of avg_birds_per_person, which in these simulations is the true value."
  },
  {
    "objectID": "exercises/01-simulation/index.html#parameter-recovery-in-rstanarm-sampling-the-posterior",
    "href": "exercises/01-simulation/index.html#parameter-recovery-in-rstanarm-sampling-the-posterior",
    "title": "Introduction to Simulation for validating a model",
    "section": "Parameter recovery in rstanarm – sampling the posterior",
    "text": "Parameter recovery in rstanarm – sampling the posterior\nTime for the HMC Slides!\n\n\n\n\n\n\nEXERCISE: parameter recovery in Stan\n\n\n\nUse the Stan code above to fit the model to our simulated data. Do we recover the parameters? That is, rerun the example above but change the prior_PD argument to “FALSE”\n\n\n\n\n\n\nSOLUTION\n\n\n\n\n\nthe brms syntax is only slightly changed! note the different filename and the different object name as well:\n\nbird_posterior &lt;- stan_glm(bird_count ~ 1, \n                       family = poisson(link = \"identity\"),\n                       data = bird_simulation,\n                       refresh = 0L,\n                       ## PRIOR ONLY\n                       prior_PD = FALSE,\n                       prior_intercept = normal(location = 30, scale = 10)\n)\n\nsummary(bird_posterior)\n\n\nModel Info:\n function:     stan_glm\n family:       poisson [identity]\n formula:      bird_count ~ 1\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 21\n predictors:   1\n\nEstimates:\n              mean   sd   10%   50%   90%\n(Intercept) 17.4    0.9 16.2  17.4  18.6 \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 17.4    1.3 15.8  17.4  19.0 \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.0  1.0  1352 \nmean_PPD      0.0  1.0  2146 \nlog-posterior 0.0  1.0   986 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1)."
  },
  {
    "objectID": "exercises/01-simulation/index.html#visualize-everything",
    "href": "exercises/01-simulation/index.html#visualize-everything",
    "title": "Introduction to Simulation for validating a model",
    "section": "Visualize everything!",
    "text": "Visualize everything!\nBayesian workflows are highly visual. Make as many plots as you can: of your parameters, your predictions, the performance of your chains, etc.\nAnother essential package for working with posterior samples is called bayesplot. Let’s use it to compare the posterior and prior distribution for the intercept.\n\n# tidybayes::get_variables(bird_posterior)\n\nbayesplot::mcmc_areas(bird_posterior) + \n  geom_vline(xintercept = avg_birds_per_person, col = \"orange\", lwd = 2)\n\n\n\n\nposterior distribution for avg_birds_per_person. The orange line is the true parameter value, which we simulated in R.\n\n\n\n\n\nPosterior predictive checks\nBayesian models MAKE data, which suggests a clear way to validate our models: ask the model to make some data, then see how well these data correspond to biology (e.g. to our real data). Here, we will take 50 fake datasets of bird counts and compare them to the simulation we first did in R.\nThe process involves a bit of fiddling around in R to get the simulated data, but then bayesplot does all the work:\n\nrstanarm::pp_check(bird_posterior, plotfun = \"dens_overlay\")\n\n\n\n\n\n\n\n\n\n\nShinystan\n\nshinystan::launch_shinystan(bird_posterior)"
  },
  {
    "objectID": "exercises/01-simulation/index.html#exercises",
    "href": "exercises/01-simulation/index.html#exercises",
    "title": "Introduction to Simulation for validating a model",
    "section": "Exercises",
    "text": "Exercises\n\nLevel 1\n\nWhat would you do next to add complexity the bird-counting model above?\nWe plotted histograms to evaluate our model. Experiment with other types of plots. For example, what is the maximum value in each posterior simulation? What is the minimum? How to these compare to the real data? TIP: check out ?bayesplot::`PPC-overview`\n\n\n\nLevel 2\n\nTry to fit YOUR data to this model!. Check to see if the distribution you chose is implemented in Stan – see, for example, this list.\nCheck your model using the plots we have already seen today.\n\n\n\nLevel 3\n\nYou would never actually do the analysis in this exercise! If all you want is the average of a Poisson distribution, you can get that without any sampling at all. Start by writing the model with a different prior:\n\n\\[\n\\begin{align}\n\\text{Number of Birds}_{\\text{seen by person i}} &\\sim \\text{Poisson}(\\lambda) \\\\\n\\lambda &\\sim \\text{Gamma}(9, .5)\n\\end{align}\n\\]\nThis lets us calculate the posterior distribution directly. See the equation on Wikipedia and calculate the posterior for our bird data."
  },
  {
    "objectID": "exercises/01-simulation/index.html#footnotes",
    "href": "exercises/01-simulation/index.html#footnotes",
    "title": "Introduction to Simulation for validating a model",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nin Andrew’s experience anyway!↩︎"
  },
  {
    "objectID": "exercises/02_regression/index.html",
    "href": "exercises/02_regression/index.html",
    "title": "Univariate regression",
    "section": "",
    "text": "library(ggplot2)\nsuppressPackageStartupMessages(library(dplyr))\nlibrary(tidybayes)\nsuppressPackageStartupMessages(library(rstanarm))"
  },
  {
    "objectID": "exercises/02_regression/index.html#load-packages-and-data",
    "href": "exercises/02_regression/index.html#load-packages-and-data",
    "title": "Univariate regression",
    "section": "",
    "text": "library(ggplot2)\nsuppressPackageStartupMessages(library(dplyr))\nlibrary(tidybayes)\nsuppressPackageStartupMessages(library(rstanarm))"
  },
  {
    "objectID": "exercises/02_regression/index.html#statistical-models-of-penguin-bill-morphology.",
    "href": "exercises/02_regression/index.html#statistical-models-of-penguin-bill-morphology.",
    "title": "Univariate regression",
    "section": "Statistical models of Penguin bill morphology.",
    "text": "Statistical models of Penguin bill morphology.\nWe’ll be studying the relationship between two numbers about penguin bills. Specifically, we’ll ask “Are longer bills also deeper?”. This question might not be the most interesting ecologically, but it is a great chance to practice some interesting stats.\nLet’s begin with plotting the data:\n\npenguins |&gt; \n  ggplot(aes(x = bill_len, y = bill_dep)) + \n  geom_point() + \n  stat_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nBill depth (mm) as predicted by bill length (mm) across the entire palmerpenguins dataset.\n\n\n\n\nLet’s write a simple statistical model for these data:\n\\[\n\\begin{align}\n\\text{Bill depth}_i &\\sim \\text{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\beta_0 + \\beta_1\\times\\text{Bill length}_i \\\\\n\\beta_0 &\\sim \\text{Normal}(??) \\\\\n\\beta_1 &\\sim \\text{Normal}(??) \\\\\n\\sigma &\\sim \\text{Exponential}(??)\n\\end{align}\n\\]\nWhat should our priors be? Before we can answer that, we have a more important question:\n\n\n\n\n\n\nWHERE IS ZERO??\n\n\n\nIt has to be somewhere. Does it make sense? take control and choose for yourself.\n\n\nIf we fit a model like this without thinking about the location of zero, we get some pretty silly answers:\n\ncoef(lm(bill_dep ~ bill_len, data = penguins))\n\n(Intercept)    bill_len \n20.88546832 -0.08502128 \n\n\nWhen the value of bill length is 0, the average of the response is the intercept:\n\\[\n\\begin{align}\n\\mu_i &= \\beta_0 + \\beta_1\\times\\text{Bill length}_i \\\\\n\\mu_i &= \\beta_0 + \\beta_1\\times0 \\\\\n\\mu_i &= \\beta_0 \\\\\n\\end{align}\n\\]\nBut, if we take the data as we found it, we’re going to be talking about \\(\\beta_0\\) as the depth of a penguin’s bill when the bill has 0 length! Clearly that isn’t a very meaningful value. From the point of view of setting priors and interpreting coefficients, it helps a lot to set a meaningful 0.\nA very common choice is to subtract the average from your independent variable, so that penguins with an average bill length now have an average of 0:\n\\[\n\\begin{align}\n\\text{Bill depth}_i &\\sim \\text{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\beta_0 + \\beta_1\\times(\\text{Bill length}_i  - \\overline{\\text{Bill length}})\\\\\n\\beta_0 &\\sim \\text{Normal}(??) \\\\\n\\beta_1 &\\sim \\text{Normal}(??)\n\\end{align}\n\\]\nNow \\(\\beta_0\\) means the average bill depth at the average bill length. It becomes easier to think about priors:\n\\[\n\\begin{align}\n\\text{Bill depth}_i &\\sim \\text{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\beta_0 + \\beta_1\\times(\\text{Bill length}_i  - \\overline{\\text{Bill length}})\\\\\n\\beta_0 &\\sim \\text{Normal}(17,2) \\\\\n\\beta_1 &\\sim \\text{Normal}(0,.5) \\\\\n\\sigma &\\sim \\text{Exponential}(0.5)\n\\end{align}\n\\]\n\n\n\n\n\n\nExercise\n\n\n\nWhat continuous predictors have you used in your analysis? How would you find a biologically meaningful zero? Think about how you would center time, age, mass, fitness etc."
  },
  {
    "objectID": "exercises/02_regression/index.html#prior-predictive-simulations",
    "href": "exercises/02_regression/index.html#prior-predictive-simulations",
    "title": "Univariate regression",
    "section": "Prior predictive simulations",
    "text": "Prior predictive simulations\nArmed with this model, it becomes much easier to think about prior predictions.\nWe’ll make a bunch of lines implied by the equation above. There’s two steps:\n\nCenter the predictor\nMake up a vector that goes from the minimum to the maximum of the predictor. This is just for convenience!\n\n\nbill_len_centered &lt;- with(penguins,\n                          bill_len - mean(bill_len,\n                                                na.rm = TRUE))\n\n## make up a short vector\nsome_bill_lengths &lt;- seq(\n  from = min(bill_len_centered, na.rm = TRUE), \n  to = max(bill_len_centered, na.rm = TRUE),\n  length.out = 10\n  )\n\n\n\n\n\n\n\nShortcuts to these common tasks\n\n\n\nThese tasks are so common that they are automated in helper functions.\nFor centering predictors, see the base R function ?scale (however, doing this by hand is often more convenient)\nFor creating a short vector over the range of a predictor, see modelr::seq_range. The R package modelr has many different functions to help with modelling.\n\n\nTo simulate, we’ll use some matrix algebra, as we saw in lecture:\n\nslopes &lt;- rnorm(7, 0, .5)\ninters &lt;- rnorm(7, 17, 2)\n\nX &lt;- cbind(1, some_bill_lengths)\nB &lt;- rbind(inters, slopes)\n\nknitr::kable(head(X))\n\n\n\n\n\nsome_bill_lengths\n\n\n\n\n1\n-11.8219298\n\n\n1\n-8.7663743\n\n\n1\n-5.7108187\n\n\n1\n-2.6552632\n\n\n1\n0.4002924\n\n\n1\n3.4558480\n\n\n\n\nknitr::kable(head(B))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninters\n15.5371199\n19.1658140\n16.8749622\n14.8732681\n13.493175\n18.6040575\n17.2760168\n\n\nslopes\n0.2915136\n-0.5075052\n0.3472609\n-0.8665946\n-1.097236\n-0.0070939\n0.3765543\n\n\n\n\nprior_mus &lt;- X %*% B\n\nmatplot(x = some_bill_lengths,\n        y = prior_mus, type = \"l\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCopy the code above. Increase the number of simulations. Which priors are too wide? Which are too narrow?\n\n\n\nSimulating Observations\nThere are always at least TWO kinds of predictions we can be thinking about:\n\nPredicted averages. This is often called a “confidence” interval for a regression line.\nPredicted observations. This is often called a “prediction” interval.\n\nWe can use the full model to simulate observations!\n\nslopes &lt;- rnorm(7, 0, .5)\ninters &lt;- rnorm(7, 17, 2)\nsigmas &lt;- rexp(7, rate = 0.3)\n\nX &lt;- cbind(1, some_bill_lengths)\nB &lt;- rbind(inters, slopes)\n\nprior_mus &lt;- X %*% B\n\nprior_obs &lt;- matrix(0, nrow = nrow(prior_mus), ncol = ncol(prior_mus))\n\nfor (j in 1:ncol(prior_obs)) {\n  prior_obs[,j] &lt;- rnorm(n = nrow(prior_mus),\n                         mean = prior_mus[,j],\n                         sd = sigmas[j])\n}\n\nmatplot(x = some_bill_lengths,\n        y = prior_obs, type = \"p\")\n\n\n\n\n\n\n\n\nTidyverse style for those who indulge:\n\ntibble(\n  sim_id = 1:7,\n  slopes = rnorm(7, 0, .5),\n  inters = rnorm(7, 17, 2),\n  sigmas = rexp(7, rate = 0.2)\n  ) |&gt; \n  mutate(x = list(seq(from = -10, to = 10, length.out = 6))) |&gt; \n  rowwise() |&gt; \n  mutate(avg = list(x * slopes + inters),\n         obs = list(rnorm(length(avg), mean = avg, sd = sigmas)),\n         sim_id = as.factor(sim_id)) |&gt; \n  tidyr::unnest(cols = c(\"x\", \"avg\", \"obs\")) |&gt; \n  ggplot(aes(x= x, y = avg, group = sim_id, fill = sim_id)) + \n  geom_line(aes(colour = sim_id)) + \n  geom_point(aes(y = obs, fill = sim_id), pch = 21, size = 3) + \n  scale_fill_brewer(type = \"qual\") + \n  scale_colour_brewer(type = \"qual\") + \n  facet_wrap(~sim_id)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEXERCISE\n\n\n\nPick one of the two simulations above and modify it. Here are some suggested modifications:\n\nExperiment with priors that are “too narrow” or “too wide”.\nTry a different distribution than the one used\nInstead of bill size, imagine that we are applying this model to YOUR data. What would you change?"
  },
  {
    "objectID": "exercises/02_regression/index.html#linear-regression-in-rstanarm",
    "href": "exercises/02_regression/index.html#linear-regression-in-rstanarm",
    "title": "Univariate regression",
    "section": "Linear regression in rstanarm",
    "text": "Linear regression in rstanarm\nNow we write some rstanarm code for this model. We’ll begin with a simple model that has no posterior predictions:\n\n## get data ready\npeng_dep_len_df &lt;- penguins |&gt; \n  tidyr::drop_na(bill_dep, bill_len) |&gt; \n  mutate(bill_len_cen = bill_len - mean(bill_len))\n\n## fit model with rstanarm\nnormal_reg_stan &lt;- stan_glm(\n  bill_dep ~ 1 + bill_len_cen,\n  data = peng_dep_len_df,\n  family = gaussian(),\n\n  # priors\n  prior = normal(0, 0.5),          # slope (bill_len_cen)\n  prior_intercept = normal(17, 2), # intercept\n  prior_aux = exponential(0.5),    # sigma\n\n  chains = 4,\n  iter = 2000,\n  refresh = 0,\n  seed = 525600\n)\n\nnormal_reg_stan\n\nstan_glm\n family:       gaussian [identity]\n formula:      bill_dep ~ 1 + bill_len_cen\n observations: 342\n predictors:   2\n------\n             Median MAD_SD\n(Intercept)  17.1    0.1  \nbill_len_cen -0.1    0.0  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 1.9    0.1   \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nget the variable names, which will show us the names of parameters we can plot later:\n\ntidybayes::get_variables(normal_reg_stan)\n\n[1] \"(Intercept)\"   \"bill_len_cen\"  \"sigma\"         \"accept_stat__\"\n[5] \"stepsize__\"    \"treedepth__\"   \"n_leapfrog__\"  \"divergent__\"  \n[9] \"energy__\"     \n\n\n\nnormal_reg_stan |&gt; \n  bayesplot::mcmc_areas(pars = c(\"bill_len_cen\", \"(Intercept)\", \"sigma\"))\n\n\n\n\n\n\n\np &lt;- normal_reg_stan |&gt; \n  bayesplot::mcmc_areas(pars = \"bill_len_cen\") + \n  coord_cartesian(xlim = c(-0.16, 0.16))  \n\n\n\n\n\n\n\nEXERCISE\n\n\n\nDiscussion : Look just at the posterior distribution of the slope right above. Do we have evidence that there’s a relationship between bill length and bill depth?"
  },
  {
    "objectID": "exercises/02_regression/index.html#posterior-predictions-in-r",
    "href": "exercises/02_regression/index.html#posterior-predictions-in-r",
    "title": "Univariate regression",
    "section": "Posterior predictions in R",
    "text": "Posterior predictions in R\nWe can calculate a posterior prediction line directly in R for these data. I’ll show each step in this workflow separately:\n\nnormal_reg_stan |&gt; \n  tidybayes::spread_rvars(bill_len_cen, `(Intercept)`, sigma)\n\n# A tibble: 1 × 3\n     bill_len_cen `(Intercept)`        sigma\n       &lt;rvar[1d]&gt;    &lt;rvar[1d]&gt;   &lt;rvar[1d]&gt;\n1  -0.085 ± 0.019      17 ± 0.1  1.9 ± 0.073\n\n\ntidybayes helps us extract the posterior distribution of the parameters into a convenient object called an rvar. Learn more about tidybayes here and about the rvar datatype here\nNext we combine these posteriors with a vector of observations to make a posterior distribution of LINES:\n\nnormal_reg_predline &lt;- normal_reg_stan |&gt; \n  tidybayes::spread_rvars(bill_len_cen, `(Intercept)`) |&gt; \n  tidyr::expand_grid(x = seq(from = -15, to = 15, length.out = 5)) |&gt; \n  mutate(mu = `(Intercept)` + bill_len_cen*x)\n\nnormal_reg_predline\n\n# A tibble: 5 × 4\n     bill_len_cen `(Intercept)`     x         mu\n       &lt;rvar[1d]&gt;    &lt;rvar[1d]&gt; &lt;dbl&gt; &lt;rvar[1d]&gt;\n1  -0.085 ± 0.019      17 ± 0.1 -15    18 ± 0.30\n2  -0.085 ± 0.019      17 ± 0.1  -7.5  18 ± 0.18\n3  -0.085 ± 0.019      17 ± 0.1   0    17 ± 0.10\n4  -0.085 ± 0.019      17 ± 0.1   7.5  17 ± 0.18\n5  -0.085 ± 0.019      17 ± 0.1  15    16 ± 0.31\n\n\nFinally we’ll plot these:\n\nnormal_reg_predline |&gt; \n  ggplot(aes(x = x, dist = mu)) + \n  tidybayes::stat_lineribbon() + \n  geom_point(aes(x = bill_len_cen, y = bill_dep),\n             inherit.aes = FALSE,\n             data = peng_dep_len_df)\n\n\n\n\n\n\n\n\nThe above workflow makes a nice figure, but perhaps it helps to see the individual lines to understand what is happening here. We can get these with another handy function, unnest_rvars()\n\nnormal_reg_predline |&gt; \n  unnest_rvars() |&gt; \n  ggplot(aes(x = x, y = mu)) + \n  # tidybayes::stat_lineribbon() +\n  geom_line(aes(group = .draw), alpha = 5/100) + \n  geom_point(aes(x = bill_len_cen, y = bill_dep),\n             inherit.aes = FALSE,\n             data = peng_dep_len_df)"
  },
  {
    "objectID": "exercises/02_regression/index.html#posterior-predicted-observations",
    "href": "exercises/02_regression/index.html#posterior-predicted-observations",
    "title": "Univariate regression",
    "section": "Posterior predicted observations",
    "text": "Posterior predicted observations\nThere is a SECOND kind of prediction that we can make using this line. Instead of looking at the average of the line, we look at the possible observations around the line.\n\ndata.frame(bill_len_cen = seq(from = -15, to = 15, length.out = 5)) |&gt; \n  tidybayes::add_predicted_rvars(normal_reg_stan) |&gt; \n  ggplot(aes(x = bill_len_cen, ydist = .prediction)) + \n  stat_lineribbon() +\n  geom_point(aes(x = bill_len_cen, y = bill_dep),\n             inherit.aes = FALSE,\n             data = peng_dep_len_df) + \n  scale_fill_brewer(palette = \"Greens\", direction = -1)\n\n\n\n\n\n\n\n\n\nsummary(normal_reg_stan)\n\n\nModel Info:\n function:     stan_glm\n family:       gaussian [identity]\n formula:      bill_dep ~ 1 + bill_len_cen\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 342\n predictors:   2\n\nEstimates:\n               mean   sd   10%   50%   90%\n(Intercept)  17.1    0.1 17.0  17.1  17.3 \nbill_len_cen -0.1    0.0 -0.1  -0.1  -0.1 \nsigma         1.9    0.1  1.8   1.9   2.0 \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 17.1    0.1 17.0  17.2  17.3 \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.0  1.0  3798 \nbill_len_cen  0.0  1.0  3596 \nsigma         0.0  1.0  3808 \nmean_PPD      0.0  1.0  3607 \nlog-posterior 0.0  1.0  2082 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n\n\n\n\n\n\n\n\nEXERCISE\n\n\n\nExtend this model to include species. Specifically, let each species have its own value of the intercept. This involves combining this regression example with the previous activity on discrete predictors.\nWhen you’re done, look at the resulting summary of coefficients. What do you notice that’s different?\n\n\n\n\n\n\n\n\nSOLUTION\n\n\n\n\n\n\n## dataset\nglimpse(peng_dep_len_df)\n\nRows: 342\nColumns: 9\n$ species      &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, A…\n$ island       &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgersen, To…\n$ bill_len     &lt;dbl&gt; 39.1, 39.5, 40.3, 36.7, 39.3, 38.9, 39.2, 34.1, 42.0, 37.…\n$ bill_dep     &lt;dbl&gt; 18.7, 17.4, 18.0, 19.3, 20.6, 17.8, 19.6, 18.1, 20.2, 17.…\n$ flipper_len  &lt;int&gt; 181, 186, 195, 193, 190, 181, 195, 193, 190, 186, 180, 18…\n$ body_mass    &lt;int&gt; 3750, 3800, 3250, 3450, 3650, 3625, 4675, 3475, 4250, 330…\n$ sex          &lt;fct&gt; male, female, female, female, male, female, male, NA, NA,…\n$ year         &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 200…\n$ bill_len_cen &lt;dbl&gt; -4.82193, -4.42193, -3.62193, -7.22193, -4.62193, -5.0219…\n\n## fit model (rstanarm)\nbill_dep_len_sp_stan &lt;- stan_glm(\n  bill_dep ~ 0 + bill_len_cen + species,\n  data = peng_dep_len_df,\n  family = gaussian(),\n\n  prior = normal(c(17, 17, 17, 0), c(2, 2, 2, .5)),   # bill_len_cen\n  prior_intercept = NULL,\n  prior_aux = exponential(0.5),\n\n  chains = 4,\n  iter = 2000,\n  refresh = 0,\n  seed = 525600\n)\n\nbill_dep_len_sp_stan\n\nstan_glm\n family:       gaussian [identity]\n formula:      bill_dep ~ 0 + bill_len_cen + species\n observations: 342\n predictors:   4\n------\n                 Median MAD_SD\nbill_len_cen      0.3    0.0  \nspeciesAdelie    19.7    0.1  \nspeciesChinstrap 17.1    0.2  \nspeciesGentoo    13.6    0.1  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 1.0    0.0   \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\n\ntidyr::expand_grid(bill_len_cen = seq(from = -15, to = 15, length.out = 5),\n                   species = unique(peng_dep_len_df$species)) |&gt;\n  tidybayes::add_epred_rvars(bill_dep_len_sp_stan) |&gt;\n  ggplot(aes(x = bill_len_cen, dist = .epred, group = species)) + \n  stat_dist_lineribbon() + \n  facet_wrap(~species) + \n  geom_point(aes(x = bill_len_cen, y = bill_dep), \n             data = peng_dep_len_df, inherit.aes = FALSE)\n\n\n\n\n\n\n\n\n\nsummary(bill_dep_len_sp_stan)\n\n\nModel Info:\n function:     stan_glm\n family:       gaussian [identity]\n formula:      bill_dep ~ 0 + bill_len_cen + species\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 342\n predictors:   4\n\nEstimates:\n                   mean   sd   10%   50%   90%\nbill_len_cen      0.3    0.0  0.2   0.3   0.3 \nspeciesAdelie    19.7    0.1 19.6  19.7  19.9 \nspeciesChinstrap 17.1    0.2 16.9  17.1  17.3 \nspeciesGentoo    13.6    0.1 13.4  13.6  13.7 \nsigma             1.0    0.0  1.0   1.0   1.1 \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 17.0    0.1 16.9  17.0  17.1 \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n                 mcse Rhat n_eff\nbill_len_cen     0.0  1.0  1661 \nspeciesAdelie    0.0  1.0  1746 \nspeciesChinstrap 0.0  1.0  1963 \nspeciesGentoo    0.0  1.0  1859 \nsigma            0.0  1.0  2331 \nmean_PPD         0.0  1.0  4015 \nlog-posterior    0.0  1.0  1731 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1)."
  },
  {
    "objectID": "exercises/02_regression/index.html#exercise-5",
    "href": "exercises/02_regression/index.html#exercise-5",
    "title": "Univariate regression",
    "section": "Exercise!",
    "text": "Exercise!\n\nWe have one model without species identity as an independent variable, and one which includes species. Look at the difference in \\(\\sigma\\) between these two models. Why did the value change?\nPosterior predictions Compare the model with species identity to the one without it, by performing posterior predictive checks for each of them (e.g. using `pp_check(..., type = \"dens_overlay\")) ) which model do you prefer?"
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#ecological-data-comes-in-groups",
    "href": "slides/06_Simple_hierarchical_model/index.html#ecological-data-comes-in-groups",
    "title": "Hierarchical models",
    "section": "Ecological data comes in groups",
    "text": "Ecological data comes in groups\nWhy do we need them? Biological data is very often in groups:\n\nSpecies\nSites\nHumans: Lab groups, field workers\nTransects or subplots\nIndividuals – measured at different times. or different parts (e.g. multiple leaves from same tree)\n\n.. but so what? How do we respect this structure in our statistical models?"
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#models-that-respect-reality",
    "href": "slides/06_Simple_hierarchical_model/index.html#models-that-respect-reality",
    "title": "Hierarchical models",
    "section": "Models that respect reality",
    "text": "Models that respect reality\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModels should approximate our domain knowledge at least a little.\nHierarchical models are cartoons of similarity.\nNot the mechanism, but kind of close."
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#hierarchical-models-what-is-in-it-for-me",
    "href": "slides/06_Simple_hierarchical_model/index.html#hierarchical-models-what-is-in-it-for-me",
    "title": "Hierarchical models",
    "section": "Hierarchical models: what is in it for me?",
    "text": "Hierarchical models: what is in it for me?\n\ngenerative process – at least, as a cartoon\nbetter estimates through regularization : mathematical skepticism\nuseful for prediction of new groups\nstudying variance for its own sake"
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#hierarchical-models-in-math",
    "href": "slides/06_Simple_hierarchical_model/index.html#hierarchical-models-in-math",
    "title": "Hierarchical models",
    "section": "Hierarchical models: in math",
    "text": "Hierarchical models: in math\n\n\n\\[\n\\begin{align}\n\\text{Body mass}_i &\\sim \\text{Normal}(\\mu_i, \\sigma_{\\text{obs}}) \\\\\n\\mu_i &= \\bar\\beta + \\beta_{\\text{group}[i]} \\\\\n\\bar\\beta &\\sim \\text{Normal}(5, 2) \\\\\n\\beta_{\\text{group}} &\\sim \\text{Normal}(0, 1) \\\\\n\\sigma_{\\text{obs}} &\\sim \\text{Exponential}(.5)\n\\end{align}\n\\]\n\n\\[\n\\begin{align}\n\\text{Body mass}_i &\\sim \\text{Normal}(\\mu_i, \\sigma_{\\text{obs}}) \\\\\n\\mu_i &= \\bar\\beta + \\beta_{\\text{group}[i]} \\\\\n\\bar\\beta &\\sim \\text{Normal}(5, 2) \\\\\n\\beta_{\\text{group}} &\\sim \\text{Normal}(0, \\sigma_{\\text{group}\n}) \\\\\n\\sigma_{\\text{obs}} &\\sim \\text{Exponential}(.5) \\\\\n\\sigma_{\\text{group}} &\\sim \\text{Exponential}(.7)\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#hierarchical-models-in-math-1",
    "href": "slides/06_Simple_hierarchical_model/index.html#hierarchical-models-in-math-1",
    "title": "Hierarchical models",
    "section": "Hierarchical models: in math",
    "text": "Hierarchical models: in math\n\\[\n[\\sigma_{\\text{obs}},\\bar{\\beta},\\beta_{\\text{group}} |\\text{Body mass}] \\propto \\\\\n[\\text{Body mass}|\\sigma_{\\text{obs}},\\bar{\\beta},\\beta_{\\text{group}}] \\times [\\sigma_{\\text{obs}}] \\times [\\bar{\\beta}] \\times [\\beta_{\\text{group}}]\n\\] vs\n\\[\n[\\sigma_{\\text{obs}},\\bar{\\beta},\\beta_{\\text{group}} |\\text{Body mass}] \\propto \\\\\n[\\text{Body mass}|\\sigma_{\\text{obs}},\\bar{\\beta},\\beta_{\\text{group}}] \\times [\\sigma_{\\text{obs}}] \\times [\\bar{\\beta}] \\times [\\beta_{\\text{group}}|\\sigma_{\\text{group}}] \\times [\\sigma_{\\text{group}}]\n\\]"
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#hierarchical-models-in-math-2",
    "href": "slides/06_Simple_hierarchical_model/index.html#hierarchical-models-in-math-2",
    "title": "Hierarchical models",
    "section": "Hierarchical models: in math",
    "text": "Hierarchical models: in math\n\\[\n[\\sigma_{\\text{obs}},\\bar{\\beta},\\beta_{\\text{group}} |\\text{Body mass}] \\propto \\\\\n[\\text{Body mass}|\\sigma_{\\text{obs}},\\bar{\\beta},\\beta_{\\text{group}}] \\times [\\sigma_{\\text{obs}}] \\times [\\bar{\\beta}] \\times [\\beta_{\\text{group}}]\n\\] vs\n\\[\n[\\sigma_{\\text{obs}},\\bar{\\beta},\\beta_{\\text{group}} |\\text{Body mass}] \\propto \\\\\n[\\text{Body mass}|\\sigma_{\\text{obs}},\\bar{\\beta},\\beta_{\\text{group}}] \\times [\\sigma_{\\text{obs}}] \\times [\\bar{\\beta}] \\times \\color{red} {[\\beta_{\\text{group}}|\\sigma_{\\text{group}}]} \\times [\\sigma_{\\text{group}}]\n\\]"
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#hierarchial-models-in-math-one-more-time",
    "href": "slides/06_Simple_hierarchical_model/index.html#hierarchial-models-in-math-one-more-time",
    "title": "Hierarchical models",
    "section": "Hierarchial models in math ONE MORE TIME",
    "text": "Hierarchial models in math ONE MORE TIME\n\nMathematically, the basic structure of a hierarchical model is\n\n\n\\[\\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\mathbf{Z}\\mathbf{b} + \\boldsymbol{\\varepsilon}\\]\n\n\nwhere\n\n\n\n\\(\\mathbf{y}\\) : Vector of response variable\n\\(\\mathbf{X}\\) : Matrix of explanatory variables on which no hierarchies are accounted for\n\\(\\mathbf{Z}\\) : Matrix of explanatory variables on which hierarchies are accounted for\n\\(\\boldsymbol{\\beta}\\) : parameter estimated without a hierarchy\n\\(\\mathbf{b}\\) : parameter estimated with a hierarchy\n\\(\\boldsymbol{\\varepsilon}\\) : a vector that follows a Gaussian distribution such that \\({\\cal N}(0, \\sigma^2)\\)"
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#hierarchical-models-in-code",
    "href": "slides/06_Simple_hierarchical_model/index.html#hierarchical-models-in-code",
    "title": "Hierarchical models",
    "section": "Hierarchical models: in code",
    "text": "Hierarchical models: in code\nThe “|”\n\nhandy but a giant source of confusion\nread it like “(things groups do differently | names of the groups)”\nadds parameters to a model"
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#hierarchy-on-the-intercept",
    "href": "slides/06_Simple_hierarchical_model/index.html#hierarchy-on-the-intercept",
    "title": "Hierarchical models",
    "section": "Hierarchy on the intercept",
    "text": "Hierarchy on the intercept"
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#hierarchy-on-the-slopes",
    "href": "slides/06_Simple_hierarchical_model/index.html#hierarchy-on-the-slopes",
    "title": "Hierarchical models",
    "section": "Hierarchy on the slopes",
    "text": "Hierarchy on the slopes"
  },
  {
    "objectID": "slides/01-03-building-plotting/index.html#illustrative-datasets",
    "href": "slides/01-03-building-plotting/index.html#illustrative-datasets",
    "title": "Introduction to the Datasets",
    "section": "Illustrative datasets",
    "text": "Illustrative datasets\nTo illustrate the different models and methods we will discuss in this course, we will rely on a few data sets, which are directly available in different R packages\n\n\nmite, mite.env and mite.xy available in the vegan R package\n\n\n\n\npenguins available in the palmerpenguins R package\n\n\n\nThese datasets are practical because they are manageable in size and will allow you to see how to work out the different example presented in this course.\n\n\nLet’s look at them in more details…"
  },
  {
    "objectID": "slides/01-03-building-plotting/index.html#oribatid-mite-data",
    "href": "slides/01-03-building-plotting/index.html#oribatid-mite-data",
    "title": "Introduction to the Datasets",
    "section": "Oribatid mite data",
    "text": "Oribatid mite data\nAside from being very interesting, this dataset has been sampled at the Station biologique des Laurentides, so ~200 km north-west from here.\n\nSampling was carried out in June 1989 on the partially floating vegetation mat surrounding a lake, from the forest border to the free water by Daniel Borcard."
  },
  {
    "objectID": "slides/01-03-building-plotting/index.html#oribatid-mite-data-1",
    "href": "slides/01-03-building-plotting/index.html#oribatid-mite-data-1",
    "title": "Introduction to the Datasets",
    "section": "Oribatid mite data",
    "text": "Oribatid mite data\n\nOribatid mites are small (usually ranging in size from 0.2 to 1.4 mm) invertebrates that are part of the Arachnida class (so they have 8 legs).\n\n\n\n\n\n\n\nIn the mite data, 35 morphospecies were identified and counted across 70 samples."
  },
  {
    "objectID": "slides/01-03-building-plotting/index.html#sites-coordinates",
    "href": "slides/01-03-building-plotting/index.html#sites-coordinates",
    "title": "Introduction to the Datasets",
    "section": "Sites coordinates",
    "text": "Sites coordinates\nmite.xy"
  },
  {
    "objectID": "slides/01-03-building-plotting/index.html#vegetation-cover",
    "href": "slides/01-03-building-plotting/index.html#vegetation-cover",
    "title": "Introduction to the Datasets",
    "section": "Vegetation cover",
    "text": "Vegetation cover\nmite.env"
  },
  {
    "objectID": "slides/01-03-building-plotting/index.html#microtopography-and-shrub-cover",
    "href": "slides/01-03-building-plotting/index.html#microtopography-and-shrub-cover",
    "title": "Introduction to the Datasets",
    "section": "Microtopography and shrub cover",
    "text": "Microtopography and shrub cover\nmite.env"
  },
  {
    "objectID": "slides/01-03-building-plotting/index.html#substrate-density-and-water-content",
    "href": "slides/01-03-building-plotting/index.html#substrate-density-and-water-content",
    "title": "Introduction to the Datasets",
    "section": "Substrate density and water content",
    "text": "Substrate density and water content\nmite.env"
  },
  {
    "objectID": "slides/01-03-building-plotting/index.html#getting-the-data",
    "href": "slides/01-03-building-plotting/index.html#getting-the-data",
    "title": "Introduction to the Datasets",
    "section": "Getting the data",
    "text": "Getting the data"
  },
  {
    "objectID": "slides/01-03-building-plotting/index.html#palmer-penguins",
    "href": "slides/01-03-building-plotting/index.html#palmer-penguins",
    "title": "Introduction to the Datasets",
    "section": "Palmer penguins",
    "text": "Palmer penguins\n\nThe Palmer Archipelago penguins. Artwork by @allison_horst\nThese data were collected from 2007 to 2009 by Dr. Kristen Gorman with the Palmer Station Long Term Ecological Research Program, part of the US Long Term Ecological Research Network.\n\n\n\nThe data were imported directly from the Environmental Data Initiative (EDI) Data Portal, and are available for use by CC0 license (“No Rights Reserved”) in accordance with the Palmer Station Data Policy. [@gorman2014; @horst2020]\n\nhttps://allisonhorst.github.io/palmerpenguins/"
  },
  {
    "objectID": "slides/01-03-building-plotting/index.html#behold-simpsons-paradox",
    "href": "slides/01-03-building-plotting/index.html#behold-simpsons-paradox",
    "title": "Introduction to the Datasets",
    "section": "Behold: Simpson’s Paradox!",
    "text": "Behold: Simpson’s Paradox!"
  },
  {
    "objectID": "slides/01-03-building-plotting/index.html#behold-simpsons-paradox-1",
    "href": "slides/01-03-building-plotting/index.html#behold-simpsons-paradox-1",
    "title": "Introduction to the Datasets",
    "section": "Behold: Simpson’s Paradox!",
    "text": "Behold: Simpson’s Paradox!"
  },
  {
    "objectID": "slides/01-03-building-plotting/index.html#theres-lots-more",
    "href": "slides/01-03-building-plotting/index.html#theres-lots-more",
    "title": "Introduction to the Datasets",
    "section": "There’s lots more!",
    "text": "There’s lots more!\n\n\n\nand also see the official site: https://allisonhorst.github.io/palmerpenguins/"
  },
  {
    "objectID": "slides/01-04-Linear_model/index.html#basic-regression-model",
    "href": "slides/01-04-Linear_model/index.html#basic-regression-model",
    "title": "Linear models",
    "section": "Basic regression model",
    "text": "Basic regression model\n\nHierarchical models are a generalized version of the classic regression models you have seen in your undergraduate courses.\n\n\n\nIn its simplest form, a regression model is usually presented as\n\n\n\n\\[\ny_i = \\beta_0 + \\beta_1 x_{i} + \\varepsilon\n\\]\n\n\n\nIt is known as a simple linear model, where :\n\n\n\n\n\\(y_i\\) is the value of a response variable for observation \\(i\\)\n\n\n\n\n\\(x_i\\) is the value of an explanatory variable for observation \\(i\\)\n\n\n\n\n\\(\\beta_0\\) is the model intercept\n\n\n\n\n\\(\\beta_1\\) is the model slope\n\n\n\n\n\\(\\varepsilon\\) is the error term"
  },
  {
    "objectID": "slides/01-04-Linear_model/index.html#basic-regression-model-1",
    "href": "slides/01-04-Linear_model/index.html#basic-regression-model-1",
    "title": "Linear models",
    "section": "Basic regression model",
    "text": "Basic regression model\nThe cool thing about the simple linear model is that it can be studied visually quite easily.\n\nFor example if we are interested in knowing how a newly discovered plant species (Bidonia exemplaris) reacts to humidity, we can relate the biomass of B. exemplaris sampled at 100 sites with the soil humidity content and readily visual the data and the trend."
  },
  {
    "objectID": "slides/01-04-Linear_model/index.html#basic-regression-model-2",
    "href": "slides/01-04-Linear_model/index.html#basic-regression-model-2",
    "title": "Linear models",
    "section": "Basic regression model",
    "text": "Basic regression model"
  },
  {
    "objectID": "slides/01-04-Linear_model/index.html#regression-parameters",
    "href": "slides/01-04-Linear_model/index.html#regression-parameters",
    "title": "Linear models",
    "section": "Regression parameters",
    "text": "Regression parameters\nGenerally, the slope and the intercept are the regression parameters we focus on when studying the simple linear model, but there is another one that is very important to consider, especially for this course.\n\nAny ideas which one it is ?"
  },
  {
    "objectID": "slides/01-04-Linear_model/index.html#regression-parameters-1",
    "href": "slides/01-04-Linear_model/index.html#regression-parameters-1",
    "title": "Linear models",
    "section": "Regression parameters",
    "text": "Regression parameters\n\nIf we go back to the mathematical description of the model\n\\[\ny_i = \\beta_0 + \\beta_1 x_{i} + \\varepsilon\n\\]\n\n\n\nwe can see that in the simple linear regression the error term (\\(\\varepsilon\\)) has actually a very precise definition:\n\n\n\n\n\\[\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)\\]\n\n\nwhere \\(\\sigma^2\\) is an estimated variance.\n\n\n\n\nIn words, it means that the error in a simple linear regression follows a Gaussian distribution with a variance that is estimated.\n\n\n\n\nFor most of the course, we will play with the variance parameter \\(\\sigma^2\\) in a bunch of different ways.\n\n\n\n\nBut before we do this, we need to understand a bit more about how this parameter influence the model."
  },
  {
    "objectID": "slides/01-04-Linear_model/index.html#regression-parameters-2",
    "href": "slides/01-04-Linear_model/index.html#regression-parameters-2",
    "title": "Linear models",
    "section": "Regression parameters",
    "text": "Regression parameters\nA first way to do this is to think about the simple linear regression in a slightly different way. Specifically, based on what we learned in the previous slide the simple linear regression can be rewritten as \\[\ny \\sim \\mathcal{N}(\\beta_0 + \\beta_1 x_{i}, \\sigma^2)\n\\]\n\nAs we will see later in this course, this writting style will become particularly useful."
  },
  {
    "objectID": "slides/01-04-Linear_model/index.html#regression-parameters-3",
    "href": "slides/01-04-Linear_model/index.html#regression-parameters-3",
    "title": "Linear models",
    "section": "Regression parameters",
    "text": "Regression parameters\nVariance of the model (\\(\\sigma^2\\))\n\nIn essence, \\(\\sigma^2\\) tells us about what the model could not account for.\n\n\n\nFor example, let’s compare the biomass of Bidonia exemplaris with that of Ilovea hockeyshow, another species (a carnivorous plant)"
  },
  {
    "objectID": "slides/01-04-Linear_model/index.html#regression-parameters-4",
    "href": "slides/01-04-Linear_model/index.html#regression-parameters-4",
    "title": "Linear models",
    "section": "Regression parameters",
    "text": "Regression parameters\nVariance of the model (\\(\\sigma^2\\))\nBy regressing humidity on the biomass of both plants, we can obtain the estimated parameters for each regression (which are all available using summary.lm)\n\n\n# Regression model\nregBexemplaris &lt;- lm(b.exemplaris ~ humidity)\nregIhockeyshow &lt;- lm(i.hockeyshow ~ humidity)\n\n# Summary\nsummaryRegBexemplaris &lt;- summary(regBexemplaris)\nsummaryRegIhockeyshow &lt;- summary(regIhockeyshow)"
  },
  {
    "objectID": "slides/01-04-Linear_model/index.html#regression-parameters-5",
    "href": "slides/01-04-Linear_model/index.html#regression-parameters-5",
    "title": "Linear models",
    "section": "Regression parameters",
    "text": "Regression parameters\nVariance of the model (\\(\\sigma^2\\))\n\n\nFor Bidonia exemplaris\n\n# Estimated coefficients\nsummaryRegBexemplaris$coefficients[,1:2]\n\n            Estimate Std. Error\n(Intercept) 4.015999 0.05235200\nhumidity    1.313897 0.08845811\n\n# Estimated variance\nsummaryRegBexemplaris$sigma\n\n[1] 0.5232624\n\n\n\nFor Ilovea hockeyshow\n\n# Estimated coefficients\nsummaryRegIhockeyshow$coefficients[,1:2]\n\n            Estimate  Std. Error\n(Intercept) 3.991785 0.008928294\nhumidity    1.271250 0.015085956\n\n# Estimated variance\nsummaryRegIhockeyshow$sigma\n\n[1] 0.089239"
  },
  {
    "objectID": "slides/01-04-Linear_model/index.html#limits-of-the-simple-linear-regression",
    "href": "slides/01-04-Linear_model/index.html#limits-of-the-simple-linear-regression",
    "title": "Linear models",
    "section": "Limits of the simple linear regression",
    "text": "Limits of the simple linear regression\nThere are two major pitfalls of the simple linear model for problems in the life sciences\n\n\nOne explanatory is almost never enough to approach biological questions nowadays.\n\n\n\n\nThe simple linear model assumes that the error follows a Gaussian distribution."
  },
  {
    "objectID": "slides/01-04-Linear_model/index.html#multiple-linear-regression",
    "href": "slides/01-04-Linear_model/index.html#multiple-linear-regression",
    "title": "Linear models",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\n\nSimple linear regression can be extended to account for multiple explanatory variables to study more complex problems. This type of regression model is known as a multiple linear regression.\n\n\n\nMathematically, a multiple linear regression can be defined as\n\\[\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_p x_{ip} + \\varepsilon\n\\]\n\n\n\n\nTheoretically, estimating the parameters of a multiple regression model is done the same ways as for simple linear regression. However, technically, matrix algebra is quite practical to use in this context and especially for this course."
  },
  {
    "objectID": "slides/01-04-Linear_model/index.html#multiple-linear-regression-1",
    "href": "slides/01-04-Linear_model/index.html#multiple-linear-regression-1",
    "title": "Linear models",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\n\nIn this course, we will rely heavily on multiple linear regression and expand on it by studying how some of the parameters (the \\(\\beta\\)s) can depend on other other data and parameters.\n\n\nAs we saw earlier, a classic way to write multiple linear regression is\n\\[\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_p x_{ip} + \\varepsilon\n\\]\n\n\nHowever, we can rewrite this using matrix notation has\n\\[\n\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\n\\]"
  },
  {
    "objectID": "slides/01-04-Linear_model/index.html#multiple-linear-regression-2",
    "href": "slides/01-04-Linear_model/index.html#multiple-linear-regression-2",
    "title": "Linear models",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\nMatrix notation\n\\[\n\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\n\\]\nUsing the matrix notation, we assume that\n\n\n\n\\(\\mathbf{y}\\) is a vector of length \\(n\\) (samples)\n\n\n\n\n\n\n\\(\\mathbf{X}\\) is a matrix with \\(n\\) samples (rows) and representing \\(p\\) explanatory variables (columns)\n\n\n\n\n\n\n\\(\\boldsymbol{\\beta}\\) is a vector of \\(p\\) regression coefficients\n\n\n\n\n\n\n\\(\\boldsymbol{\\varepsilon}\\) is a vector of residuals of length \\(n\\)"
  },
  {
    "objectID": "slides/01-04-Linear_model/index.html#error-in-linear-models",
    "href": "slides/01-04-Linear_model/index.html#error-in-linear-models",
    "title": "Linear models",
    "section": "Error in linear models",
    "text": "Error in linear models\nAs previously mentioned, in (simple and multiple!) linear regression the error term (\\(\\varepsilon\\)) has actually a very precise definition:\n\\[\\boldsymbol{\\varepsilon} \\sim \\mathcal{MVN}(0, \\sigma^2\\mathbf{I})\\] where \\(\\sigma^2\\) is an estimated variance\n\nwhich means that the error in a linear regression follows a Gaussian distribution with an estimated variance.\n\n\nNote The model can be written using matrix notation as\n\\[\\mathbf{y} \\sim \\mathcal{MVN}(\\mathbf{X}\\boldsymbol{\\beta}, \\sigma^2\\mathbf{I})\\] where \\(\\sigma^2\\)"
  },
  {
    "objectID": "slides/01-04-Linear_model/index.html#generalized-linear-models",
    "href": "slides/01-04-Linear_model/index.html#generalized-linear-models",
    "title": "Linear models",
    "section": "Generalized linear models",
    "text": "Generalized linear models\n\nIf for some reason we do not want our model to have a Gaussian error, generalized linear models (GLMs) have been proposed. In essence, GLMs use link functions to adapt models for them to be used on non-Gaussian data.\n\n\nMathematically, the generic way to write generalized linear model is\n\\[\n\\widehat{y}_i = g^{-1}(\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_p x_{ip})\n\\]\n\n\nor in matrix notation\n\\[\n\\widehat{\\mathbf{y}} = g^{-1}(\\mathbf{X}\\boldsymbol{\\beta})\n\\]\nwhere \\(g\\) is the link function and \\(g^{-1}\\) the inverse link function."
  },
  {
    "objectID": "slides/01-04-Linear_model/index.html#generalized-linear-models-1",
    "href": "slides/01-04-Linear_model/index.html#generalized-linear-models-1",
    "title": "Linear models",
    "section": "Generalized linear models",
    "text": "Generalized linear models\nLink functions\n\nThere are many types of link functions and they are usually directly associated to the underlying data the analysis is carried out on.\n\n\n\nArguably the most common link function in ecology is\n\n\n\nlogit link function\n\nIt is commonly used for modelling binary (0-1) data.\n\\[\n\\mathbf{X}\\boldsymbol{\\beta} = \\ln\\left(\\frac{\\widehat{\\mathbf{y}}}{1 - \\widehat{\\mathbf{y}}}\\right)\n\\]\n\n\n\n\nThe inverse logit link function is\n\\[\n\\widehat{\\mathbf{y}} = \\frac{\\exp(\\mathbf{X}\\boldsymbol{\\beta})}{1 + \\exp(\\mathbf{X}\\boldsymbol{\\beta})} = \\frac{1}{1 + \\exp(-\\mathbf{X}\\boldsymbol{\\beta})}\n\\]"
  },
  {
    "objectID": "slides/01-04-Linear_model/index.html#generalized-linear-models-2",
    "href": "slides/01-04-Linear_model/index.html#generalized-linear-models-2",
    "title": "Linear models",
    "section": "Generalized linear models",
    "text": "Generalized linear models\nLink functions\nAnother commonly used link function is\n\nlog link function\nIt is commonly used for modelling count data.\n\\[\n\\mathbf{X}\\boldsymbol{\\beta} = \\ln\\left(\\widehat{\\mathbf{y}}\\right)\n\\]\n\n\nThe inverse logit link function is\n\\[\n\\widehat{\\mathbf{y}} = \\exp(\\mathbf{X}\\boldsymbol{\\beta})\n\\]"
  },
  {
    "objectID": "slides/03-01-multilevel-intro/index.html",
    "href": "slides/03-01-multilevel-intro/index.html",
    "title": "index",
    "section": "",
    "text": ". . .\n\nHierarchical models have been implemented in many software packages,\n\n. . .\n\nin R\n\nlme4, brms, nlme, glmmTMB, MCMCglmm, …\n\n\n. . .\n\nin SAS\n\nMIXED, HPMIXED, GLMMIX, …\n\n\n. . .\n\nin Julia\n\nMixedModels.jl\n\n…"
  },
  {
    "objectID": "slides/03-01-multilevel-intro/index.html#implementation",
    "href": "slides/03-01-multilevel-intro/index.html#implementation",
    "title": "index",
    "section": "",
    "text": ". . .\n\nHierarchical models have been implemented in many software packages,\n\n. . .\n\nin R\n\nlme4, brms, nlme, glmmTMB, MCMCglmm, …\n\n\n. . .\n\nin SAS\n\nMIXED, HPMIXED, GLMMIX, …\n\n\n. . .\n\nin Julia\n\nMixedModels.jl\n\n…"
  },
  {
    "objectID": "slides/03-01-multilevel-intro/index.html#a-bit-of-vocabulary",
    "href": "slides/03-01-multilevel-intro/index.html#a-bit-of-vocabulary",
    "title": "index",
    "section": "A bit of vocabulary",
    "text": "A bit of vocabulary\nMultiple Definition of fixed and random effects\n. . .\n\n(Kreft and De Leeuw 1998) Fixed effects are constant and random effect vary\n\n. . .\n\n(Searl et al. 1992) Effects are fixed if they are interesting in themselves or random if there is interest in the underlying population\n\n. . .\n\n(Green and Tukey 1960) When a sample exhausts the population, the corresponding variable is fixed; when the sample is a small (i.e., negligible) part of the population the corresponding variable is random\n\n. . .\n\n(Roy LaMotte 2014) If an effect is assumed to be a realized value of a random variable, it is called a random effect\n\n. . .\n\n(Robinson 1991) Fixed effects are estimated using least squares (or, more generally, maximum likelihood) and random effects are estimated with shrinkage."
  }
]