[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "exercises/01-simulation/index.html",
    "href": "exercises/01-simulation/index.html",
    "title": "Introduction to Simulation for validating a model",
    "section": "",
    "text": "library(tidyverse)\nlibrary(lme4)\nlibrary(rstanarm)"
  },
  {
    "objectID": "exercises/01-simulation/index.html#simple-exercise-in-simulation",
    "href": "exercises/01-simulation/index.html#simple-exercise-in-simulation",
    "title": "Introduction to Simulation for validating a model",
    "section": "Simple exercise in simulation",
    "text": "Simple exercise in simulation\nLet’s imagine we are taking a walk as a group today at this beautiful field site. What is the number of birds (total abundance of ALL species) each of us is going to see on our hike?\n\nSome questions to ask about simulated data\n\nWhat kind of observations are you going to make? Do they have a minimum or maximum value? Are they integers, or are they decimal numbers, or something else?\nWhere do the numbers come from? This could be anything, from simple linear approximations (i.e. the models we’re looking at in this course) to ODEs, mathematical models, GAMs, etc.\nHow many observations will we be making?\n\nOne of the most useful traits of Bayesian models is that they are generative: they can be used to make a simulated dataset. We’ll do that now for our bird example.\nlet’s simulate from a Poisson distribution:\n\nset.seed(525600)\nn_people &lt;- 21\navg_birds_per_person &lt;- runif(1, min = 0, max = 30)\nbird_count &lt;- rpois(n_people, lambda = avg_birds_per_person)\n\nSome things to note in the code above:\nEvery statistical distribution that is in R (which is a lot! almost all! ) has four different functions. If the distribution is called dist, then they are:\n\nrdist = draw random numbers from dist\nqdist = the quantile function – what value gives a certain proportion of the distribution?\npdist = the probability density function – what proportion of the distribution is below a certain value?\nddist the density function = draws the “shape” of a distribution. How probable are specific values?\n\nThe other thing to note is that there are TWO simulation steps here: first, simulating a value of the average (\\(\\lambda\\)) and second, simulating observations. In our model, the Uniform distribution was referred to as the prior, and the Poisson distribution was referred to as a likelihood, but here you can see that they are very nearly the same thing: just statements about what distribution of values might be most consistent with the data.\n\nPlotting the result\nLet’s take a look at our simulated values:\n\nhist(bird_count, col = \"lightblue\", xlim = c(0, 50))\n\n\n\n\nHistogram of simulated counts of birds\n\n\n\n\nThis is pretty great, and represents one possible realization of sampling. However, one sample isn’t enough to tell us about what our \\(\\text{Uniform}(0, 60)\\) prior really means.\n\n\n\n\n\n\nEXERCISE\n\n\n\nTry to make many different simulations (say, 12 simulations). This represents 12 different repeats of the whole process: draw a value from the uniform prior, THEN draw a value from the poisson. Visualize them any way you want! (the worked example below uses ggplot2)\n\n\n\n\n\n\n\n\nSOLUTION\n\n\n\n\n\n\nset.seed(525600)\n\nsimulate_some_birds &lt;- function() {\n  lambda &lt;- runif(1, min = 0, max = 60)\n  tibble(birds_seen = rpois(23, lambda = lambda),\n         lambda = lambda)\n}\n\nbird_simulations &lt;- purrr::map(1:12, function(x) simulate_some_birds()) |&gt; \n  list_rbind(names_to = \"simulation_id\")\n  \n\nbird_simulations |&gt; \n  ggplot(aes(x = birds_seen)) + \n  geom_histogram(bins = 28) + \n  facet_wrap(~simulation_id) + \n  theme_bw() + \n  labs(x = \"Number of birds observed per person\")\n\n\n\n\nTwelve different simulations of a possible bird dataset. Do all of these seem plausible?\n\n\n\n\n\n\n\nThis figure shows different simulations of what, according to our prior, might be reasonable datasets for us to study. Do any of them seem implausible to you? If so, try changing the prior. The goal is to make fake datasets that seem plausible, but which still include the possibility of some surprising observations.\nWhen you have a prior that generates observations that cover a range of scientifically reasonable values, then you are ready to move on to fitting real data.\nAnd then translate it into rstanarm.\n\n\n\nThe model formula\nBoth lme4 and rstanarm use the same classic R formula syntax:\nbirds_seen ~ 1\nWe’re using ~ 1 because our model is very simple, requiring only an intercept.\nBecause it is a Poisson distribution, we also specify the response distribution here, via the family argument.\n\n\nrstanarm code for prior simulation\n\n## make the dataset\nbird_simulation &lt;- data.frame(bird_count = bird_count)\n\nbird_model &lt;- stan_glm(bird_count ~ 1, \n                       family = poisson(link = \"identity\"),\n                       data = bird_simulation,\n                       refresh = 0L,\n                       ## PRIOR ONLY\n                       prior_PD = TRUE,\n                       prior_intercept = normal(location = 30, scale = 10)\n)\n\nbird_model\n\nstan_glm\n family:       poisson [identity]\n formula:      bird_count ~ 1\n observations: 21\n predictors:   1\n------\n            Median MAD_SD\n(Intercept) 30.5   10.0  \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\nprior_summary(bird_model)\n\nPriors for model 'bird_model' \n------\nIntercept (after predictors centered)\n ~ normal(location = 30, scale = 10)\n------\nSee help('prior_summary.stanreg') for more details\n\n\n\n\nSampling a prior in rstanarm\nSimulating from a prior is so essential that many Bayesian tools allow you to do this directly.\nWhen we run stan_glm, with prior_PD = TRUE, we sample only from the prior. This generates a large number of simulated datasets – the default is 4000! Each time the model samples, it draws a new value for the unobserved average (avg_birds_per_person) and then 22 values for the number of birds seen by each person.\nLet’s pull out just a few of these datasets and visualize them.\nWe’ll use a wonderful package called tidybayes to easily extract posterior draws from the result of rstanarm.\n\nbird_counts_simulated &lt;- tidybayes::add_predicted_draws(\n  bird_simulation, \n  bird_model)\n\nlibrary(tidybayes)\n\nbird_counts_simulated |&gt; \n  ungroup() |&gt; \n  filter(.draw %in% sample(1:4000, replace = FALSE, size = 16)) |&gt;   \n  ggplot(aes(x = .prediction)) + \n  geom_dotplot() + \n  facet_wrap(~.draw)\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n\nPrior simulations of bird observations"
  },
  {
    "objectID": "exercises/01-simulation/index.html#parameter-recovery",
    "href": "exercises/01-simulation/index.html#parameter-recovery",
    "title": "Introduction to Simulation for validating a model",
    "section": "Parameter recovery",
    "text": "Parameter recovery\nLet’s go back and look at the fake datasets we created in R\n\navg_birds_per_person\n\n[1] 17.12789\n\nbird_count\n\n [1] 23 10 19 27 20 15 16 18 18 22 14 14 14 18 17 13 26 19 16 13 10\n\n\nand let’s see if we can recapture the only known parameter, avg_birds_per_person, which is equal to 17.127887.\nWe’ll do it first in R, using the function fitdistr from the MASS package:\n\nMASS::fitdistr(bird_count, dpois, start = list(lambda=10))\n\nWarning in stats::optim(x = c(23L, 10L, 19L, 27L, 20L, 15L, 16L, 18L, 18L, : one-dimensional optimization by Nelder-Mead is unreliable:\nuse \"Brent\" or optimize() directly\n\n\n     lambda  \n  17.2382812 \n ( 0.9060239)\n\n\nThis could also be done with glm\n\nbird_glm &lt;- glm(bird_count ~ 1, family = \"poisson\")\nexp(coef(bird_glm))\n\n(Intercept) \n    17.2381 \n\n\nYou can see that in all cases we are getting close to the value of avg_birds_per_person, which in these simulations is the true value."
  },
  {
    "objectID": "exercises/01-simulation/index.html#parameter-recovery-in-rstanarm-sampling-the-posterior",
    "href": "exercises/01-simulation/index.html#parameter-recovery-in-rstanarm-sampling-the-posterior",
    "title": "Introduction to Simulation for validating a model",
    "section": "Parameter recovery in rstanarm – sampling the posterior",
    "text": "Parameter recovery in rstanarm – sampling the posterior\nTime for the HMC Slides!\n\n\n\n\n\n\nEXERCISE: parameter recovery in Stan\n\n\n\nUse the Stan code above to fit the model to our simulated data. Do we recover the parameters? That is, rerun the example above but change the prior_PD argument to “FALSE”\n\n\n\n\n\n\nSOLUTION\n\n\n\n\n\nthe brms syntax is only slightly changed! note the different filename and the different object name as well:\n\nbird_posterior &lt;- stan_glm(bird_count ~ 1, \n                       family = poisson(link = \"identity\"),\n                       data = bird_simulation,\n                       refresh = 0L,\n                       ## PRIOR ONLY\n                       prior_PD = FALSE,\n                       prior_intercept = normal(location = 30, scale = 10)\n)\n\nsummary(bird_posterior)\n\n\nModel Info:\n function:     stan_glm\n family:       poisson [identity]\n formula:      bird_count ~ 1\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 21\n predictors:   1\n\nEstimates:\n              mean   sd   10%   50%   90%\n(Intercept) 17.4    0.9 16.2  17.4  18.6 \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 17.4    1.3 15.8  17.4  19.0 \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.0  1.0  1352 \nmean_PPD      0.0  1.0  2146 \nlog-posterior 0.0  1.0   986 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1)."
  },
  {
    "objectID": "exercises/01-simulation/index.html#visualize-everything",
    "href": "exercises/01-simulation/index.html#visualize-everything",
    "title": "Introduction to Simulation for validating a model",
    "section": "Visualize everything!",
    "text": "Visualize everything!\nBayesian workflows are highly visual. Make as many plots as you can: of your parameters, your predictions, the performance of your chains, etc.\nAnother essential package for working with posterior samples is called bayesplot. Let’s use it to compare the posterior and prior distribution for the intercept.\n\n# tidybayes::get_variables(bird_posterior)\n\nbayesplot::mcmc_areas(bird_posterior) + \n  geom_vline(xintercept = avg_birds_per_person, col = \"orange\", lwd = 2)\n\n\n\n\nposterior distribution for avg_birds_per_person. The orange line is the true parameter value, which we simulated in R.\n\n\n\n\n\nPosterior predictive checks\nBayesian models MAKE data, which suggests a clear way to validate our models: ask the model to make some data, then see how well these data correspond to biology (e.g. to our real data). Here, we will take 50 fake datasets of bird counts and compare them to the simulation we first did in R.\nThe process involves a bit of fiddling around in R to get the simulated data, but then bayesplot does all the work:\n\nrstanarm::pp_check(bird_posterior, plotfun = \"dens_overlay\")\n\n\n\n\n\n\nShinystan\n\nshinystan::launch_shinystan(bird_posterior)"
  },
  {
    "objectID": "exercises/01-simulation/index.html#exercises",
    "href": "exercises/01-simulation/index.html#exercises",
    "title": "Introduction to Simulation for validating a model",
    "section": "Exercises",
    "text": "Exercises\n\nLevel 1\n\nWhat would you do next to add complexity the bird-counting model above?\nWe plotted histograms to evaluate our model. Experiment with other types of plots. For example, what is the maximum value in each posterior simulation? What is the minimum? How to these compare to the real data? TIP: check out ?bayesplot::`PPC-overview`\n\n\n\nLevel 2\n\nTry to fit YOUR data to this model!. Check to see if the distribution you chose is implemented in Stan – see, for example, this list.\nCheck your model using the plots we have already seen today.\n\n\n\nLevel 3\n\nYou would never actually do the analysis in this exercise! If all you want is the average of a Poisson distribution, you can get that without any sampling at all. Start by writing the model with a different prior:\n\n\\[\n\\begin{align}\n\\text{Number of Birds}_{\\text{seen by person i}} &\\sim \\text{Poisson}(\\lambda) \\\\\n\\lambda &\\sim \\text{Gamma}(9, .5)\n\\end{align}\n\\]\nThis lets us calculate the posterior distribution directly. See the equation on Wikipedia and calculate the posterior for our bird data."
  },
  {
    "objectID": "exercises/01-simulation/index.html#footnotes",
    "href": "exercises/01-simulation/index.html#footnotes",
    "title": "Introduction to Simulation for validating a model",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nin Andrew’s experience anyway!↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MMIE02: Mixed Methods in Ecology",
    "section": "",
    "text": "What is a Linear Model? \nIntro to some useful datasets\nLinear models in more detail\nProbability distributions\nsimulation exercise"
  },
  {
    "objectID": "index.html#monday-02-feb",
    "href": "index.html#monday-02-feb",
    "title": "MMIE02: Mixed Methods in Ecology",
    "section": "",
    "text": "What is a Linear Model? \nIntro to some useful datasets\nLinear models in more detail\nProbability distributions\nsimulation exercise"
  },
  {
    "objectID": "slides/01-01-intro/index.html#what-we-want-to-do-during-this-course",
    "href": "slides/01-01-intro/index.html#what-we-want-to-do-during-this-course",
    "title": "Introduction",
    "section": "What we want to do during this course",
    "text": "What we want to do during this course\n\nGeneral overview of linear models\nProbability distributions\nFrequentist and Bayesian approaches\nGLMMs for different kinds of data"
  },
  {
    "objectID": "slides/01-01-intro/index.html#quick-personal-introduction",
    "href": "slides/01-01-intro/index.html#quick-personal-introduction",
    "title": "Introduction",
    "section": "Quick personal introduction",
    "text": "Quick personal introduction\n\n\n\nField ecologist turned statistican\nstarted off in insect-plant interactions, has now done:\n\nfish populations\nkangaroo life history\nfly mating behaviour\nanimal personality\n\nMostly a Bayesian\nPlease introduce yourselves and your projects!"
  },
  {
    "objectID": "slides/01-01-intro/index.html#statistical-rethinking",
    "href": "slides/01-01-intro/index.html#statistical-rethinking",
    "title": "Introduction",
    "section": "Statistical Rethinking",
    "text": "Statistical Rethinking\n\nA good portion of this course material is based on this book! it is a fabulous guide to self-teaching!"
  },
  {
    "objectID": "slides/01-01-intro/index.html#bayesian-data-analysis",
    "href": "slides/01-01-intro/index.html#bayesian-data-analysis",
    "title": "Introduction",
    "section": "Bayesian Data Analysis",
    "text": "Bayesian Data Analysis\n\nEverything is there – rather technical !"
  },
  {
    "objectID": "slides/01-01-intro/index.html#bayesian-data-analysis-1",
    "href": "slides/01-01-intro/index.html#bayesian-data-analysis-1",
    "title": "Introduction",
    "section": "Bayesian Data Analysis",
    "text": "Bayesian Data Analysis\n\nExcellent pencil-and-paper intro to Bayesian thinking"
  },
  {
    "objectID": "slides/01-01-intro/index.html#course-datasets",
    "href": "slides/01-01-intro/index.html#course-datasets",
    "title": "Introduction",
    "section": "Course datasets",
    "text": "Course datasets\n\nPalmer Penguins\nmite data from the vegan package\nQuinn and Keough datasets"
  },
  {
    "objectID": "slides/01-01-intro/index.html#some-examples-of-linear-models-in-ecology",
    "href": "slides/01-01-intro/index.html#some-examples-of-linear-models-in-ecology",
    "title": "Introduction",
    "section": "Some examples of linear models in Ecology:",
    "text": "Some examples of linear models in Ecology:\n\na randomized block design\nBACI - pre-treatment variable\ngrowth over time\nfactorial designs"
  },
  {
    "objectID": "slides/01-01-intro/index.html#extensions-to-other-kinds-of-data",
    "href": "slides/01-01-intro/index.html#extensions-to-other-kinds-of-data",
    "title": "Introduction",
    "section": "Extensions to other kinds of data",
    "text": "Extensions to other kinds of data\nLinear models can be “generalized” to deal with many kinds of data types:\n\ncounts\nproportions\nsurvival times\nand more"
  },
  {
    "objectID": "slides/01-01-intro/index.html#putting-models-inside-models-many-names-for-the-same-idea",
    "href": "slides/01-01-intro/index.html#putting-models-inside-models-many-names-for-the-same-idea",
    "title": "Introduction",
    "section": "Putting models inside models: Many names for the same idea",
    "text": "Putting models inside models: Many names for the same idea\nHierarchical models, AKA:\n\nRandom effect models\nMixed models\nMultilevel models\nVariance component models\nError component models"
  },
  {
    "objectID": "slides/01-01-intro/index.html#structure-of-linear-models",
    "href": "slides/01-01-intro/index.html#structure-of-linear-models",
    "title": "Introduction",
    "section": "Structure of linear models",
    "text": "Structure of linear models\n\\[\n\\begin{align}\ny &\\sim \\text{Normal}(\\bar{y}, \\sigma) \\\\\n\\bar{y} &= \\textbf{X} \\beta_1 + b_0 \\\\\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/01-01-intro/index.html#frequentist-vs-bayes-rival-stastical-paradigms",
    "href": "slides/01-01-intro/index.html#frequentist-vs-bayes-rival-stastical-paradigms",
    "title": "Introduction",
    "section": "Frequentist vs Bayes: “Rival” stastical paradigms?",
    "text": "Frequentist vs Bayes: “Rival” stastical paradigms?"
  },
  {
    "objectID": "slides/01-01-intro/index.html#frequentist-approach",
    "href": "slides/01-01-intro/index.html#frequentist-approach",
    "title": "Introduction",
    "section": "Frequentist approach",
    "text": "Frequentist approach\n\nMaximum likelihood – finding parameter values that make the data most likely to have happened.\nHypothesis testing: if the data are more extreme than we would observe 1/20 times if the null were true, then we assume that the null is NOT true."
  },
  {
    "objectID": "slides/01-01-intro/index.html#bayesian-approach",
    "href": "slides/01-01-intro/index.html#bayesian-approach",
    "title": "Introduction",
    "section": "Bayesian approach",
    "text": "Bayesian approach\nLooking for the joint probability of the parameters given the data.\n\\[\n[\\theta|\\text{data}] = \\frac{[\\text{data}|\\theta] \\times [\\theta]}{[\\text{data}]}\n\\]\n\nusing Bayes Rule does not make you a Bayesian. Using probability as a measure of uncertainty makes you a Bayesian"
  },
  {
    "objectID": "slides/01-01-intro/index.html#math-explanation",
    "href": "slides/01-01-intro/index.html#math-explanation",
    "title": "Introduction",
    "section": "Math explanation",
    "text": "Math explanation\n$$ [a,b,| X] = \n$$\n\nHypothesis testing: asking if a parameter is well-separated from 0."
  },
  {
    "objectID": "slides/01-01-intro/index.html#discussion",
    "href": "slides/01-01-intro/index.html#discussion",
    "title": "Introduction",
    "section": "Discussion",
    "text": "Discussion\n\nbefore telling you my opinion of linear models, I’d like to have a group discussion about what limitations each of US has experienced with them."
  },
  {
    "objectID": "slides/01-01-intro/index.html#linear-models-strengths",
    "href": "slides/01-01-intro/index.html#linear-models-strengths",
    "title": "Introduction",
    "section": "Linear models strengths",
    "text": "Linear models strengths\n\nEasy to interpret (?)\nEasy to fit .. most of the time\nDesigned to match experiments and test specific hypotheses\nDecent approximation to natural phenomenon (c.f. Taylor expansions)"
  },
  {
    "objectID": "slides/01-01-intro/index.html#linear-model-weaknesses",
    "href": "slides/01-01-intro/index.html#linear-model-weaknesses",
    "title": "Introduction",
    "section": "Linear model weaknesses",
    "text": "Linear model weaknesses\n\nEasy to OVER interpret\nDoesn’t always fit if your data aren’t suitable\noverused and uncritically used\nLinear models don’t establish causes on their own\nNature has no straight lines"
  },
  {
    "objectID": "slides/01-02-simulation/index.html#simulation-can-change-your-science",
    "href": "slides/01-02-simulation/index.html#simulation-can-change-your-science",
    "title": "Model building with Simulation",
    "section": "Simulation can change your science",
    "text": "Simulation can change your science\nIf you can’t simulate from a model, you do not understand it"
  },
  {
    "objectID": "slides/01-02-simulation/index.html#simulation-recipie",
    "href": "slides/01-02-simulation/index.html#simulation-recipie",
    "title": "Model building with Simulation",
    "section": "Simulation recipie",
    "text": "Simulation recipie\n\nwrite down your statistical model\nfor each unknown variable: Assign priors (if you’re Bayesian) or pick a plausible value (if you’re frequentist)\ntreat those values like the truth and make a fake dataset with them\ntake this fake dataset and fit the very same model back to these values\nsee if the models estimates are close to the truth"
  },
  {
    "objectID": "slides/01-02-simulation/index.html#why-do-this",
    "href": "slides/01-02-simulation/index.html#why-do-this",
    "title": "Model building with Simulation",
    "section": "Why do this",
    "text": "Why do this\n\nthe MINIMUM test of a model’s ability is its ability to recover these known parameters\nin reality we DON’T know the true model, but we can hope!"
  },
  {
    "objectID": "slides/01-02-simulation/index.html#exercise-together-simulate-for-a-regression",
    "href": "slides/01-02-simulation/index.html#exercise-together-simulate-for-a-regression",
    "title": "Model building with Simulation",
    "section": "Exercise together – simulate for a regression",
    "text": "Exercise together – simulate for a regression"
  },
  {
    "objectID": "slides/01-02-simulation/index.html#solo-exercise-simulate-for-a-regression-with-two-predictors",
    "href": "slides/01-02-simulation/index.html#solo-exercise-simulate-for-a-regression-with-two-predictors",
    "title": "Model building with Simulation",
    "section": "Solo exercise – simulate for a regression with two predictors",
    "text": "Solo exercise – simulate for a regression with two predictors\n\nshould we use the Penguin data actually? have something simple."
  },
  {
    "objectID": "slides/01-02-simulation/index.html#simulating-for-factors",
    "href": "slides/01-02-simulation/index.html#simulating-for-factors",
    "title": "Model building with Simulation",
    "section": "Simulating for factors",
    "text": "Simulating for factors\n\nneed to consider the contrast matrix that we use for the model\nthere are several forms to consider!"
  },
  {
    "objectID": "slides/01-02-simulation/index.html#discussion-think-about-your-design",
    "href": "slides/01-02-simulation/index.html#discussion-think-about-your-design",
    "title": "Model building with Simulation",
    "section": "Discussion – think about YOUR design",
    "text": "Discussion – think about YOUR design"
  },
  {
    "objectID": "slides/01-02-simulation/index.html#plotting-to-understand-models",
    "href": "slides/01-02-simulation/index.html#plotting-to-understand-models",
    "title": "Model building with Simulation",
    "section": "Plotting to understand models",
    "text": "Plotting to understand models\nWe’ve already seen the power of a simulation and plots to understand the model BEFORE we run it.\nLet’s examine plots to understand models AFTER we have run a model."
  },
  {
    "objectID": "slides/01-02-simulation/index.html#prediction-vs-expectation",
    "href": "slides/01-02-simulation/index.html#prediction-vs-expectation",
    "title": "Model building with Simulation",
    "section": "Prediction vs expectation",
    "text": "Prediction vs expectation\nThere are two kinds of predictions we can make for a simple model:\n\nWhat does the model think is the average response: this is called the “fitted” or “expected” value.\nWhat kind of observations might we make in the future? : these are called predicted observations."
  },
  {
    "objectID": "slides/01-03-building-plotting/index.html#illustrative-datasets",
    "href": "slides/01-03-building-plotting/index.html#illustrative-datasets",
    "title": "Introduction to the Datasets",
    "section": "Illustrative datasets",
    "text": "Illustrative datasets\nTo illustrate the different models and methods we will discuss in this course, we will rely on a few data sets, which are directly available in different R packages\n\n\nmite, mite.env and mite.xy available in the vegan R package\n\n\n\n\npenguins available in the palmerpenguins R package\n\n\n\nThese datasets are practical because they are manageable in size and will allow you to see how to work out the different example presented in this course.\n\n\nLet’s look at them in more details…"
  },
  {
    "objectID": "slides/01-03-building-plotting/index.html#oribatid-mite-data",
    "href": "slides/01-03-building-plotting/index.html#oribatid-mite-data",
    "title": "Introduction to the Datasets",
    "section": "Oribatid mite data",
    "text": "Oribatid mite data\nAside from being very interesting, this dataset has been sampled at the Station biologique des Laurentides, so ~200 km north-west from here.\n\nSampling was carried out in June 1989 on the partially floating vegetation mat surrounding a lake, from the forest border to the free water by Daniel Borcard."
  },
  {
    "objectID": "slides/01-03-building-plotting/index.html#oribatid-mite-data-1",
    "href": "slides/01-03-building-plotting/index.html#oribatid-mite-data-1",
    "title": "Introduction to the Datasets",
    "section": "Oribatid mite data",
    "text": "Oribatid mite data\n\nOribatid mites are small (usually ranging in size from 0.2 to 1.4 mm) invertebrates that are part of the Arachnida class (so they have 8 legs).\n\n\n\n\n\n\n\nIn the mite data, 35 morphospecies were identified and counted across 70 samples."
  },
  {
    "objectID": "slides/01-03-building-plotting/index.html#sites-coordinates",
    "href": "slides/01-03-building-plotting/index.html#sites-coordinates",
    "title": "Introduction to the Datasets",
    "section": "Sites coordinates",
    "text": "Sites coordinates\nmite.xy"
  },
  {
    "objectID": "slides/01-03-building-plotting/index.html#vegetation-cover",
    "href": "slides/01-03-building-plotting/index.html#vegetation-cover",
    "title": "Introduction to the Datasets",
    "section": "Vegetation cover",
    "text": "Vegetation cover\nmite.env"
  },
  {
    "objectID": "slides/01-03-building-plotting/index.html#microtopography-and-shrub-cover",
    "href": "slides/01-03-building-plotting/index.html#microtopography-and-shrub-cover",
    "title": "Introduction to the Datasets",
    "section": "Microtopography and shrub cover",
    "text": "Microtopography and shrub cover\nmite.env"
  },
  {
    "objectID": "slides/01-03-building-plotting/index.html#substrate-density-and-water-content",
    "href": "slides/01-03-building-plotting/index.html#substrate-density-and-water-content",
    "title": "Introduction to the Datasets",
    "section": "Substrate density and water content",
    "text": "Substrate density and water content\nmite.env"
  },
  {
    "objectID": "slides/01-03-building-plotting/index.html#getting-the-data",
    "href": "slides/01-03-building-plotting/index.html#getting-the-data",
    "title": "Introduction to the Datasets",
    "section": "Getting the data",
    "text": "Getting the data"
  },
  {
    "objectID": "slides/01-03-building-plotting/index.html#palmer-penguins",
    "href": "slides/01-03-building-plotting/index.html#palmer-penguins",
    "title": "Introduction to the Datasets",
    "section": "Palmer penguins",
    "text": "Palmer penguins\n\nThe Palmer Archipelago penguins. Artwork by @allison_horst\nThese data were collected from 2007 to 2009 by Dr. Kristen Gorman with the Palmer Station Long Term Ecological Research Program, part of the US Long Term Ecological Research Network.\n\n\n\nThe data were imported directly from the Environmental Data Initiative (EDI) Data Portal, and are available for use by CC0 license (“No Rights Reserved”) in accordance with the Palmer Station Data Policy. [@gorman2014; @horst2020]\n\nhttps://allisonhorst.github.io/palmerpenguins/"
  },
  {
    "objectID": "slides/01-03-building-plotting/index.html#behold-simpsons-paradox",
    "href": "slides/01-03-building-plotting/index.html#behold-simpsons-paradox",
    "title": "Introduction to the Datasets",
    "section": "Behold: Simpson’s Paradox!",
    "text": "Behold: Simpson’s Paradox!"
  },
  {
    "objectID": "slides/01-03-building-plotting/index.html#behold-simpsons-paradox-1",
    "href": "slides/01-03-building-plotting/index.html#behold-simpsons-paradox-1",
    "title": "Introduction to the Datasets",
    "section": "Behold: Simpson’s Paradox!",
    "text": "Behold: Simpson’s Paradox!"
  },
  {
    "objectID": "slides/01-03-building-plotting/index.html#theres-lots-more",
    "href": "slides/01-03-building-plotting/index.html#theres-lots-more",
    "title": "Introduction to the Datasets",
    "section": "There’s lots more!",
    "text": "There’s lots more!\n\n\n\nand also see the official site: https://allisonhorst.github.io/palmerpenguins/"
  },
  {
    "objectID": "slides/01-04-Linear_model/index.html#basic-regression-model",
    "href": "slides/01-04-Linear_model/index.html#basic-regression-model",
    "title": "Linear models",
    "section": "Basic regression model",
    "text": "Basic regression model\n\nHierarchical models are a generalized version of the classic regression models you have seen in your undergraduate courses.\n\n\n\nIn its simplest form, a regression model is usually presented as\n\n\n\n\\[\ny_i = \\beta_0 + \\beta_1 x_{i} + \\varepsilon\n\\]\n\n\n\nIt is known as a simple linear model, where :\n\n\n\n\n\\(y_i\\) is the value of a response variable for observation \\(i\\)\n\n\n\n\n\\(x_i\\) is the value of an explanatory variable for observation \\(i\\)\n\n\n\n\n\\(\\beta_0\\) is the model intercept\n\n\n\n\n\\(\\beta_1\\) is the model slope\n\n\n\n\n\\(\\varepsilon\\) is the error term"
  },
  {
    "objectID": "slides/01-04-Linear_model/index.html#basic-regression-model-1",
    "href": "slides/01-04-Linear_model/index.html#basic-regression-model-1",
    "title": "Linear models",
    "section": "Basic regression model",
    "text": "Basic regression model\nThe cool thing about the simple linear model is that it can be studied visually quite easily.\n\nFor example if we are interested in knowing how a newly discovered plant species (Bidonia exemplaris) reacts to humidity, we can relate the biomass of B. exemplaris sampled at 100 sites with the soil humidity content and readily visual the data and the trend."
  },
  {
    "objectID": "slides/01-04-Linear_model/index.html#basic-regression-model-2",
    "href": "slides/01-04-Linear_model/index.html#basic-regression-model-2",
    "title": "Linear models",
    "section": "Basic regression model",
    "text": "Basic regression model"
  },
  {
    "objectID": "slides/01-04-Linear_model/index.html#regression-parameters",
    "href": "slides/01-04-Linear_model/index.html#regression-parameters",
    "title": "Linear models",
    "section": "Regression parameters",
    "text": "Regression parameters\nGenerally, the slope and the intercept are the regression parameters we focus on when studying the simple linear model, but there is another one that is very important to consider, especially for this course.\n\nAny ideas which one it is ?"
  },
  {
    "objectID": "slides/01-04-Linear_model/index.html#regression-parameters-1",
    "href": "slides/01-04-Linear_model/index.html#regression-parameters-1",
    "title": "Linear models",
    "section": "Regression parameters",
    "text": "Regression parameters\n\nIf we go back to the mathematical description of the model\n\\[\ny_i = \\beta_0 + \\beta_1 x_{i} + \\varepsilon\n\\]\n\n\n\nwe can see that in the simple linear regression the error term (\\(\\varepsilon\\)) has actually a very precise definition:\n\n\n\n\n\\[\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)\\]\n\n\nwhere \\(\\sigma^2\\) is an estimated variance.\n\n\n\n\nIn words, it means that the error in a simple linear regression follows a Gaussian distribution with a variance that is estimated.\n\n\n\n\nFor most of the course, we will play with the variance parameter \\(\\sigma^2\\) in a bunch of different ways.\n\n\n\n\nBut before we do this, we need to understand a bit more about how this parameter influence the model."
  },
  {
    "objectID": "slides/01-04-Linear_model/index.html#regression-parameters-2",
    "href": "slides/01-04-Linear_model/index.html#regression-parameters-2",
    "title": "Linear models",
    "section": "Regression parameters",
    "text": "Regression parameters\nA first way to do this is to think about the simple linear regression in a slightly different way. Specifically, based on what we learned in the previous slide the simple linear regression can be rewritten as \\[\ny \\sim \\mathcal{N}(\\beta_0 + \\beta_1 x_{i}, \\sigma^2)\n\\]\n\nAs we will see later in this course, this writting style will become particularly useful."
  },
  {
    "objectID": "slides/01-04-Linear_model/index.html#regression-parameters-3",
    "href": "slides/01-04-Linear_model/index.html#regression-parameters-3",
    "title": "Linear models",
    "section": "Regression parameters",
    "text": "Regression parameters\nVariance of the model (\\(\\sigma^2\\))\n\nIn essence, \\(\\sigma^2\\) tells us about what the model could not account for.\n\n\n\nFor example, let’s compare the biomass of Bidonia exemplaris with that of Ilovea hockeyshow, another species (a carnivorous plant)"
  },
  {
    "objectID": "slides/01-04-Linear_model/index.html#regression-parameters-4",
    "href": "slides/01-04-Linear_model/index.html#regression-parameters-4",
    "title": "Linear models",
    "section": "Regression parameters",
    "text": "Regression parameters\nVariance of the model (\\(\\sigma^2\\))\nBy regressing humidity on the biomass of both plants, we can obtain the estimated parameters for each regression (which are all available using summary.lm)\n\n\n# Regression model\nregBexemplaris &lt;- lm(b.exemplaris ~ humidity)\nregIhockeyshow &lt;- lm(i.hockeyshow ~ humidity)\n\n# Summary\nsummaryRegBexemplaris &lt;- summary(regBexemplaris)\nsummaryRegIhockeyshow &lt;- summary(regIhockeyshow)"
  },
  {
    "objectID": "slides/01-04-Linear_model/index.html#regression-parameters-5",
    "href": "slides/01-04-Linear_model/index.html#regression-parameters-5",
    "title": "Linear models",
    "section": "Regression parameters",
    "text": "Regression parameters\nVariance of the model (\\(\\sigma^2\\))\n\n\nFor Bidonia exemplaris\n\n# Estimated coefficients\nsummaryRegBexemplaris$coefficients[,1:2]\n\n            Estimate Std. Error\n(Intercept) 4.015999 0.05235200\nhumidity    1.313897 0.08845811\n\n# Estimated variance\nsummaryRegBexemplaris$sigma\n\n[1] 0.5232624\n\n\n\nFor Ilovea hockeyshow\n\n# Estimated coefficients\nsummaryRegIhockeyshow$coefficients[,1:2]\n\n            Estimate  Std. Error\n(Intercept) 3.991785 0.008928294\nhumidity    1.271250 0.015085956\n\n# Estimated variance\nsummaryRegIhockeyshow$sigma\n\n[1] 0.089239"
  },
  {
    "objectID": "slides/01-04-Linear_model/index.html#limits-of-the-simple-linear-regression",
    "href": "slides/01-04-Linear_model/index.html#limits-of-the-simple-linear-regression",
    "title": "Linear models",
    "section": "Limits of the simple linear regression",
    "text": "Limits of the simple linear regression\nThere are two major pitfalls of the simple linear model for problems in the life sciences\n\n\nOne explanatory is almost never enough to approach biological questions nowadays.\n\n\n\n\nThe simple linear model assumes that the error follows a Gaussian distribution."
  },
  {
    "objectID": "slides/01-04-Linear_model/index.html#multiple-linear-regression",
    "href": "slides/01-04-Linear_model/index.html#multiple-linear-regression",
    "title": "Linear models",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\n\nSimple linear regression can be extended to account for multiple explanatory variables to study more complex problems. This type of regression model is known as a multiple linear regression.\n\n\n\nMathematically, a multiple linear regression can be defined as\n\\[\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_p x_{ip} + \\varepsilon\n\\]\n\n\n\n\nTheoretically, estimating the parameters of a multiple regression model is done the same ways as for simple linear regression. However, technically, matrix algebra is quite practical to use in this context and especially for this course."
  },
  {
    "objectID": "slides/01-04-Linear_model/index.html#multiple-linear-regression-1",
    "href": "slides/01-04-Linear_model/index.html#multiple-linear-regression-1",
    "title": "Linear models",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\n\nIn this course, we will rely heavily on multiple linear regression and expand on it by studying how some of the parameters (the \\(\\beta\\)s) can depend on other other data and parameters.\n\n\nAs we saw earlier, a classic way to write multiple linear regression is\n\\[\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_p x_{ip} + \\varepsilon\n\\]\n\n\nHowever, we can rewrite this using matrix notation has\n\\[\n\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\n\\]"
  },
  {
    "objectID": "slides/01-04-Linear_model/index.html#multiple-linear-regression-2",
    "href": "slides/01-04-Linear_model/index.html#multiple-linear-regression-2",
    "title": "Linear models",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\nMatrix notation\n\\[\n\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\n\\]\nUsing the matrix notation, we assume that\n\n\n\n\\(\\mathbf{y}\\) is a vector of length \\(n\\) (samples)\n\n\n\n\n\n\n\\(\\mathbf{X}\\) is a matrix with \\(n\\) samples (rows) and representing \\(p\\) explanatory variables (columns)\n\n\n\n\n\n\n\\(\\boldsymbol{\\beta}\\) is a vector of \\(p\\) regression coefficients\n\n\n\n\n\n\n\\(\\boldsymbol{\\varepsilon}\\) is a vector of residuals of length \\(n\\)"
  },
  {
    "objectID": "slides/01-04-Linear_model/index.html#error-in-linear-models",
    "href": "slides/01-04-Linear_model/index.html#error-in-linear-models",
    "title": "Linear models",
    "section": "Error in linear models",
    "text": "Error in linear models\nAs previously mentioned, in (simple and multiple!) linear regression the error term (\\(\\varepsilon\\)) has actually a very precise definition:\n\\[\\boldsymbol{\\varepsilon} \\sim \\mathcal{MVN}(0, \\sigma^2\\mathbf{I})\\] where \\(\\sigma^2\\) is an estimated variance\n\nwhich means that the error in a linear regression follows a Gaussian distribution with an estimated variance.\n\n\nNote The model can be written using matrix notation as\n\\[\\mathbf{y} \\sim \\mathcal{MVN}(\\mathbf{X}\\boldsymbol{\\beta}, \\sigma^2\\mathbf{I})\\] where \\(\\sigma^2\\)"
  },
  {
    "objectID": "slides/01-04-Linear_model/index.html#generalized-linear-models",
    "href": "slides/01-04-Linear_model/index.html#generalized-linear-models",
    "title": "Linear models",
    "section": "Generalized linear models",
    "text": "Generalized linear models\n\nIf for some reason we do not want our model to have a Gaussian error, generalized linear models (GLMs) have been proposed. In essence, GLMs use link functions to adapt models for them to be used on non-Gaussian data.\n\n\nMathematically, the generic way to write generalized linear model is\n\\[\n\\widehat{y}_i = g^{-1}(\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_p x_{ip})\n\\]\n\n\nor in matrix notation\n\\[\n\\widehat{\\mathbf{y}} = g^{-1}(\\mathbf{X}\\boldsymbol{\\beta})\n\\]\nwhere \\(g\\) is the link function and \\(g^{-1}\\) the inverse link function."
  },
  {
    "objectID": "slides/01-04-Linear_model/index.html#generalized-linear-models-1",
    "href": "slides/01-04-Linear_model/index.html#generalized-linear-models-1",
    "title": "Linear models",
    "section": "Generalized linear models",
    "text": "Generalized linear models\nLink functions\n\nThere are many types of link functions and they are usually directly associated to the underlying data the analysis is carried out on.\n\n\n\nArguably the most common link function in ecology is\n\n\n\nlogit link function\n\nIt is commonly used for modelling binary (0-1) data.\n\\[\n\\mathbf{X}\\boldsymbol{\\beta} = \\ln\\left(\\frac{\\widehat{\\mathbf{y}}}{1 - \\widehat{\\mathbf{y}}}\\right)\n\\]\n\n\n\n\nThe inverse logit link function is\n\\[\n\\widehat{\\mathbf{y}} = \\frac{\\exp(\\mathbf{X}\\boldsymbol{\\beta})}{1 + \\exp(\\mathbf{X}\\boldsymbol{\\beta})} = \\frac{1}{1 + \\exp(-\\mathbf{X}\\boldsymbol{\\beta})}\n\\]"
  },
  {
    "objectID": "slides/01-04-Linear_model/index.html#generalized-linear-models-2",
    "href": "slides/01-04-Linear_model/index.html#generalized-linear-models-2",
    "title": "Linear models",
    "section": "Generalized linear models",
    "text": "Generalized linear models\nLink functions\nAnother commonly used link function is\n\nlog link function\nIt is commonly used for modelling count data.\n\\[\n\\mathbf{X}\\boldsymbol{\\beta} = \\ln\\left(\\widehat{\\mathbf{y}}\\right)\n\\]\n\n\nThe inverse logit link function is\n\\[\n\\widehat{\\mathbf{y}} = \\exp(\\mathbf{X}\\boldsymbol{\\beta})\n\\]"
  },
  {
    "objectID": "slides/03-01-multilevel-intro/index.html",
    "href": "slides/03-01-multilevel-intro/index.html",
    "title": "index",
    "section": "",
    "text": ". . .\n\nHierarchical models have been implemented in many software packages,\n\n. . .\n\nin R\n\nlme4, brms, nlme, glmmTMB, MCMCglmm, …\n\n\n. . .\n\nin SAS\n\nMIXED, HPMIXED, GLMMIX, …\n\n\n. . .\n\nin Julia\n\nMixedModels.jl\n\n…"
  },
  {
    "objectID": "slides/03-01-multilevel-intro/index.html#implementation",
    "href": "slides/03-01-multilevel-intro/index.html#implementation",
    "title": "index",
    "section": "",
    "text": ". . .\n\nHierarchical models have been implemented in many software packages,\n\n. . .\n\nin R\n\nlme4, brms, nlme, glmmTMB, MCMCglmm, …\n\n\n. . .\n\nin SAS\n\nMIXED, HPMIXED, GLMMIX, …\n\n\n. . .\n\nin Julia\n\nMixedModels.jl\n\n…"
  },
  {
    "objectID": "slides/03-01-multilevel-intro/index.html#a-bit-of-vocabulary",
    "href": "slides/03-01-multilevel-intro/index.html#a-bit-of-vocabulary",
    "title": "index",
    "section": "A bit of vocabulary",
    "text": "A bit of vocabulary\nMultiple Definition of fixed and random effects\n. . .\n\n(Kreft and De Leeuw 1998) Fixed effects are constant and random effect vary\n\n. . .\n\n(Searl et al. 1992) Effects are fixed if they are interesting in themselves or random if there is interest in the underlying population\n\n. . .\n\n(Green and Tukey 1960) When a sample exhausts the population, the corresponding variable is fixed; when the sample is a small (i.e., negligible) part of the population the corresponding variable is random\n\n. . .\n\n(Roy LaMotte 2014) If an effect is assumed to be a realized value of a random variable, it is called a random effect\n\n. . .\n\n(Robinson 1991) Fixed effects are estimated using least squares (or, more generally, maximum likelihood) and random effects are estimated with shrinkage."
  }
]