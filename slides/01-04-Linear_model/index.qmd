---
title: "Linear models"
author: "Andrew MacDonald & Guillaume Blanchet"
date: "2026-02-02"
execute:
  echo: true
format: 
  revealjs:
    transition: slide
    background-transition: fade
---

## Basic regression model

::: {style="font-size: 0.8em"}
Hierarchical models are a generalized version of the classic regression models you have seen in your undergraduate courses.
:::

. . .

::: {style="font-size: 0.8em"}
In its simplest form, a regression model is usually presented as
:::

. . .

$$
y_i = \beta_0 + \beta_1 x_{i} + \varepsilon
$$

. . .

::: {style="font-size: 0.8em"}
It is known as a **simple linear model**, where :
:::

. . .

::: {style="font-size: 0.8em"}
$y_i$ is the value of a response variable for observation $i$
:::

. . .

::: {style="font-size: 0.8em"}
$x_i$ is the value of an explanatory variable for observation $i$
:::

. . .

::: {style="font-size: 0.8em"}
$\beta_0$ is the model intercept
:::

. . .

::: {style="font-size: 0.8em"}
$\beta_1$ is the model slope
:::

. . .

::: {style="font-size: 0.8em"}
$\varepsilon$ is the error term
:::

## Basic regression model

The cool thing about the simple linear model is that it can be studied visually quite easily.

. . .

For example if we are interested in knowing how a newly discovered plant species (*Bidonia exemplaris*) reacts to humidity, we can relate the biomass of *B. exemplaris* sampled at 100 sites with the soil humidity content and readily visual the data and the trend.

## Basic regression model

```{r, echo = FALSE, fig.width=8,  fig.height=8, fig.align='center'}
# Simulate some data
set.seed(42) # The answer !

covParam <- matrix(c(0.6, 0.3,0.3,0.8), 2, 2)
meanParam <- c(1.5, 0.2)
param <- MASS::mvrnorm(1, mu = meanParam, Sigma = covParam)
yintercept <- param[1]
slope <- param[2]
obs_error <- 0.5

x <- runif(100, min = -1, max = 1)
y_mean <- yintercept + slope * x
y_obs <- rnorm(100, mean = y_mean, sd = obs_error)

b.exemplaris <- y_obs
humidity <- x

par(mar = c(6,7,0.1,0.1))
plot(x, y_obs,
     pch = 19,
     col="blue", 
     las=1,
     cex.axis=2.3,
     bty="L",
     xaxs="i",
     xlab = "",
     ylab = "",
     cex = 2)

mtext("Humidity", side = 1, line = 4.5, cex = 3.5)
mtext("Biomass", side = 2, line = 4.5, cex = 3.5)

abline(a = yintercept,
       b = slope,
       lwd = 10,
       col = "orange")
```

## Regression parameters

Generally, the **slope** and the **intercept** are the regression parameters we focus on when studying the simple linear model, but there is another one that is very important to consider, especially for this course.

. . .

**Any ideas which one it is ?**

![](https://www.i2symbol.com/pictures/emojis/3/9/7/2/3972d0a5cdf7dffdaf58a912331839c7_384.png){fig-align="center" width="35%"}

## Regression parameters

::: {style="font-size: 0.75em"}
If we go back to the mathematical description of the model

$$
y_i = \beta_0 + \beta_1 x_{i} + \varepsilon
$$
:::

. . .

::: {style="font-size: 0.75em"}
we can see that in the simple linear regression the error term ($\varepsilon$) has actually a very precise definition:
:::

. . .

::: {style="font-size: 0.75em"}
$$\varepsilon \sim \mathcal{N}(0, \sigma^2)$$
:::

::: {style="font-size: 0.75em"}
where $\sigma^2$ is an estimated variance.
:::

. . .

::: {style="font-size: 0.75em"}
In words, it means that the error in a simple linear regression follows a Gaussian distribution with a variance that is estimated.
:::

. . .

::: {style="font-size: 0.75em"}
For most of the course, we will play with the variance parameter $\sigma^2$ in a bunch of different ways.
:::

. . .

::: {style="font-size: 0.75em"}
But before we do this, we need to understand a bit more about how this parameter influence the model.
:::

## Regression parameters

A first way to do this is to think about the simple linear regression in a slightly different way.
Specifically, based on what we learned in the previous slide the simple linear regression can be rewritten as $$
y \sim \mathcal{N}(\beta_0 + \beta_1 x_{i}, \sigma^2)
$$

. . .

As we will see later in this course, this writting style will become particularly useful.

## Regression parameters

**Variance of the model (**$\sigma^2$)

::: {style="font-size: 0.75em"}
In essence, $\sigma^2$ tells us about what the model could not account for.
:::

. . .

::: {style="font-size: 0.75em"}
For example, let's compare the biomass of *Bidonia exemplaris* with that of *Ilovea hockeyshow*, another species (a carnivorous plant)
:::

. . .

```{r, echo = FALSE, fig.width=16,  fig.height=6.5, fig.align='center'}
# Simulate some data
set.seed(42) # The answer !

yintercept <- 4
slope <- 1.3
obs_error <- 0.5
obs_error_small <- 0.1

y_mean <- yintercept + slope * humidity
y_obs <- rnorm(100, mean = y_mean, sd = obs_error)
y_obs_small <- rnorm(100, mean = y_mean, sd = obs_error_small)

par(mfrow = c(1,2),
    mar = c(6,7,3,0.1))

plot(humidity, y_obs,
     pch = 19,
     col="blue", 
     las=1,
     cex.axis=2.3,
     bty="L",
     xaxs="i",
     xlab = "",
     ylab = "",
     cex = 2,
     asp = 1,
     main = "B. exemplaris",
     cex.main = 3.5)

mtext("Humidity", side = 1, line = 4.5, cex = 3.5)
mtext("Biomass", side = 2, line = 4.5, cex = 3.5)

plot(humidity, y_obs_small,
     pch = 19,
     col="blue", 
     las=1,
     cex.axis=2.3,
     bty="L",
     xaxs="i",
     xlab = "",
     ylab = "",
     cex.lab = 2.5,
     cex = 2,
     asp = 1,
     main = "I. hockeyshow",
     cex.main = 3.5)

mtext("Humidity", side = 1, line = 4.5, cex = 3.5)


# For the next slide
b.exemplaris <- y_obs
i.hockeyshow <- y_obs_small
```

## Regression parameters

**Variance of the model (**$\sigma^2$)

By regressing humidity on the biomass of both plants, we can obtain the estimated parameters for each regression (which are all available using `summary.lm`)

. . .

```{r}
# Regression model
regBexemplaris <- lm(b.exemplaris ~ humidity)
regIhockeyshow <- lm(i.hockeyshow ~ humidity)

# Summary
summaryRegBexemplaris <- summary(regBexemplaris)
summaryRegIhockeyshow <- summary(regIhockeyshow)
```

## Regression parameters

**Variance of the model (**$\sigma^2$)

::::: columns
::: {.column width="50%"}
For *Bidonia exemplaris*

```{r}
# Estimated coefficients
summaryRegBexemplaris$coefficients[,1:2]

# Estimated variance
summaryRegBexemplaris$sigma
```
:::

::: {.column width="50%"}
For *Ilovea hockeyshow*

```{r}
# Estimated coefficients
summaryRegIhockeyshow$coefficients[,1:2]

# Estimated variance
summaryRegIhockeyshow$sigma
```
:::
:::::

## Limits of the simple linear regression

There are two major pitfalls of the simple linear model for problems in the life sciences

. . .

1.  One explanatory is almost never enough to approach biological questions nowadays.

. . .

2.  The simple linear model assumes that the error follows a Gaussian distribution.

## Multiple linear regression

::: {style="font-size: 0.85em"}
Simple linear regression can be extended to account for multiple explanatory variables to study more complex problems.
This type of regression model is known as a **multiple linear regression**.
:::

. . .

::: {style="font-size: 0.85em"}
Mathematically, a multiple linear regression can be defined as

$$
y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_p x_{ip} + \varepsilon
$$
:::

. . .

::: {style="font-size: 0.85em"}
Theoretically, estimating the parameters of a multiple regression model is done the same ways as for simple linear regression.
However, technically, matrix algebra is quite practical to use in this context and especially for this course.
:::

. . .


## Multiple linear regression

. . .

In this course, we will rely heavily on multiple linear regression and expand on it by studying how some of the parameters (the $\beta$s) can depend on other other data and parameters.

. . .

As we saw earlier, a classic way to write multiple linear regression is

$$
y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_p x_{ip} + \varepsilon
$$

. . .

However, we can rewrite this using matrix notation has

$$
\mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon}
$$

## Multiple linear regression

**Matrix notation**

$$
\mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon}
$$

Using the matrix notation, we assume that

. . .

::: {style="font-size: 0.9em"}
-   $\mathbf{y}$ is a vector of length $n$ (samples)
:::

. . .

::: {style="font-size: 0.9em"}
-   $\mathbf{X}$ is a matrix with $n$ samples (rows) and representing $p$ explanatory variables (columns)
:::

. . .

::: {style="font-size: 0.9em"}
-   $\boldsymbol{\beta}$ is a vector of $p$ regression coefficients
:::

. . .

::: {style="font-size: 0.9em"}
-   $\boldsymbol{\varepsilon}$ is a vector of residuals of length $n$
:::

## Error in linear models

As previously mentioned, in (simple and multiple!) linear regression the error term ($\varepsilon$) has actually a very precise definition:

$$\boldsymbol{\varepsilon} \sim \mathcal{MVN}(0, \sigma^2\mathbf{I})$$ where $\sigma^2$ is an estimated variance

. . .

which means that the error in a linear regression follows a Gaussian distribution with an estimated variance.

. . .

**Note** The model can be written using matrix notation as\
$$\mathbf{y} \sim \mathcal{MVN}(\mathbf{X}\boldsymbol{\beta}, \sigma^2\mathbf{I})$$ where $\sigma^2$

## Generalized linear models

. . .

If for some reason we do not want our model to have a Gaussian error, *generalized* linear models (GLMs) have been proposed.
In essence, GLMs use *link functions* to adapt models for them to be used on non-Gaussian data.

. . .

Mathematically, the generic way to write generalized linear model is

$$
\widehat{y}_i = g^{-1}(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_p x_{ip})
$$

. . .

or in matrix notation

$$
\widehat{\mathbf{y}} = g^{-1}(\mathbf{X}\boldsymbol{\beta})
$$

where $g$ is the link function and $g^{-1}$ the inverse link function.

## Generalized linear models

**Link functions**

::: {style="font-size: 0.75em"}
There are many types of link functions and they are usually directly associated to the underlying data the analysis is carried out on.
:::

. . .

::: {style="font-size: 0.75em"}
Arguably the most common link function in ecology is
:::

. . .

*logit link function*

::: {style="font-size: 0.75em"}
It is commonly used for modelling binary (0-1) data.

$$
\mathbf{X}\boldsymbol{\beta} = \ln\left(\frac{\widehat{\mathbf{y}}}{1 - \widehat{\mathbf{y}}}\right)
$$
:::

. . .

::: {style="font-size: 0.75em"}
The inverse logit link function is

$$
\widehat{\mathbf{y}} = \frac{\exp(\mathbf{X}\boldsymbol{\beta})}{1 + \exp(\mathbf{X}\boldsymbol{\beta})} = \frac{1}{1 + \exp(-\mathbf{X}\boldsymbol{\beta})}
$$
:::

## Generalized linear models

**Link functions**

Another commonly used link function is

. . .

*log link function*

It is commonly used for modelling count data.

$$
\mathbf{X}\boldsymbol{\beta} = \ln\left(\widehat{\mathbf{y}}\right)
$$

. . .

The inverse logit link function is

$$
\widehat{\mathbf{y}} = \exp(\mathbf{X}\boldsymbol{\beta})
$$
